{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/SMIED/blob/adding-semantic-decomposition/BeamSemantic_Decomposition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J9B-9RI5lRP"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vww_V3sIQzh"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSlaJDUEPnfT"
      },
      "source": [
        "## Import, config stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Jb_gtRCvh-DF"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Semantic decomposition of (\"cat\", \"eats\", \"mouse\") using WordNet + spaCy + depth-limited GBFS.\n",
        "- Uses spaCy to parse verb synset glosses and detect subject/object dependencies.\n",
        "- If both subject and object tokens are present, branches directly toward original triple synsets.\n",
        "- Otherwise falls back to WordNet relations.\n",
        "\"\"\"\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from heapq import heappush, heappop\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gensim.downloader as api\n",
        "from collections import deque\n",
        "import heapq\n",
        "import itertools\n",
        "from typing import Callable, Dict, Iterable, List, Optional, Set, Tuple\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj4iZlAfh_t8",
        "outputId": "98757067-9eb9-4087-de40-587ab62654df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "# Load spaCy English model for dependency parsing\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5TGC7I88EvX",
        "outputId": "c1befd6a-cb91-4e37-c749-febfa801f277"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "embedding_model = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7x7t4YeLHwid"
      },
      "outputs": [],
      "source": [
        "# Type aliases\n",
        "SynsetName = str  # e.g., \"dog.n.01\"\n",
        "Path = List[SynsetName]\n",
        "BeamElement = Tuple[Tuple[SynsetName, str], Tuple[SynsetName, str], float]\n",
        "GetNewBeamsFn = Callable[[nx.DiGraph, SynsetName, SynsetName], List[BeamElement]]\n",
        "TopKBranchFn = Callable[[List[List], object, int], List[BeamElement]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6Ahax9MciBc"
      },
      "source": [
        "# Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZkUj-vdOivAP"
      },
      "outputs": [],
      "source": [
        "def wn_to_nx():\n",
        "    # Initialize directed graph\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    synset_rels = {\n",
        "        # holonyms\n",
        "        \"part_holonyms\": lambda x: x.part_holonyms(),\n",
        "        \"substance_holonyms\": lambda x: x.substance_holonyms(),\n",
        "        \"member_holonyms\": lambda x: x.member_holonyms(),\n",
        "\n",
        "        # meronyms\n",
        "        \"part_meronyms\": lambda x: x.part_meronyms(),\n",
        "        \"substance_meronyms\": lambda x: x.substance_meronyms(),\n",
        "        \"member_meronyms\": lambda x: x.member_meronyms(),\n",
        "\n",
        "        # other\n",
        "        \"hypernyms\": lambda x: x.hypernyms(),\n",
        "        \"hyponyms\": lambda x: x.hyponyms(),\n",
        "        \"entailments\": lambda x: x.entailments(),\n",
        "        \"causes\": lambda x: x.causes(),\n",
        "        \"also_sees\": lambda x: x.also_sees(),\n",
        "        \"verb_groups\": lambda x: x.verb_groups(),\n",
        "    }\n",
        "\n",
        "    # add nodes (synsets) and all their edges (lexical relations) to the nx graph\n",
        "    for synset in wn.all_synsets():\n",
        "        for rel_name, rel_func in synset_rels.items():\n",
        "            for target in rel_func(synset):\n",
        "                G.add_edge(\n",
        "                    synset.name(),\n",
        "                    target.name(),\n",
        "                    relation = rel_name[:-1]\n",
        "                )\n",
        "    return G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xoGoYrlVcjiG"
      },
      "outputs": [],
      "source": [
        "def get_all_neighbors(synset: wn.synset) -> List[wn.synset]:\n",
        "    \"\"\"Get all neighbors of a synset based on its POS.\"\"\"\n",
        "    neighbors = []\n",
        "\n",
        "    # Add hypernyms and hyponyms\n",
        "    neighbors.extend(synset.hypernyms())\n",
        "    neighbors.extend(synset.hyponyms())\n",
        "\n",
        "    # Add POS-specific neighbors\n",
        "    if synset.pos() == 'n':\n",
        "        neighbors.extend(get_noun_neighbors(synset))\n",
        "    else:\n",
        "        neighbors.extend(get_verb_neighbors(synset))\n",
        "\n",
        "    return neighbors\n",
        "\n",
        "\n",
        "def get_noun_neighbors(syn: wn.synset):\n",
        "    \"\"\"Get neighbors for a noun synset.\"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.part_meronyms())\n",
        "    nbrs.update(syn.substance_meronyms())\n",
        "    nbrs.update(syn.member_meronyms())\n",
        "    nbrs.update(syn.part_holonyms())\n",
        "    nbrs.update(syn.substance_holonyms())\n",
        "    nbrs.update(syn.member_holonyms())\n",
        "    return list(nbrs)\n",
        "\n",
        "\n",
        "def get_verb_neighbors(syn: wn.synset):\n",
        "    \"\"\"Get neighbors for a verb synset.\"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.entailments())\n",
        "    nbrs.update(syn.causes())\n",
        "    nbrs.update(syn.also_sees())\n",
        "    nbrs.update(syn.verb_groups())\n",
        "    return list(nbrs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL_kkfMUY1bJ"
      },
      "source": [
        "# Embedding Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CRWsaZ8JHojT"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Embedding-based Helper Functions\n",
        "# ============================================================================\n",
        "\n",
        "def get_synset_embedding_centroid(synset, model) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Given a wn.synset, compute centroid (mean) of embeddings for lemmas.\n",
        "    Returns empty np.array if nothing found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        lemmas = [lemma.name().lower().replace(\"_\", \" \") for lemma in synset.lemmas()]\n",
        "        embeddings = []\n",
        "        for lemma in lemmas:\n",
        "            if lemma in model:\n",
        "                embeddings.append(np.asarray(model[lemma], dtype=float))\n",
        "            elif lemma.replace(\" \", \"_\") in model:\n",
        "                embeddings.append(np.asarray(model[lemma.replace(\" \", \"_\")], dtype=float))\n",
        "            elif \" \" in lemma:\n",
        "                # try individual words\n",
        "                words = lemma.split()\n",
        "                word_embs = [np.asarray(model[w], dtype=float) for w in words if w in model]\n",
        "                if word_embs:\n",
        "                    embeddings.append(np.mean(word_embs, axis=0))\n",
        "        if not embeddings:\n",
        "            return np.array([])  # empty\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    except Exception as e:\n",
        "        # defensive: return empty arr on any error\n",
        "        return np.array([])\n",
        "\n",
        "\n",
        "def embed_lexical_relations(synset, model) -> Dict[str, List[Tuple[SynsetName, np.ndarray]]]:\n",
        "    \"\"\"\n",
        "    Return map: lexical_rel_name -> list of (synset_name, centroid ndarray)\n",
        "    Filters out relations whose centroid is empty.\n",
        "    \"\"\"\n",
        "    def _rel_centroids(get_attr):\n",
        "        try:\n",
        "            items = []\n",
        "            for s in get_attr(synset):\n",
        "                cent = get_synset_embedding_centroid(s, model)\n",
        "                if cent.size > 0:\n",
        "                    items.append((s.name(), cent))\n",
        "            return items\n",
        "        except Exception:\n",
        "            return []\n",
        "\n",
        "    return {\n",
        "        \"part_holonyms\": _rel_centroids(lambda x: x.part_holonyms()),\n",
        "        \"substance_holonyms\": _rel_centroids(lambda x: x.substance_holonyms()),\n",
        "        \"member_holonyms\": _rel_centroids(lambda x: x.member_holonyms()),\n",
        "        \"part_meronyms\": _rel_centroids(lambda x: x.part_meronyms()),\n",
        "        \"substance_meronyms\": _rel_centroids(lambda x: x.substance_meronyms()),\n",
        "        \"member_meronyms\": _rel_centroids(lambda x: x.member_meronyms()),\n",
        "        \"hypernyms\": _rel_centroids(lambda x: x.hypernyms()),\n",
        "        \"hyponyms\": _rel_centroids(lambda x: x.hyponyms()),\n",
        "        \"entailments\": _rel_centroids(lambda x: x.entailments()),\n",
        "        \"causes\": _rel_centroids(lambda x: x.causes()),\n",
        "        \"also_sees\": _rel_centroids(lambda x: x.also_sees()),\n",
        "        \"verb_groups\": _rel_centroids(lambda x: x.verb_groups()),\n",
        "    }\n",
        "\n",
        "\n",
        "def get_embedding_similarities(rel_embs_1: List[Tuple[str, np.ndarray]],\n",
        "                              rel_embs_2: List[Tuple[str, np.ndarray]]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Return cosine similarity matrix (m x n) for lists of (name, centroid).\n",
        "    If either is empty, returns empty (0xN or Mx0) array.\n",
        "    \"\"\"\n",
        "    if not rel_embs_1 or not rel_embs_2:\n",
        "        return np.zeros((0, 0))\n",
        "\n",
        "    e1 = np.array([x[1] for x in rel_embs_1], dtype=float)  # (m,d)\n",
        "    e2 = np.array([x[1] for x in rel_embs_2], dtype=float)  # (n,d)\n",
        "\n",
        "    # avoid divide-by-zero: replace zero norms with eps\n",
        "    e1_norms = np.linalg.norm(e1, axis=1, keepdims=True)\n",
        "    e2_norms = np.linalg.norm(e2, axis=1, keepdims=True)\n",
        "    e1_norms[e1_norms == 0] = 1e-8\n",
        "    e2_norms[e2_norms == 0] = 1e-8\n",
        "\n",
        "    e1u = e1 / e1_norms\n",
        "    e2u = e2 / e2_norms\n",
        "\n",
        "    sims = np.dot(e1u, e2u.T)\n",
        "    return sims\n",
        "\n",
        "\n",
        "def get_top_k_aligned_lex_rel_pairs(\n",
        "    src_tgt_rel_map: Dict[str, str],\n",
        "    src_emb_dict: Dict[str, List[Tuple[SynsetName, np.ndarray]]],\n",
        "    tgt_emb_dict: Dict[str, List[Tuple[SynsetName, np.ndarray]]],\n",
        "    beam_width: int = 3,\n",
        ") -> List[BeamElement]:\n",
        "    \"\"\"\n",
        "    src_tgt_rel_map: mapping from relation name in src to relation name in tgt,\n",
        "      e.g., {'hypernyms': 'hyponyms', ...}\n",
        "\n",
        "    Returns list of ((src_syn_name, src_rel), (tgt_syn_name, tgt_rel), similarity)\n",
        "    \"\"\"\n",
        "    rel_sims = []\n",
        "    for e1_rel, e2_rel in src_tgt_rel_map.items():\n",
        "        e1_list = src_emb_dict.get(e1_rel, [])\n",
        "        e2_list = tgt_emb_dict.get(e2_rel, [])\n",
        "        if not e1_list or not e2_list:\n",
        "            continue\n",
        "        sims = get_embedding_similarities(e1_list, e2_list)  # shape (m,n)\n",
        "        if sims.size == 0:\n",
        "            continue\n",
        "        for i in range(sims.shape[0]):\n",
        "            for j in range(sims.shape[1]):\n",
        "                try:\n",
        "                    rel_sims.append(((e1_list[i][0], e1_rel), (e2_list[j][0], e2_rel), float(sims[i, j])))\n",
        "                except IndexError:\n",
        "                    continue\n",
        "\n",
        "    # sort and return top-k\n",
        "    rel_sims.sort(key=lambda x: x[2], reverse=True)\n",
        "    if beam_width <= 0:\n",
        "        return rel_sims\n",
        "    return rel_sims[:beam_width]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Refactored: Embedding-based synset branch ranking\n",
        "# ============================================================================\n",
        "\n",
        "def get_top_k_synset_branch_pairs(\n",
        "    candidates: List[List],  # List of lists of synsets\n",
        "    target_synsets,  # Single synset or list of synsets\n",
        "    beam_width: int = 3,\n",
        "    model=None,  # embedding model\n",
        "    wn_module=None  # WordNet module\n",
        ") -> List[BeamElement]:\n",
        "    \"\"\"\n",
        "    Refactored to use embedding-based alignment instead of get_synset_relatedness.\n",
        "\n",
        "    Given a list of candidate synset lists and target synset(s),\n",
        "    return the k synset-relation pairs most similar to the target.\n",
        "\n",
        "    Returns: List of ((synset_name, lexical_rel), (target_syn_name, lexical_rel), similarity)\n",
        "    \"\"\"\n",
        "    if model is None or wn_module is None:\n",
        "        return []\n",
        "\n",
        "    # Handle target synsets (single or list)\n",
        "    if not isinstance(target_synsets, (list, tuple)):\n",
        "        target_synsets = [target_synsets]\n",
        "\n",
        "    if not target_synsets:\n",
        "        return []\n",
        "\n",
        "    # Relation maps for alignment\n",
        "    asymm_map = {\n",
        "        \"hypernyms\": \"hyponyms\",\n",
        "        \"hyponyms\": \"hypernyms\",\n",
        "        \"part_meronyms\": \"part_holonyms\",\n",
        "        \"member_meronyms\": \"member_holonyms\",\n",
        "        \"substance_meronyms\": \"substance_holonyms\",\n",
        "        \"entailments\": \"causes\",\n",
        "        \"causes\": \"entailments\",\n",
        "    }\n",
        "    symm_map = {\n",
        "        \"hypernyms\": \"hypernyms\",\n",
        "        \"hyponyms\": \"hyponyms\",\n",
        "        \"part_meronyms\": \"part_meronyms\",\n",
        "        \"member_meronyms\": \"member_meronyms\",\n",
        "        \"also_sees\": \"also_sees\",\n",
        "        \"verb_groups\": \"verb_groups\",\n",
        "    }\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    # Process each target synset\n",
        "    for target_syn in target_synsets:\n",
        "        if target_syn is None:\n",
        "            continue\n",
        "\n",
        "        # Precompute target embeddings\n",
        "        tgt_emb_dict = embed_lexical_relations(target_syn, model)\n",
        "\n",
        "        # Process each candidate list\n",
        "        for synset_list in candidates:\n",
        "            if not synset_list:\n",
        "                continue\n",
        "\n",
        "            for synset in synset_list:\n",
        "                if synset is None:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Compute candidate embeddings\n",
        "                    src_emb_dict = embed_lexical_relations(synset, model)\n",
        "\n",
        "                    # Get aligned pairs from asymmetric relations\n",
        "                    asymm_pairs = get_top_k_aligned_lex_rel_pairs(\n",
        "                        asymm_map, src_emb_dict, tgt_emb_dict, beam_width=beam_width\n",
        "                    )\n",
        "                    all_results.extend(asymm_pairs)\n",
        "\n",
        "                    # Get aligned pairs from symmetric relations\n",
        "                    symm_pairs = get_top_k_aligned_lex_rel_pairs(\n",
        "                        symm_map, src_emb_dict, tgt_emb_dict, beam_width=beam_width\n",
        "                    )\n",
        "                    all_results.extend(symm_pairs)\n",
        "\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "    # Sort by similarity and return top-k\n",
        "    all_results.sort(key=lambda x: x[2], reverse=True)\n",
        "    return all_results[:beam_width]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "th4qyXLmYH4j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import Dict, List, Tuple, Optional, Iterable, Set, Callable\n",
        "import networkx as nx\n",
        "\n",
        "# Type aliases\n",
        "SynsetName = str\n",
        "BeamElement = Tuple[Tuple[SynsetName, str], Tuple[SynsetName, str], float]\n",
        "GetNewBeamsFn = Callable[[nx.DiGraph, SynsetName, SynsetName], List[BeamElement]]\n",
        "TopKBranchFn = Callable[[List[List], object, int], List[BeamElement]]\n",
        "\n",
        "# functions moved to EmbeddingHelper.py\n",
        "# These functions are now imported from EmbeddingHelper.py\n",
        "# from EmbeddingHelper import (\n",
        "#     get_top_k_synset_branch_pairs,\n",
        "#     get_top_k_aligned_lex_rel_pairs,\n",
        "#     get_embedding_similarities,\n",
        "#     get_synset_embedding_centroid,\n",
        "#     embed_lexical_relations,\n",
        "#     get_all_neighbors,\n",
        "#     get_noun_neighbors,\n",
        "#     get_verb_neighbors,\n",
        "#     wn_to_nx,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cmuVbqxY6ot"
      },
      "source": [
        "# Beam Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25u1o6wq5req"
      },
      "outputs": [],
      "source": [
        "#  Note: beam construction and search functions have been moved to BeamSearch.py\n",
        "# from BeamSearch import (\n",
        "#     get_new_beams,\n",
        "#     get_top_k_branches,\n",
        "#     get_top_k_branches_depth_limited,\n",
        "#     get_top_k_branches_depth_limited_lexical,\n",
        "#     get_top_k_branches_depth_limited_embedding,\n",
        "#     get_top_k_branches_depth_limited_embedding_lexical,\n",
        "#     get_top_k_branches_depth_limited_embedding_lexical_wn,\n",
        "#     get_top_k_branches_depth_limited_embedding_lexical_wn_lexical,\n",
        "#     get_top_k_branches_depth_limited_embedding_lexical_wn_embedding,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTYjzwULZCJd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1WdFnm55oTZ"
      },
      "source": [
        "# Pathing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n34jblYV2SSB"
      },
      "source": [
        "## PairwiseBidirectionalAStar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBK1W1I-Eqsv"
      },
      "outputs": [],
      "source": [
        "# Note: PairWiseBidirectionalAStar has been moved to PairWiseBidirectionalAStar.py\n",
        "# from PairWiseBidirectionalAStar import PairWiseBidirectionalAStar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRn74UEhEhB_"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Yar9RUwGqcJc"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Refactored top_k_branch_wrapper using new embedding functions\n",
        "# -------------------------\n",
        "def top_k_branch_wrapper(\n",
        "    candidates_lists: List[List],\n",
        "    target_synset_or_list,\n",
        "    beam_width: int = 3,\n",
        "    model=None,  # embedding model\n",
        "    wn_module=None  # WordNet module\n",
        ") -> List[BeamElement]:\n",
        "    \"\"\"\n",
        "    Adaptation so that find_connected_paths can call a branch-ranking function\n",
        "    that uses the new embedding-based alignment functions.\n",
        "\n",
        "    Args:\n",
        "      candidates_lists: list of lists of wn.synset objects (from gloss tokens)\n",
        "      target_synset_or_list: either a single wn.synset or a list of target synsets\n",
        "      beam_width: number of top pairs to return\n",
        "      model: embedding model (e.g., Word2Vec, GloVe)\n",
        "      wn_module: WordNet module\n",
        "    Returns:\n",
        "      List of beam elements ((synset_name, rel), (target_syn_name, rel), sim)\n",
        "    \"\"\"\n",
        "    if model is None or wn_module is None:\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Choose a single representative target synset to align against.\n",
        "    # If a list is provided, pick the first (you can change this strategy).\n",
        "    if isinstance(target_synset_or_list, (list, tuple)):\n",
        "        if not target_synset_or_list:\n",
        "            return []\n",
        "        target_syn = target_synset_or_list[0]\n",
        "    else:\n",
        "        target_syn = target_synset_or_list\n",
        "\n",
        "    if target_syn is None:\n",
        "        return []\n",
        "\n",
        "    # Import the embedding functions (assuming they're available)\n",
        "    from embedding_helpers import (\n",
        "        embed_lexical_relations,\n",
        "        get_top_k_aligned_lex_rel_pairs\n",
        "    )\n",
        "\n",
        "    # Precompute target synset relation embeddings\n",
        "    tgt_emb_dict = embed_lexical_relations(target_syn, model)\n",
        "\n",
        "    # Relation maps (same defaults used by the embedding-beam adapter)\n",
        "    asymm_map = {\n",
        "        \"hypernyms\": \"hyponyms\",\n",
        "        \"hyponyms\": \"hypernyms\",\n",
        "        \"part_meronyms\": \"part_holonyms\",\n",
        "        \"member_meronyms\": \"member_holonyms\",\n",
        "        \"substance_meronyms\": \"substance_holonyms\",\n",
        "        \"entailments\": \"causes\",\n",
        "        \"causes\": \"entailments\",\n",
        "    }\n",
        "    symm_map = {\n",
        "        \"hypernyms\": \"hypernyms\",\n",
        "        \"hyponyms\": \"hyponyms\",\n",
        "        \"part_meronyms\": \"part_meronyms\",\n",
        "        \"member_meronyms\": \"member_meronyms\",\n",
        "        \"also_sees\": \"also_sees\",\n",
        "        \"verb_groups\": \"verb_groups\",\n",
        "    }\n",
        "\n",
        "    # For each candidate synset in every candidate list, compute its lexical-relation embeddings\n",
        "    # and align to the target's relation embeddings using get_top_k_aligned_lex_rel_pairs\n",
        "    for cand_list in candidates_lists:\n",
        "        for cand_syn in cand_list:\n",
        "            try:\n",
        "                src_emb_dict = embed_lexical_relations(cand_syn, model)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            # get top pairs from both maps\n",
        "            try:\n",
        "                asymm_results = get_top_k_aligned_lex_rel_pairs(\n",
        "                    asymm_map, src_emb_dict, tgt_emb_dict, beam_width=beam_width\n",
        "                )\n",
        "                results.extend(asymm_results)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            try:\n",
        "                symm_results = get_top_k_aligned_lex_rel_pairs(\n",
        "                    symm_map, src_emb_dict, tgt_emb_dict, beam_width=beam_width\n",
        "                )\n",
        "                results.extend(symm_results)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # Sort aggregated results by similarity and return top beam_width\n",
        "    results.sort(key=lambda x: x[2], reverse=True)\n",
        "    return results[:beam_width]\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# High-level triple search (subject -> predicate -> object)\n",
        "# -------------------------\n",
        "def find_connected_paths(\n",
        "    g: nx.DiGraph,\n",
        "    subject_word: str,\n",
        "    predicate_word: str,\n",
        "    object_word: str,\n",
        "    wn_module,                # your WordNet interface (e.g., nltk.corpus.wordnet or custom)\n",
        "    nlp_func,                 # spaCy NLP call (callable that takes a string -> doc)\n",
        "    get_new_beams_fn=None,\n",
        "    top_k_branch_fn: Optional[TopKBranchFn] = None,\n",
        "    extract_subjects_from_gloss=None,\n",
        "    extract_objects_from_gloss=None,\n",
        "    beam_width: int = 3,\n",
        "    max_depth: int = 8,\n",
        "    max_self_intersection: int = 5,\n",
        "    max_results_per_pair: int = 3,\n",
        "    len_tolerance: int = 1,\n",
        "    relax_beam: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Find connected subject->predicate->object paths.\n",
        "\n",
        "    Strategy:\n",
        "      - Get synsets for subject (nouns), predicate (verbs), object (nouns).\n",
        "      - For each candidate predicate synset:\n",
        "          - Build gloss seeds for subject->predicate side (use predicate gloss to extract candidate subject tokens)\n",
        "          - Build gloss seeds for predicate->object side (use predicate gloss to extract object tokens)\n",
        "          - Run PairwiseBidirectionalAStar twice (subj->pred and pred->obj), requesting multiple paths each\n",
        "          - Combine pairs of returned paths that share the predicate synset and meet intersection constraints\n",
        "      - Return sorted list of connected path triples\n",
        "    \"\"\"\n",
        "\n",
        "    # get word synsets\n",
        "    subject_synsets = wn_module.synsets(subject_word, pos=wn_module.NOUN)\n",
        "    predicate_synsets = wn_module.synsets(predicate_word, pos=wn_module.VERB)\n",
        "    object_synsets = wn_module.synsets(object_word, pos=wn_module.NOUN)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # helper: safe top_k_branch function fallback (if not provided)\n",
        "    def _default_top_k_branch_fn(candidates_lists, target_synset, beam_width_inner=3):\n",
        "        # fallback: return empty (user should inject a real one)\n",
        "        return []\n",
        "\n",
        "    top_k_branch_fn_used = top_k_branch_fn or _default_top_k_branch_fn\n",
        "\n",
        "    for pred_syn in predicate_synsets:\n",
        "        pred_name = pred_syn.name()\n",
        "\n",
        "        # ------------------------\n",
        "        # Build gloss seeds for subj->pred\n",
        "        # ------------------------\n",
        "        pred_gloss_doc = nlp_func(pred_syn.definition())\n",
        "\n",
        "        # Extract subject tokens from predicate gloss\n",
        "        active_subject_tokens = []\n",
        "        if extract_subjects_from_gloss:\n",
        "            try:\n",
        "                active_subject_tokens, _ = extract_subjects_from_gloss(pred_gloss_doc)\n",
        "            except Exception:\n",
        "                active_subject_tokens = []\n",
        "\n",
        "        # Convert tokens to candidate synset lists and get top_k branches\n",
        "        subject_candidate_synsets = []\n",
        "        for t in active_subject_tokens:\n",
        "            try:\n",
        "                synsets = wn_module.synsets(t.text, pos=wn_module.NOUN)\n",
        "                if synsets:\n",
        "                    subject_candidate_synsets.append(synsets)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Get top-k synsets that best align with subject synsets\n",
        "        subj_gloss_seed_nodes = set()\n",
        "        if subject_candidate_synsets and top_k_branch_fn_used:\n",
        "            try:\n",
        "                subj_top_k = top_k_branch_fn_used(subject_candidate_synsets, subject_synsets, beam_width)\n",
        "                # Extract synset names from the results\n",
        "                for beam_elem in subj_top_k:\n",
        "                    s_pair = beam_elem[0]  # (synset_name, rel)\n",
        "                    if isinstance(s_pair[0], str):\n",
        "                        subj_gloss_seed_nodes.add(s_pair[0])\n",
        "                    elif hasattr(s_pair[0], 'name'):\n",
        "                        subj_gloss_seed_nodes.add(s_pair[0].name())\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # ------------------------\n",
        "        # Build gloss seeds for pred->obj\n",
        "        # ------------------------\n",
        "        objects_tokens = []\n",
        "        if extract_objects_from_gloss:\n",
        "            try:\n",
        "                objects_tokens = extract_objects_from_gloss(pred_gloss_doc)\n",
        "            except Exception:\n",
        "                objects_tokens = []\n",
        "\n",
        "        # Convert tokens to candidate synset lists\n",
        "        object_candidate_synsets = []\n",
        "        for t in objects_tokens:\n",
        "            try:\n",
        "                synsets = wn_module.synsets(t.text, pos=wn_module.NOUN)\n",
        "                if synsets:\n",
        "                    object_candidate_synsets.append(synsets)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Get top-k synsets that best align with object synsets\n",
        "        obj_gloss_seed_nodes = set()\n",
        "        if object_candidate_synsets and top_k_branch_fn_used:\n",
        "            try:\n",
        "                obj_top_k = top_k_branch_fn_used(object_candidate_synsets, object_synsets, beam_width)\n",
        "                # Extract synset names from the results\n",
        "                for beam_elem in obj_top_k:\n",
        "                    s_pair = beam_elem[0]  # (synset_name, rel)\n",
        "                    if isinstance(s_pair[0], str):\n",
        "                        obj_gloss_seed_nodes.add(s_pair[0])\n",
        "                    elif hasattr(s_pair[0], 'name'):\n",
        "                        obj_gloss_seed_nodes.add(s_pair[0].name())\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Now run pairwise searches:\n",
        "        # For subject->predicate: src = each subject synset, tgt = pred_syn (pred_name)\n",
        "        subject_paths_map = {}  # subj_syn_name -> list of (path, cost)\n",
        "        for subj_syn in subject_synsets:\n",
        "            # Check if cross-POS search\n",
        "            if subj_syn.pos() != pred_syn.pos():\n",
        "                # Cross-POS: rely more on gloss seeds, relax beam constraints\n",
        "                pair_search = PairwiseBidirectionalAStar(\n",
        "                    g=g,\n",
        "                    src=subj_syn.name(),\n",
        "                    tgt=pred_name,\n",
        "                    get_new_beams_fn=None,  # Don't use beam function for cross-POS\n",
        "                    gloss_seed_nodes=subj_gloss_seed_nodes,  # Use gloss seeds\n",
        "                    beam_width=beam_width,\n",
        "                    max_depth=max_depth,\n",
        "                    relax_beam=True  # Allow exploration outside beams\n",
        "                )\n",
        "            else:\n",
        "                # Same-POS: use normal beam search with gloss enrichment\n",
        "                pair_search = PairwiseBidirectionalAStar(\n",
        "                    g=g,\n",
        "                    src=subj_syn.name(),\n",
        "                    tgt=pred_name,\n",
        "                    get_new_beams_fn=get_new_beams_fn,\n",
        "                    gloss_seed_nodes=subj_gloss_seed_nodes,\n",
        "                    beam_width=beam_width,\n",
        "                    max_depth=max_depth,\n",
        "                    relax_beam=relax_beam\n",
        "                )\n",
        "\n",
        "            subj_paths = pair_search.find_paths(\n",
        "                max_results=max_results_per_pair,\n",
        "                len_tolerance=len_tolerance\n",
        "            )\n",
        "            if subj_paths:\n",
        "                subject_paths_map[subj_syn.name()] = subj_paths\n",
        "\n",
        "        # For predicate->object: src = pred_name, tgt = each object synset\n",
        "        object_paths_map = {}  # obj_syn_name -> list of (path, cost)\n",
        "        for obj_syn in object_synsets:\n",
        "            # Check if cross-POS search\n",
        "            if pred_syn.pos() != obj_syn.pos():\n",
        "                # Cross-POS: rely more on gloss seeds, relax beam constraints\n",
        "                pair_search = PairwiseBidirectionalAStar(\n",
        "                    g=g,\n",
        "                    src=pred_name,\n",
        "                    tgt=obj_syn.name(),\n",
        "                    get_new_beams_fn=None,  # Don't use beam function for cross-POS\n",
        "                    gloss_seed_nodes=obj_gloss_seed_nodes,  # Use gloss seeds\n",
        "                    beam_width=beam_width,\n",
        "                    max_depth=max_depth,\n",
        "                    relax_beam=True  # Allow exploration outside beams\n",
        "                )\n",
        "            else:\n",
        "                # Same-POS: use normal beam search with gloss enrichment\n",
        "                pair_search = PairwiseBidirectionalAStar(\n",
        "                    g=g,\n",
        "                    src=pred_name,\n",
        "                    tgt=obj_syn.name(),\n",
        "                    get_new_beams_fn=get_new_beams_fn,\n",
        "                    gloss_seed_nodes=obj_gloss_seed_nodes,\n",
        "                    beam_width=beam_width,\n",
        "                    max_depth=max_depth,\n",
        "                    relax_beam=relax_beam\n",
        "                )\n",
        "\n",
        "            obj_paths = pair_search.find_paths(\n",
        "                max_results=max_results_per_pair,\n",
        "                len_tolerance=len_tolerance\n",
        "            )\n",
        "            if obj_paths:\n",
        "                object_paths_map[obj_syn.name()] = obj_paths\n",
        "\n",
        "        # Combine: for any subj_path and obj_path that both go through `pred_name`,\n",
        "        # produce combined results if intersection is low enough.\n",
        "        for subj_syn_name, subj_paths in subject_paths_map.items():\n",
        "            for subj_path, subj_cost in subj_paths:\n",
        "                for obj_syn_name, obj_paths in object_paths_map.items():\n",
        "                    for obj_path, obj_cost in obj_paths:\n",
        "                        # Make sure paths connect through the predicate\n",
        "                        if subj_path[-1] == pred_name and obj_path[0] == pred_name:\n",
        "                            # Check intersection to avoid tautological results\n",
        "                            intersection_size = len(set(subj_path).intersection(set(obj_path)))\n",
        "                            if intersection_size <= max_self_intersection:\n",
        "                                # Combined path: subject_path + object_path[1:] (drop duplicate predicate)\n",
        "                                combined_path = subj_path + obj_path[1:]\n",
        "                                combined_cost = subj_cost + obj_cost\n",
        "                                combined_len = len(combined_path)\n",
        "                                results.append({\n",
        "                                    \"predicate_synset\": pred_name,\n",
        "                                    \"subject_path\": subj_path,\n",
        "                                    \"object_path\": obj_path,\n",
        "                                    \"combined_path\": combined_path,\n",
        "                                    \"combined_cost\": combined_cost,\n",
        "                                    \"combined_len\": combined_len,\n",
        "                                })\n",
        "\n",
        "    # sort results by combined_len then cost\n",
        "    results = sorted(results, key=lambda r: (r[\"combined_len\"], r[\"combined_cost\"]))\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wli_MPyM2OvY"
      },
      "source": [
        "## Gloss Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mblVDdWG3Fg"
      },
      "outputs": [],
      "source": [
        "# Note: Gloss parsing functions have been moved to GlossParser.py\n",
        "# from GlossParser import (\n",
        "#     extract_subjects_from_gloss,\n",
        "#     extract_objects_from_gloss,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5FzdxGNHUlR"
      },
      "outputs": [],
      "source": [
        "# Note: old cross-POS search functions have been moved to CrossPOSSearch.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ7mH1XuhtBN"
      },
      "outputs": [],
      "source": [
        "# Note: old wrapper functions have been moved to SemanticDecomposer.py\n",
        "# from SemanticDecomposer import (\n",
        "#     top_k_branch_wrapper,\n",
        "#     find_connected_paths,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwShzrxtIhOh"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "c1XSDI7phuvN"
      },
      "outputs": [],
      "source": [
        "g = wn_to_nx()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qMzn_CMzk0s"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "mDjnq9s_w29o",
        "outputId": "6a013290-7f4c-49e8-be8c-1bcb056051cd"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "find_connected_paths() got an unexpected keyword argument 'extract_verbs_from_gloss'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-456310539.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Example call of find_connected_paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m results = find_connected_paths(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msubject_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dog\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: find_connected_paths() got an unexpected keyword argument 'extract_verbs_from_gloss'"
          ]
        }
      ],
      "source": [
        "# ------------------------------\n",
        "# Example call of find_connected_paths\n",
        "# ------------------------------\n",
        "results = find_connected_paths(\n",
        "    g=g,\n",
        "    subject_word=\"dog\",\n",
        "    predicate_word=\"chase\",\n",
        "    object_word=\"cat\",\n",
        "    wn_module=wn,\n",
        "    nlp_func=nlp,\n",
        "    get_new_beams_fn=lambda G, s, t: get_new_beams_from_embeddings(\n",
        "        G, s, t, wn_module=wn, model=embedding_model, beam_width=4\n",
        "    ),\n",
        "    top_k_branch_fn=lambda candidates, target, bw: top_k_branch_wrapper(\n",
        "        candidates, target, beam_width=bw, model=embedding_model, wn_module=wn\n",
        "    ),\n",
        "    extract_subjects_from_gloss=extract_subjects_from_gloss,\n",
        "    extract_objects_from_gloss=extract_objects_from_gloss,\n",
        "    extract_verbs_from_gloss=extract_verbs_from_gloss,  # This parameter is now included\n",
        "    beam_width=10,\n",
        "    max_depth=10,\n",
        "    max_self_intersection=5,\n",
        "    max_results_per_pair=10,\n",
        "    len_tolerance=5,\n",
        "    relax_beam=True,\n",
        "    max_sample_size=5  # Also added this parameter for controlling gloss token sampling\n",
        ")\n",
        "\n",
        "# Print the top results\n",
        "for idx, r in enumerate(results[:10], start=1):\n",
        "    print(f\"Result #{idx}\")\n",
        "    print(\" Predicate synset:\", r[\"predicate_synset\"])\n",
        "    print(\" Combined length:\", r[\"combined_len\"])\n",
        "    print(\" Combined cost:\", r[\"combined_cost\"])\n",
        "    print(\" Combined path:\")\n",
        "    print(\"  -> \".join(r[\"combined_path\"]))\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfiM249sw_1s",
        "outputId": "b99d62a1-99ba-4009-93ad-a841bd348391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dvVuRcGtJdN",
        "outputId": "cd4edd85-b41b-42dc-976a-56d4e7452bee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Diagnostic start ===\n",
            "src: dog.n.01\n",
            "tgt: cat.n.01\n",
            "\n",
            "1) Node presence checks:\n",
            " g has src node? True\n",
            " g has tgt node? True\n",
            "\n",
            "2) Connectivity check using networkx:\n",
            " Undirected connectivity between src and tgt? True\n",
            " Example undirected shortest path (hop count): 3 hops. path sample: ['dog.n.01', 'domestic_animal.n.01', 'domestic_cat.n.01', 'cat.n.01']\n",
            "\n",
            "3) Node neighborhood:\n",
            " src out-neighbors (sample): ['canis.n.01', 'pack.n.06', 'flag.n.07', 'domestic_animal.n.01', 'canine.n.02', 'corgi.n.01', 'leonberg.n.01', 'cur.n.01', 'poodle.n.01', 'pug.n.01']\n",
            " tgt in-neighbors (sample): ['feline.n.01', 'domestic_cat.n.01', 'wildcat.n.03']\n",
            "\n",
            "4) Embedding beam seeding (get_new_beams_fn):\n",
            " beams count: 6\n",
            "[(('canine.n.02', 'hypernyms'),\n",
            "  ('feline.n.01', 'hypernyms'),\n",
            "  0.6772804990703681),\n",
            " (('domestic_animal.n.01', 'hypernyms'),\n",
            "  ('domestic_cat.n.01', 'hyponyms'),\n",
            "  0.6423323762382909),\n",
            " (('pooch.n.01', 'hyponyms'), ('feline.n.01', 'hypernyms'), 0.634097330718018),\n",
            " (('canine.n.02', 'hypernyms'),\n",
            "  ('domestic_cat.n.01', 'hyponyms'),\n",
            "  0.6148699834455494),\n",
            " (('pug.n.01', 'hyponyms'), ('feline.n.01', 'hypernyms'), 0.5972350530420178),\n",
            " (('newfoundland.n.01', 'hyponyms'),\n",
            "  ('feline.n.01', 'hypernyms'),\n",
            "  0.5958320114102831)]\n",
            "\n",
            "5) Gloss seeds (from predicate gloss):\n",
            " gloss seeds count: 1\n",
            " seeds sample: ['mammal.n.01']\n",
            "\n",
            "6) Run PairwiseBidirectionalAStar with relaxed settings (beam_width=10, relax_beam=True, max_depth=10)\n",
            " found paths (count): 4\n",
            " cost: 3.0  hops: 3  path: ['dog.n.01', 'domestic_animal.n.01', 'domestic_cat.n.01', 'cat.n.01']\n",
            " cost: 4.0  hops: 4  path: ['dog.n.01', 'canine.n.02', 'paw.n.01', 'feline.n.01', 'cat.n.01']\n",
            " cost: 5.0  hops: 5  path: ['dog.n.01', 'canis.n.01', 'mammal_genus.n.01', 'felis.n.01', 'wildcat.n.03', 'cat.n.01']\n",
            " cost: 4.0  hops: 4  path: ['dog.n.01', 'canine.n.02', 'carnivore.n.01', 'feline.n.01', 'cat.n.01']\n",
            "\n",
            "=== Diagnostic end ===\n"
          ]
        }
      ],
      "source": [
        "diagnostic_run(\n",
        "    g=g,\n",
        "    src_name=\"dog.n.01\",\n",
        "    tgt_name=\"cat.n.01\",\n",
        "    wn_module=wn,\n",
        "    nlp_func=nlp,\n",
        "    embedding_model=embedding_model,\n",
        "    get_new_beams_fn_wrapped=lambda G,s,t: get_new_beams_from_embeddings(G,s,t, wn_module=wn, model=embedding_model, beam_width=6),\n",
        "    build_gloss_seed_nodes_fn=lambda pred, wn_mod, nlpfn: build_gloss_seed_nodes_from_predicate(pred, wn_mod, nlpfn,\n",
        "                                                                                              extract_subjects_fn=globals().get('extract_subjects_from_gloss'),\n",
        "                                                                                              extract_objects_fn=globals().get('extract_objects_from_gloss'),\n",
        "                                                                                              extract_verbs_fn=globals().get('extract_verbs_from_gloss'),\n",
        "                                                                                              top_k_branch_fn=lambda cand, target, bw: top_k_branch_wrapper(cand, target, bw, model=embedding_model, wn_module=wn),\n",
        "                                                                                              mode=\"subjects\",\n",
        "                                                                                              max_sample_size=6,\n",
        "                                                                                              beam_width=6),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h0Rj1Ohva9b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsdJQGddzhq4"
      },
      "source": [
        "## Diagnostics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_s3EqGiVvmhC"
      },
      "outputs": [],
      "source": [
        "# Debug / diagnostic script\n",
        "from pprint import pprint\n",
        "import networkx as nx\n",
        "\n",
        "def diagnostic_run(\n",
        "    g, src_name, tgt_name, wn_module, nlp_func, embedding_model,\n",
        "    get_new_beams_fn_wrapped, build_gloss_seed_nodes_fn,\n",
        "    pairwise_class=PairwiseBidirectionalAStar\n",
        "):\n",
        "    print(\"=== Diagnostic start ===\")\n",
        "    print(\"src:\", src_name)\n",
        "    print(\"tgt:\", tgt_name)\n",
        "    print()\n",
        "\n",
        "    # 1) Node presence\n",
        "    print(\"1) Node presence checks:\")\n",
        "    print(\" g has src node?\", g.has_node(src_name))\n",
        "    print(\" g has tgt node?\", g.has_node(tgt_name))\n",
        "    if not g.has_node(src_name) or not g.has_node(tgt_name):\n",
        "        print(\" -> Node name mismatch likely. Check g.nodes() samples:\")\n",
        "        print(\"  sample nodes:\", list(g.nodes)[:20])\n",
        "        print(\"Stopping diagnostics early (node mismatch).\")\n",
        "        return\n",
        "\n",
        "    # 2) Connectivity check (fast)\n",
        "    print(\"\\n2) Connectivity check using networkx:\")\n",
        "    try:\n",
        "        has_path = nx.has_path(g.to_undirected(), src_name, tgt_name)\n",
        "    except Exception:\n",
        "        # graceful fallback for directed graphs if conversion fails\n",
        "        has_path = nx.has_path(g, src_name, tgt_name) if nx.is_directed(g) else False\n",
        "    print(\" Undirected connectivity between src and tgt?\", has_path)\n",
        "    if not has_path:\n",
        "        print(\" -> Graph disconnected between src and tgt. Try increasing graph connectivity or check edges.\")\n",
        "    else:\n",
        "        try:\n",
        "            sp = nx.shortest_path(g.to_undirected(), src_name, tgt_name)\n",
        "            print(\" Example undirected shortest path (hop count):\", len(sp)-1, \"hops. path sample:\", sp[:10])\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # 3) print node degrees and first neighbors\n",
        "    print(\"\\n3) Node neighborhood:\")\n",
        "    try:\n",
        "        nbrs_src = list(g.neighbors(src_name))[:10]\n",
        "    except Exception:\n",
        "        nbrs_src = []\n",
        "    try:\n",
        "        preds_tgt = list(g.predecessors(tgt_name))[:10]\n",
        "    except Exception:\n",
        "        preds_tgt = []\n",
        "    print(\" src out-neighbors (sample):\", nbrs_src)\n",
        "    print(\" tgt in-neighbors (sample):\", preds_tgt)\n",
        "\n",
        "    # 4) Beam seeding via embeddings\n",
        "    print(\"\\n4) Embedding beam seeding (get_new_beams_fn):\")\n",
        "    try:\n",
        "        beams = get_new_beams_fn_wrapped(g, src_name, tgt_name)\n",
        "        print(\" beams count:\", len(beams))\n",
        "        pprint(beams[:10])\n",
        "    except Exception as e:\n",
        "        print(\" get_new_beams_fn raised exception:\", e)\n",
        "        beams = []\n",
        "\n",
        "    # 5) Gloss seed extraction for predicate (if available)\n",
        "    print(\"\\n5) Gloss seeds (from predicate gloss):\")\n",
        "    try:\n",
        "        # attempt to get a predicate synset if tgt is a predicate; otherwise try wn.synset(tgt)\n",
        "        try:\n",
        "            pred_syn = wn_module.synset(tgt_name)\n",
        "        except Exception:\n",
        "            pred_syn = None\n",
        "        seeds = set()\n",
        "        if pred_syn:\n",
        "            seeds = build_gloss_seed_nodes_fn(pred_syn, wn_module, nlp_func)\n",
        "        print(\" gloss seeds count:\", len(seeds))\n",
        "        print(\" seeds sample:\", list(seeds)[:10])\n",
        "    except Exception as e:\n",
        "        print(\" gloss seeding raised exception:\", e)\n",
        "        seeds = set()\n",
        "\n",
        "    # 6) Run pairwise search with relaxed settings (wider beam, relax_beam True)\n",
        "    print(\"\\n6) Run PairwiseBidirectionalAStar with relaxed settings (beam_width=10, relax_beam=True, max_depth=10)\")\n",
        "    try:\n",
        "        search = pairwise_class(\n",
        "            g=g,\n",
        "            src=src_name,\n",
        "            tgt=tgt_name,\n",
        "            get_new_beams_fn=lambda G, s, t: get_new_beams_fn_wrapped(G, s, t),\n",
        "            gloss_seed_nodes=seeds,\n",
        "            beam_width=10,\n",
        "            max_depth=10,\n",
        "            relax_beam=True\n",
        "        )\n",
        "        paths = search.find_paths(max_results=5, len_tolerance=2)\n",
        "        print(\" found paths (count):\", len(paths))\n",
        "        for p, cost in paths:\n",
        "            print(\" cost:\", cost, \" hops:\", len(p)-1, \" path:\", p)\n",
        "    except Exception as e:\n",
        "        print(\" pairwise search raised exception:\", e)\n",
        "\n",
        "    print(\"\\n=== Diagnostic end ===\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMdif105pphKClN5rrNxfp+",
      "collapsed_sections": [
        "2J9B-9RI5lRP",
        "H6Ahax9MciBc",
        "7cmuVbqxY6ot"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

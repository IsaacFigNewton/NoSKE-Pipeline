{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2J9B-9RI5lRP",
        "H6Ahax9MciBc",
        "7cmuVbqxY6ot"
      ],
      "authorship_tag": "ABX9TyMdif105pphKClN5rrNxfp+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/SMIED/blob/adding-semantic-decomposition/BeamSemantic_Decomposition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "2J9B-9RI5lRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "5vww_V3sIQzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "GReqNuQt7wvb",
        "outputId": "b84c4bfa-f50a-4d21-ae3b-ec600dbbd84b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "88c10c4602df47e08f32a1032842365a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import, config stuff"
      ],
      "metadata": {
        "id": "dSlaJDUEPnfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Semantic decomposition of (\"cat\", \"eats\", \"mouse\") using WordNet + spaCy + depth-limited GBFS.\n",
        "- Uses spaCy to parse verb synset glosses and detect subject/object dependencies.\n",
        "- If both subject and object tokens are present, branches directly toward original triple synsets.\n",
        "- Otherwise falls back to WordNet relations.\n",
        "\"\"\"\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from heapq import heappush, heappop\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gensim.downloader as api\n",
        "from collections import deque\n",
        "import heapq\n",
        "import itertools\n",
        "from typing import Callable, Dict, Iterable, List, Optional, Set, Tuple\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "Jb_gtRCvh-DF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "# Load spaCy English model for dependency parsing\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj4iZlAfh_t8",
        "outputId": "98757067-9eb9-4087-de40-587ab62654df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5TGC7I88EvX",
        "outputId": "c1befd6a-cb91-4e37-c749-febfa801f277"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Type aliases\n",
        "SynsetName = str  # e.g., \"dog.n.01\"\n",
        "Path = List[SynsetName]\n",
        "BeamElement = Tuple[Tuple[SynsetName, str], Tuple[SynsetName, str], float]\n",
        "GetNewBeamsFn = Callable[[nx.DiGraph, SynsetName, SynsetName], List[BeamElement]]\n",
        "TopKBranchFn = Callable[[List[List], object, int], List[BeamElement]]"
      ],
      "metadata": {
        "id": "7x7t4YeLHwid"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpers"
      ],
      "metadata": {
        "id": "H6Ahax9MciBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wn_to_nx():\n",
        "    # Initialize directed graph\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    synset_rels = {\n",
        "        # holonyms\n",
        "        \"part_holonyms\": lambda x: x.part_holonyms(),\n",
        "        \"substance_holonyms\": lambda x: x.substance_holonyms(),\n",
        "        \"member_holonyms\": lambda x: x.member_holonyms(),\n",
        "\n",
        "        # meronyms\n",
        "        \"part_meronyms\": lambda x: x.part_meronyms(),\n",
        "        \"substance_meronyms\": lambda x: x.substance_meronyms(),\n",
        "        \"member_meronyms\": lambda x: x.member_meronyms(),\n",
        "\n",
        "        # other\n",
        "        \"hypernyms\": lambda x: x.hypernyms(),\n",
        "        \"hyponyms\": lambda x: x.hyponyms(),\n",
        "        \"entailments\": lambda x: x.entailments(),\n",
        "        \"causes\": lambda x: x.causes(),\n",
        "        \"also_sees\": lambda x: x.also_sees(),\n",
        "        \"verb_groups\": lambda x: x.verb_groups(),\n",
        "    }\n",
        "\n",
        "    # add nodes (synsets) and all their edges (lexical relations) to the nx graph\n",
        "    for synset in wn.all_synsets():\n",
        "        for rel_name, rel_func in synset_rels.items():\n",
        "            for target in rel_func(synset):\n",
        "                G.add_edge(\n",
        "                    synset.name(),\n",
        "                    target.name(),\n",
        "                    relation = rel_name[:-1]\n",
        "                )\n",
        "    return G"
      ],
      "metadata": {
        "id": "ZkUj-vdOivAP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_neighbors(synset: wn.synset) -> List[wn.synset]:\n",
        "    \"\"\"Get all neighbors of a synset based on its POS.\"\"\"\n",
        "    neighbors = []\n",
        "\n",
        "    # Add hypernyms and hyponyms\n",
        "    neighbors.extend(synset.hypernyms())\n",
        "    neighbors.extend(synset.hyponyms())\n",
        "\n",
        "    # Add POS-specific neighbors\n",
        "    if synset.pos() == 'n':\n",
        "        neighbors.extend(get_noun_neighbors(synset))\n",
        "    else:\n",
        "        neighbors.extend(get_verb_neighbors(synset))\n",
        "\n",
        "    return neighbors\n",
        "\n",
        "\n",
        "def get_noun_neighbors(syn: wn.synset):\n",
        "    \"\"\"Get neighbors for a noun synset.\"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.part_meronyms())\n",
        "    nbrs.update(syn.substance_meronyms())\n",
        "    nbrs.update(syn.member_meronyms())\n",
        "    nbrs.update(syn.part_holonyms())\n",
        "    nbrs.update(syn.substance_holonyms())\n",
        "    nbrs.update(syn.member_holonyms())\n",
        "    return list(nbrs)\n",
        "\n",
        "\n",
        "def get_verb_neighbors(syn: wn.synset):\n",
        "    \"\"\"Get neighbors for a verb synset.\"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.entailments())\n",
        "    nbrs.update(syn.causes())\n",
        "    nbrs.update(syn.also_sees())\n",
        "    nbrs.update(syn.verb_groups())\n",
        "    return list(nbrs)"
      ],
      "metadata": {
        "id": "xoGoYrlVcjiG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Helpers"
      ],
      "metadata": {
        "id": "oL_kkfMUY1bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Embedding-based Helper Functions\n",
        "# ============================================================================\n",
        "\n",
        "def get_synset_embedding_centroid(synset, model) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Given a wn.synset, compute centroid (mean) of embeddings for lemmas.\n",
        "    Returns empty np.array if nothing found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        lemmas = [lemma.name().lower().replace(\"_\", \" \") for lemma in synset.lemmas()]\n",
        "        embeddings = []\n",
        "        for lemma in lemmas:\n",
        "            if lemma in model:\n",
        "                embeddings.append(np.asarray(model[lemma], dtype=float))\n",
        "            elif lemma.replace(\" \", \"_\") in model:\n",
        "                embeddings.append(np.asarray(model[lemma.replace(\" \", \"_\")], dtype=float))\n",
        "            elif \" \" in lemma:\n",
        "                # try individual words\n",
        "                words = lemma.split()\n",
        "                word_embs = [np.asarray(model[w], dtype=float) for w in words if w in model]\n",
        "                if word_embs:\n",
        "                    embeddings.append(np.mean(word_embs, axis=0))\n",
        "        if not embeddings:\n",
        "            return np.array([])  # empty\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    except Exception as e:\n",
        "        # defensive: return empty arr on any error\n",
        "        return np.array([])\n",
        "\n",
        "\n",
        "def embed_lexical_relations(synset, model) -> Dict[str, List[Tuple[SynsetName, np.ndarray]]]:\n",
        "    \"\"\"\n",
        "    Return map: lexical_rel_name -> list of (synset_name, centroid ndarray)\n",
        "    Filters out relations whose centroid is empty.\n",
        "    \"\"\"\n",
        "    def _rel_centroids(get_attr):\n",
        "        try:\n",
        "            items = []\n",
        "            for s in get_attr(synset):\n",
        "                cent = get_synset_embedding_centroid(s, model)\n",
        "                if cent.size > 0:\n",
        "                    items.append((s.name(), cent))\n",
        "            return items\n",
        "        except Exception:\n",
        "            return []\n",
        "\n",
        "    return {\n",
        "        \"part_holonyms\": _rel_centroids(lambda x: x.part_holonyms()),\n",
        "        \"substance_holonyms\": _rel_centroids(lambda x: x.substance_holonyms()),\n",
        "        \"member_holonyms\": _rel_centroids(lambda x: x.member_holonyms()),\n",
        "        \"part_meronyms\": _rel_centroids(lambda x: x.part_meronyms()),\n",
        "        \"substance_meronyms\": _rel_centroids(lambda x: x.substance_meronyms()),\n",
        "        \"member_meronyms\": _rel_centroids(lambda x: x.member_meronyms()),\n",
        "        \"hypernyms\": _rel_centroids(lambda x: x.hypernyms()),\n",
        "        \"hyponyms\": _rel_centroids(lambda x: x.hyponyms()),\n",
        "        \"entailments\": _rel_centroids(lambda x: x.entailments()),\n",
        "        \"causes\": _rel_centroids(lambda x: x.causes()),\n",
        "        \"also_sees\": _rel_centroids(lambda x: x.also_sees()),\n",
        "        \"verb_groups\": _rel_centroids(lambda x: x.verb_groups()),\n",
        "    }\n",
        "\n",
        "\n",
        "def get_embedding_similarities(rel_embs_1: List[Tuple[str, np.ndarray]],\n",
        "                              rel_embs_2: List[Tuple[str, np.ndarray]]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Return cosine similarity matrix (m x n) for lists of (name, centroid).\n",
        "    If either is empty, returns empty (0xN or Mx0) array.\n",
        "    \"\"\"\n",
        "    if not rel_embs_1 or not rel_embs_2:\n",
        "        return np.zeros((0, 0))\n",
        "\n",
        "    e1 = np.array([x[1] for x in rel_embs_1], dtype=float)  # (m,d)\n",
        "    e2 = np.array([x[1] for x in rel_embs_2], dtype=float)  # (n,d)\n",
        "\n",
        "    # avoid divide-by-zero: replace zero norms with eps\n",
        "    e1_norms = np.linalg.norm(e1, axis=1, keepdims=True)\n",
        "    e2_norms = np.linalg.norm(e2, axis=1, keepdims=True)\n",
        "    e1_norms[e1_norms == 0] = 1e-8\n",
        "    e2_norms[e2_norms == 0] = 1e-8\n",
        "\n",
        "    e1u = e1 / e1_norms\n",
        "    e2u = e2 / e2_norms\n",
        "\n",
        "    sims = np.dot(e1u, e2u.T)\n",
        "    return sims\n",
        "\n",
        "\n",
        "def get_top_k_aligned_lex_rel_pairs(\n",
        "    src_tgt_rel_map: Dict[str, str],\n",
        "    src_emb_dict: Dict[str, List[Tuple[SynsetName, np.ndarray]]],\n",
        "    tgt_emb_dict: Dict[str, List[Tuple[SynsetName, np.ndarray]]],\n",
        "    beam_width: int = 3,\n",
        ") -> List[BeamElement]:\n",
        "    \"\"\"\n",
        "    src_tgt_rel_map: mapping from relation name in src to relation name in tgt,\n",
        "      e.g., {'hypernyms': 'hyponyms', ...}\n",
        "\n",
        "    Returns list of ((src_syn_name, src_rel), (tgt_syn_name, tgt_rel), similarity)\n",
        "    \"\"\"\n",
        "    rel_sims = []\n",
        "    for e1_rel, e2_rel in src_tgt_rel_map.items():\n",
        "        e1_list = src_emb_dict.get(e1_rel, [])\n",
        "        e2_list = tgt_emb_dict.get(e2_rel, [])\n",
        "        if not e1_list or not e2_list:\n",
        "            continue\n",
        "        sims = get_embedding_similarities(e1_list, e2_list)  # shape (m,n)\n",
        "        if sims.size == 0:\n",
        "            continue\n",
        "        for i in range(sims.shape[0]):\n",
        "            for j in range(sims.shape[1]):\n",
        "                try:\n",
        "                    rel_sims.append(((e1_list[i][0], e1_rel), (e2_list[j][0], e2_rel), float(sims[i, j])))\n",
        "                except IndexError:\n",
        "                    continue\n",
        "\n",
        "    # sort and return top-k\n",
        "    rel_sims.sort(key=lambda x: x[2], reverse=True)\n",
        "    if beam_width <= 0:\n",
        "        return rel_sims\n",
        "    return rel_sims[:beam_width]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Refactored: Embedding-based synset branch ranking\n",
        "# ============================================================================\n",
        "\n",
        "def get_top_k_synset_branch_pairs(\n",
        "    candidates: List[List],  # List of lists of synsets\n",
        "    target_synsets,  # Single synset or list of synsets\n",
        "    beam_width: int = 3,\n",
        "    model=None,  # embedding model\n",
        "    wn_module=None  # WordNet module\n",
        ") -> List[BeamElement]:\n",
        "    \"\"\"\n",
        "    Refactored to use embedding-based alignment instead of get_synset_relatedness.\n",
        "\n",
        "    Given a list of candidate synset lists and target synset(s),\n",
        "    return the k synset-relation pairs most similar to the target.\n",
        "\n",
        "    Returns: List of ((synset_name, lexical_rel), (target_syn_name, lexical_rel), similarity)\n",
        "    \"\"\"\n",
        "    if model is None or wn_module is None:\n",
        "        return []\n",
        "\n",
        "    # Handle target synsets (single or list)\n",
        "    if not isinstance(target_synsets, (list, tuple)):\n",
        "        target_synsets = [target_synsets]\n",
        "\n",
        "    if not target_synsets:\n",
        "        return []\n",
        "\n",
        "    # Relation maps for alignment\n",
        "    asymm_map = {\n",
        "        \"hypernyms\": \"hyponyms\",\n",
        "        \"hyponyms\": \"hypernyms\",\n",
        "        \"part_meronyms\": \"part_holonyms\",\n",
        "        \"member_meronyms\": \"member_holonyms\",\n",
        "        \"substance_meronyms\": \"substance_holonyms\",\n",
        "        \"entailments\": \"causes\",\n",
        "        \"causes\": \"entailments\",\n",
        "    }\n",
        "    symm_map = {\n",
        "        \"hypernyms\": \"hypernyms\",\n",
        "        \"hyponyms\": \"hyponyms\",\n",
        "        \"part_meronyms\": \"part_meronyms\",\n",
        "        \"member_meronyms\": \"member_meronyms\",\n",
        "        \"also_sees\": \"also_sees\",\n",
        "        \"verb_groups\": \"verb_groups\",\n",
        "    }\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    # Process each target synset\n",
        "    for target_syn in target_synsets:\n",
        "        if target_syn is None:\n",
        "            continue\n",
        "\n",
        "        # Precompute target embeddings\n",
        "        tgt_emb_dict = embed_lexical_relations(target_syn, model)\n",
        "\n",
        "        # Process each candidate list\n",
        "        for synset_list in candidates:\n",
        "            if not synset_list:\n",
        "                continue\n",
        "\n",
        "            for synset in synset_list:\n",
        "                if synset is None:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Compute candidate embeddings\n",
        "                    src_emb_dict = embed_lexical_relations(synset, model)\n",
        "\n",
        "                    # Get aligned pairs from asymmetric relations\n",
        "                    asymm_pairs = get_top_k_aligned_lex_rel_pairs(\n",
        "                        asymm_map, src_emb_dict, tgt_emb_dict, beam_width=beam_width\n",
        "                    )\n",
        "                    all_results.extend(asymm_pairs)\n",
        "\n",
        "                    # Get aligned pairs from symmetric relations\n",
        "                    symm_pairs = get_top_k_aligned_lex_rel_pairs(\n",
        "                        symm_map, src_emb_dict, tgt_emb_dict, beam_width=beam_width\n",
        "                    )\n",
        "                    all_results.extend(symm_pairs)\n",
        "\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "    # Sort by similarity and return top-k\n",
        "    all_results.sort(key=lambda x: x[2], reverse=True)\n",
        "    return all_results[:beam_width]"
      ],
      "metadata": {
        "id": "CRWsaZ8JHojT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import Dict, List, Tuple, Optional, Iterable, Set, Callable\n",
        "import networkx as nx\n",
        "\n",
        "# Type aliases\n",
        "SynsetName = str\n",
        "BeamElement = Tuple[Tuple[SynsetName, str], Tuple[SynsetName, str], float]\n",
        "GetNewBeamsFn = Callable[[nx.DiGraph, SynsetName, SynsetName], List[BeamElement]]\n",
        "TopKBranchFn = Callable[[List[List], object, int], List[BeamElement]]\n",
        "\n",
        "# -------------------------\n",
        "# 1) Embedding centroids\n",
        "# -------------------------\n",
        "def get_synset_embedding_centroid(synset, model) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Given a wn.synset, compute centroid (mean) of embeddings for lemmas.\n",
        "    Returns empty np.array if nothing found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        lemmas = [lemma.name().lower().replace(\"_\", \" \") for lemma in synset.lemmas()]\n",
        "        embeddings = []\n",
        "        for lemma in lemmas:\n",
        "            if lemma in model:\n",
        "                embeddings.append(np.asarray(model[lemma], dtype=float))\n",
        "            elif lemma.replace(\" \", \"_\") in model:\n",
        "                embeddings.append(np.asarray(model[lemma.replace(\" \", \"_\")], dtype=float))\n",
        "            elif \" \" in lemma:\n",
        "                # try individual words\n",
        "                words = lemma.split()\n",
        "                word_embs = [np.asarray(model[w], dtype=float) for w in words if w in model]\n",
        "                if word_embs:\n",
        "                    embeddings.append(np.mean(word_embs, axis=0))\n",
        "        if not embeddings:\n",
        "            return np.array([])  # empty\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    except Exception as e:\n",
        "        # defensive: return empty arr on any error\n",
        "        print(f\"[get_synset_embedding_centroid] Error for {getattr(synset, 'name', lambda: synset)()}: {e}\")\n",
        "        return np.array([])\n",
        "\n",
        "# -------------------------\n",
        "# 2) Embed lexical relations\n",
        "# -------------------------\n",
        "def embed_lexical_relations(synset, model) -> Dict[str, List[Tuple[SynsetName, np.ndarray]]]:\n",
        "    \"\"\"\n",
        "    Return map: lexical_rel_name -> list of (synset_name, centroid ndarray)\n",
        "    Filters out relations whose centroid is empty.\n",
        "    \"\"\"\n",
        "    def _rel_centroids(get_attr):\n",
        "        try:\n",
        "            items = []\n",
        "            for s in get_attr(synset):\n",
        "                cent = get_synset_embedding_centroid(s, model)\n",
        "                if cent.size > 0:\n",
        "                    items.append((s.name(), cent))\n",
        "            return items\n",
        "        except Exception as e:\n",
        "            print(f\"[embed_lexical_relations] error for {synset.name()}: {e}\")\n",
        "            return []\n",
        "\n",
        "    return {\n",
        "        \"part_holonyms\": _rel_centroids(lambda x: x.part_holonyms()),\n",
        "        \"substance_holonyms\": _rel_centroids(lambda x: x.substance_holonyms()),\n",
        "        \"member_holonyms\": _rel_centroids(lambda x: x.member_holonyms()),\n",
        "        \"part_meronyms\": _rel_centroids(lambda x: x.part_meronyms()),\n",
        "        \"substance_meronyms\": _rel_centroids(lambda x: x.substance_meronyms()),\n",
        "        \"member_meronyms\": _rel_centroids(lambda x: x.member_meronyms()),\n",
        "        \"hypernyms\": _rel_centroids(lambda x: x.hypernyms()),\n",
        "        \"hyponyms\": _rel_centroids(lambda x: x.hyponyms()),\n",
        "        \"entailments\": _rel_centroids(lambda x: x.entailments()),\n",
        "        \"causes\": _rel_centroids(lambda x: x.causes()),\n",
        "        \"also_sees\": _rel_centroids(lambda x: x.also_sees()),\n",
        "        \"verb_groups\": _rel_centroids(lambda x: x.verb_groups()),\n",
        "    }\n",
        "\n",
        "# -------------------------\n",
        "# 3) Embedding similarities\n",
        "# -------------------------\n",
        "def get_embedding_similarities(rel_embs_1: List[Tuple[str, np.ndarray]], rel_embs_2: List[Tuple[str, np.ndarray]]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Return cosine similarity matrix (m x n) for lists of (name, centroid).\n",
        "    If either is empty, returns empty (0xN or Mx0) array.\n",
        "    \"\"\"\n",
        "    if not rel_embs_1 or not rel_embs_2:\n",
        "        return np.zeros((0, 0))\n",
        "\n",
        "    e1 = np.array([x[1] for x in rel_embs_1], dtype=float)  # (m,d)\n",
        "    e2 = np.array([x[1] for x in rel_embs_2], dtype=float)  # (n,d)\n",
        "\n",
        "    # avoid divide-by-zero: replace zero norms with eps\n",
        "    e1_norms = np.linalg.norm(e1, axis=1, keepdims=True)\n",
        "    e2_norms = np.linalg.norm(e2, axis=1, keepdims=True)\n",
        "    e1_norms[e1_norms == 0] = 1e-8\n",
        "    e2_norms[e2_norms == 0] = 1e-8\n",
        "\n",
        "    e1u = e1 / e1_norms\n",
        "    e2u = e2 / e2_norms\n",
        "\n",
        "    sims = np.dot(e1u, e2u.T)\n",
        "    return sims\n",
        "\n",
        "# -------------------------\n",
        "# 4) Top-K aligned lex relation pairs\n",
        "# -------------------------\n",
        "def get_top_k_aligned_lex_rel_pairs(\n",
        "    src_tgt_rel_map: Dict[str, str],\n",
        "    src_emb_dict: Dict[str, List[Tuple[SynsetName, np.ndarray]]],\n",
        "    tgt_emb_dict: Dict[str, List[Tuple[SynsetName, np.ndarray]]],\n",
        "    beam_width: int = 3,\n",
        ") -> List[BeamElement]:\n",
        "    \"\"\"\n",
        "    src_tgt_rel_map: mapping from relation name in src to relation name in tgt,\n",
        "      e.g., {'hypernyms': 'hyponyms', ...}\n",
        "\n",
        "    Returns list of ((src_syn_name, src_rel), (tgt_syn_name, tgt_rel), similarity)\n",
        "    \"\"\"\n",
        "    rel_sims = []\n",
        "    for e1_rel, e2_rel in src_tgt_rel_map.items():\n",
        "        e1_list = src_emb_dict.get(e1_rel, [])\n",
        "        e2_list = tgt_emb_dict.get(e2_rel, [])\n",
        "        if not e1_list or not e2_list:\n",
        "            continue\n",
        "        sims = get_embedding_similarities(e1_list, e2_list)  # shape (m,n)\n",
        "        if sims.size == 0:\n",
        "            continue\n",
        "        for i in range(sims.shape[0]):\n",
        "            for j in range(sims.shape[1]):\n",
        "                try:\n",
        "                    rel_sims.append(((e1_list[i][0], e1_rel), (e2_list[j][0], e2_rel), float(sims[i, j])))\n",
        "                except IndexError as ex:\n",
        "                    raise IndexError(f\"Index error in get_top_k_aligned_lex_rel_pairs: i={i}, j={j}, shapes e1={len(e1_list)}, e2={len(e2_list)}\") from ex\n",
        "\n",
        "    # sort and return top-k\n",
        "    rel_sims.sort(key=lambda x: x[2], reverse=True)\n",
        "    if beam_width <= 0:\n",
        "        return rel_sims\n",
        "    return rel_sims[:beam_width]\n",
        "\n",
        "# -------------------------\n",
        "# 5) Adapter: get_new_beams_fn for PairwiseBidirectionalAStar\n",
        "# -------------------------\n",
        "def get_new_beams_from_embeddings(\n",
        "    g: nx.DiGraph,\n",
        "    src_name: SynsetName,\n",
        "    tgt_name: SynsetName,\n",
        "    wn_module,\n",
        "    model,\n",
        "    beam_width: int = 3,\n",
        "    asymm_map: Optional[Dict[str, str]] = None,\n",
        "    symm_map: Optional[Dict[str, str]] = None,\n",
        ") -> List[BeamElement]:\n",
        "    \"\"\"\n",
        "    Adapter to produce the beam format expected by PairwiseBidirectionalAStar.\n",
        "    - src_name / tgt_name are synset name strings (e.g., 'dog.n.01')\n",
        "    - wn_module is your WordNet interface (e.g., nltk.corpus.wordnet as wn)\n",
        "    - model is embedding model (contains token -> vector)\n",
        "    \"\"\"\n",
        "    # default relation maps (tweak as needed)\n",
        "    if asymm_map is None:\n",
        "        asymm_map = {\n",
        "            \"hypernyms\": \"hyponyms\",\n",
        "            \"hyponyms\": \"hypernyms\",\n",
        "            \"part_meronyms\": \"part_holonyms\",\n",
        "            \"member_meronyms\": \"member_holonyms\",\n",
        "            \"substance_meronyms\": \"substance_holonyms\",\n",
        "            \"entailments\": \"causes\",\n",
        "            \"causes\": \"entailments\",\n",
        "        }\n",
        "    if symm_map is None:\n",
        "        symm_map = {\n",
        "            \"hypernyms\": \"hypernyms\",\n",
        "            \"hyponyms\": \"hyponyms\",\n",
        "            \"part_meronyms\": \"part_meronyms\",\n",
        "            \"member_meronyms\": \"member_meronyms\",\n",
        "            \"also_sees\": \"also_sees\",\n",
        "            \"verb_groups\": \"verb_groups\",\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        src_syn = wn_module.synset(src_name)\n",
        "        tgt_syn = wn_module.synset(tgt_name)\n",
        "    except Exception:\n",
        "        # If synset names invalid, return empty beams\n",
        "        return []\n",
        "\n",
        "    # Embed lexical relations for both synsets\n",
        "    src_emb_dict = embed_lexical_relations(src_syn, model)\n",
        "    tgt_emb_dict = embed_lexical_relations(tgt_syn, model)\n",
        "\n",
        "    # get top k pairs from asymmetric and symmetric maps\n",
        "    asymm_pairs = get_top_k_aligned_lex_rel_pairs(asymm_map, src_emb_dict, tgt_emb_dict, beam_width=beam_width)\n",
        "    symm_pairs = get_top_k_aligned_lex_rel_pairs(symm_map, src_emb_dict, tgt_emb_dict, beam_width=beam_width)\n",
        "\n",
        "    combined = asymm_pairs + symm_pairs\n",
        "    # sort by similarity and trim to beam_width\n",
        "    combined.sort(key=lambda x: x[2], reverse=True)\n",
        "    return combined[:beam_width]\n",
        "\n",
        "# -------------------------\n",
        "# 6) Gloss seeding helper (optional use of top_k_branch_fn)\n",
        "# -------------------------\n",
        "def build_gloss_seed_nodes_from_predicate(\n",
        "    pred_syn,\n",
        "    wn_module,\n",
        "    nlp_func,\n",
        "    mode: str = \"subjects\",  # \"subjects\" or \"objects\" or \"verbs\"\n",
        "    extract_subjects_fn: Optional[Callable] = None,\n",
        "    extract_objects_fn: Optional[Callable] = None,\n",
        "    extract_verbs_fn: Optional[Callable] = None,\n",
        "    top_k_branch_fn: Optional[TopKBranchFn] = None,\n",
        "    target_synsets: Optional[List] = None,\n",
        "    max_sample_size: int = 5,\n",
        "    beam_width: int = 3,\n",
        ") -> Set[SynsetName]:\n",
        "    \"\"\"\n",
        "    Extract tokens from pred_syn gloss and return a set of synset-name seeds.\n",
        "    If top_k_branch_fn provided, use it to select top-k matching synsets.\n",
        "    - pred_syn: wn.synset\n",
        "    - nlp_func: spaCy call (text -> doc)\n",
        "    - mode: 'subjects'|'objects'|'verbs' decides which extractor to use\n",
        "    \"\"\"\n",
        "    doc = nlp_func(pred_syn.definition())\n",
        "    tokens = []\n",
        "    if mode == \"subjects\" and extract_subjects_fn is not None:\n",
        "        tokens, _ = extract_subjects_fn(doc)\n",
        "    elif mode == \"objects\" and extract_objects_fn is not None:\n",
        "        tokens = extract_objects_fn(doc)\n",
        "    elif mode == \"verbs\" and extract_verbs_fn is not None:\n",
        "        tokens = extract_verbs_fn(doc)\n",
        "    else:\n",
        "        # fallback: use any nouns in doc\n",
        "        tokens = [tok for tok in doc if tok.pos_ == \"NOUN\"]\n",
        "\n",
        "    # candidate synset lists for each token\n",
        "    candidate_synsets = []\n",
        "    for tok in tokens[:max_sample_size]:\n",
        "        try:\n",
        "            cand = wn_module.synsets(tok.text, pos=wn_module.NOUN if mode != \"verbs\" else wn_module.VERB)\n",
        "            candidate_synsets.append(cand)\n",
        "        except Exception:\n",
        "            candidate_synsets.append([])\n",
        "\n",
        "    seeds = set()\n",
        "    if top_k_branch_fn and target_synsets is not None:\n",
        "        # top_k_branch_fn is expected to accept (candidates, target_synset_or_list, beam_width)\n",
        "        top_k = top_k_branch_fn(candidate_synsets[:max_sample_size], target_synsets, beam_width)\n",
        "        for (s_pair, _, _) in top_k:\n",
        "            # s_pair is (synset_obj_or_name, lexical_rel); convert to name if synset object\n",
        "            s = s_pair[0]\n",
        "            if hasattr(s, \"name\"):\n",
        "                seeds.add(s.name())\n",
        "            elif isinstance(s, str):\n",
        "                seeds.add(s)\n",
        "    else:\n",
        "        # conservative: add the first few candidate synsets' names\n",
        "        for cand_list in candidate_synsets:\n",
        "            for s in cand_list[:min(3, len(cand_list))]:\n",
        "                seeds.add(s.name())\n",
        "    return seeds\n"
      ],
      "metadata": {
        "id": "th4qyXLmYH4j"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam Construction"
      ],
      "metadata": {
        "id": "7cmuVbqxY6ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asymmetric_pairs_map = {\n",
        "    # holonyms\n",
        "    \"part_holonyms\": \"part_meronyms\",\n",
        "    \"substance_holonyms\": \"substance_meronyms\",\n",
        "    \"member_holonyms\": \"member_meronyms\",\n",
        "\n",
        "    # meronyms\n",
        "    \"part_meronyms\": \"part_holonyms\",\n",
        "    \"substance_meronyms\": \"substance_holonyms\",\n",
        "    \"member_meronyms\": \"member_holonyms\",\n",
        "\n",
        "    # other\n",
        "    \"hypernyms\": \"hyponyms\",\n",
        "    \"hyponyms\": \"hyponyms\"\n",
        "}\n",
        "\n",
        "\n",
        "symmetric_pairs_map = {\n",
        "    # holonyms\n",
        "    \"part_holonyms\": \"part_holonyms\",\n",
        "    \"substance_holonyms\": \"substance_holonyms\",\n",
        "    \"member_holonyms\": \"member_holonyms\",\n",
        "\n",
        "    # meronyms\n",
        "    \"part_meronyms\": \"part_meronyms\",\n",
        "    \"substance_meronyms\": \"substance_meronyms\",\n",
        "    \"member_meronyms\": \"member_meronyms\",\n",
        "\n",
        "    # other\n",
        "    \"hypernyms\": \"hypernyms\",\n",
        "    \"hyponyms\": \"hyponyms\",\n",
        "    \"entailments\": \"entailments\",\n",
        "    \"causes\": \"causes\",\n",
        "    \"also_sees\": \"also_sees\",\n",
        "    \"verb_groups\": \"verb_groups\"\n",
        "}"
      ],
      "metadata": {
        "id": "25u1o6wq5req"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_new_beams(\n",
        "      g: nx.DiGraph,\n",
        "      src: str,\n",
        "      tgt: str,\n",
        "      model=embedding_model,\n",
        "      beam_width=3\n",
        "    ) -> List[Tuple[\n",
        "        Tuple[str, str],\n",
        "        Tuple[str, str],\n",
        "        float\n",
        "    ]]:\n",
        "    \"\"\"\n",
        "    Get the k closest pairs of lexical relations between 2 synsets.\n",
        "\n",
        "    Args:\n",
        "        src: WordNet Synset object (e.g., 'dog.n.01')\n",
        "        tgt: WordNet Synset object (e.g., 'cat.n.01')\n",
        "        model: token model (if None, will load default)\n",
        "        beam_width: max number of pairs to return\n",
        "\n",
        "    Returns:\n",
        "        List of tuples of the form:\n",
        "          (\n",
        "            (synset1, lexical_rel),\n",
        "            (synset2, lexical_rel),\n",
        "            relatedness\n",
        "          )\n",
        "    \"\"\"\n",
        "\n",
        "    # Build a map of each synset's associated lexical relations\n",
        "    #   and the centroids of their associated synsets\n",
        "    src_lex_rel_embs = embed_lexical_relations(wn.synset(src), model)\n",
        "    tgt_lex_rel_embs = embed_lexical_relations(wn.synset(tgt), model)\n",
        "\n",
        "    # ensure the edges in the nx graph align with those in the embedding maps\n",
        "    src_neighbors = {n for n in g.neighbors(src)}\n",
        "    for rel, synset_list in src_lex_rel_embs.items():\n",
        "      if not all(s[0] in src_neighbors for s in synset_list):\n",
        "        raise ValueError(f\"Not all lexical properties of {src} ({[s[0] for s in synset_list]}) in graph for relation {rel}\")\n",
        "    tgt_neighbors = {n for n in g.neighbors(tgt)}\n",
        "    for rel, synset_list in tgt_lex_rel_embs.items():\n",
        "      if not all(s[0] in tgt_neighbors for s in synset_list):\n",
        "        raise ValueError(f\"Not all lexical properties of {tgt} ({[s[0] for s in synset_list]}) in graph for relation {rel}\")\n",
        "    # in the future, get neighbor relation in node metadata with g.adj[n]\n",
        "\n",
        "    # Get the asymmetric lexical relation pairings,\n",
        "    #   sorted in descending order of embedding similarity\n",
        "    #   e.x. similarity of synset1's hypernyms to synset2's hypernyms\n",
        "    asymm_lex_rel_sims = get_top_k_aligned_lex_rel_pairs(\n",
        "        asymmetric_pairs_map,\n",
        "        src_lex_rel_embs,\n",
        "        tgt_lex_rel_embs,\n",
        "        model,\n",
        "        beam_width\n",
        "    )\n",
        "    # Get the symmetric lexical relation pairings,\n",
        "    #   sorted in descending order of embedding similarity\n",
        "    #   e.x. similarity of synset1's hypernyms to synset2's hypernyms\n",
        "    symm_lex_rel_sims = get_top_k_aligned_lex_rel_pairs(\n",
        "        symmetric_pairs_map,\n",
        "        src_lex_rel_embs,\n",
        "        tgt_lex_rel_embs,\n",
        "        model,\n",
        "        beam_width\n",
        "    )\n",
        "    combined = asymm_lex_rel_sims + symm_lex_rel_sims\n",
        "    beam = sorted(combined, key=lambda x: x[2], reverse=True)[:beam_width]\n",
        "    return beam"
      ],
      "metadata": {
        "id": "iTYjzwULZCJd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pathing"
      ],
      "metadata": {
        "id": "X1WdFnm55oTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PairwiseBidirectionalAStar"
      ],
      "metadata": {
        "id": "n34jblYV2SSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "import itertools\n",
        "from collections import deque\n",
        "from typing import Callable, Dict, Iterable, List, Optional, Set, Tuple\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "# Type aliases\n",
        "SynsetName = str  # e.g., \"dog.n.01\"\n",
        "Path = List[SynsetName]\n",
        "BeamElement = Tuple[Tuple[SynsetName, str], Tuple[SynsetName, str], float]\n",
        "GetNewBeamsFn = Callable[[nx.DiGraph, SynsetName, SynsetName], List[BeamElement]]\n",
        "TopKBranchFn = Callable[[List[List], object, int], List[BeamElement]]\n",
        "GlossSeedFn = Callable[[object], List]  # e.g., (parsed_gloss_doc) -> list of tokens\n",
        "\n",
        "\n",
        "\n",
        "class PairwiseBidirectionalAStar:\n",
        "    \"\"\"\n",
        "    Beam+depth-constrained, gloss-seeded bidirectional A* for pairwise synset search.\n",
        "\n",
        "    Dependencies / injectable functions:\n",
        "      - get_new_beams_fn(g, src, tgt) -> List[((src_node, rel),(tgt_node, rel), sim)]\n",
        "      - top_k_branch_fn(candidates_lists, target_synset, beam_width) -> List[((synset, rel),(name,rel),sim)]\n",
        "      - gloss_seed_fn(gloss_doc) -> list of tokens (subject/object/verb tokens)\n",
        "      - wn and nlp (spaCy) are used by the outer pipeline; this class accepts seeds instead.\n",
        "\n",
        "    Heuristics:\n",
        "      - Embedding similarity -> h = 1 - sim (lower is better)\n",
        "      - Gloss seeds get a small bonus (h -= GLOSS_BONUS)\n",
        "    \"\"\"\n",
        "    GLOSS_BONUS = 0.15  # subtract from h for gloss-seeded nodes (tune)\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        g: nx.DiGraph,\n",
        "        src: SynsetName,\n",
        "        tgt: SynsetName,\n",
        "        get_new_beams_fn: Optional[GetNewBeamsFn] = None,\n",
        "        gloss_seed_nodes: Optional[Iterable[SynsetName]] = None,\n",
        "        beam_width: int = 3,\n",
        "        max_depth: int = 6,\n",
        "        relax_beam: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          g: nx.DiGraph — graph of synsets (nodes as synset names).\n",
        "          src, tgt: synset node ids (strings).\n",
        "          get_new_beams_fn: function to produce embedding-based beam pairs (optional).\n",
        "          gloss_seed_nodes: explicit list of synset names seeded from glosses (optional).\n",
        "          beam_width: beam width for initial seeding (passed to get_new_beams if used).\n",
        "          max_depth: maximum hops allowed (total across both sides; enforced per side).\n",
        "          relax_beam: if True, allow exploring nodes outside the allowed beams.\n",
        "        \"\"\"\n",
        "        self.g = g\n",
        "        self.src = src\n",
        "        self.tgt = tgt\n",
        "        self.get_new_beams_fn = get_new_beams_fn\n",
        "        self.gloss_seed_nodes = set(gloss_seed_nodes) if gloss_seed_nodes else set()\n",
        "        self.beam_width = beam_width\n",
        "        self.max_depth = max_depth\n",
        "        self.relax_beam = relax_beam\n",
        "\n",
        "        # will be set by _build_allowed_and_heuristics\n",
        "        self.src_allowed: Set[SynsetName] = set()\n",
        "        self.tgt_allowed: Set[SynsetName] = set()\n",
        "        self.h_forward: Dict[SynsetName, float] = {}\n",
        "        self.h_backward: Dict[SynsetName, float] = {}\n",
        "\n",
        "        # search state\n",
        "        self._counter = itertools.count()\n",
        "        self.open_f: List[Tuple[float, int, SynsetName]] = []\n",
        "        self.open_b: List[Tuple[float, int, SynsetName]] = []\n",
        "        self.g_f: Dict[SynsetName, float] = {}\n",
        "        self.g_b: Dict[SynsetName, float] = {}\n",
        "        self.depth_f: Dict[SynsetName, int] = {}\n",
        "        self.depth_b: Dict[SynsetName, int] = {}\n",
        "        self.parent_f: Dict[SynsetName, Optional[SynsetName]] = {}\n",
        "        self.parent_b: Dict[SynsetName, Optional[SynsetName]] = {}\n",
        "        self.closed_f: Set[SynsetName] = set()\n",
        "        self.closed_b: Set[SynsetName] = set()\n",
        "\n",
        "    # -------------------------\n",
        "    # Setup: allowed sets & heuristics\n",
        "    # -------------------------\n",
        "    def _build_allowed_and_heuristics(self):\n",
        "        \"\"\"\n",
        "        Build allowed node sets and heuristic maps using get_new_beams_fn and gloss seeds.\n",
        "        - src_allowed/tgt_allowed: union of beam nodes and explicit gloss seeds + src/tgt.\n",
        "        - h_forward/h_backward: h = 1 - sim (embedding), gloss seeds get bonus.\n",
        "        \"\"\"\n",
        "        beams = []\n",
        "        if self.get_new_beams_fn is not None:\n",
        "            try:\n",
        "                beams = self.get_new_beams_fn(self.g, self.src, self.tgt) or []\n",
        "            except Exception:\n",
        "                beams = []\n",
        "\n",
        "        # base allowed sets from embedding beams\n",
        "        src_beam_pairs = [b[0] for b in beams]\n",
        "        tgt_beam_pairs = [b[1] for b in beams]\n",
        "        self.src_allowed = {p[0] for p in src_beam_pairs}\n",
        "        self.tgt_allowed = {p[0] for p in tgt_beam_pairs}\n",
        "\n",
        "        # always include src and tgt\n",
        "        self.src_allowed.add(self.src)\n",
        "        self.tgt_allowed.add(self.tgt)\n",
        "\n",
        "        # Include explicit gloss seeds in allowed sets (boost their priority)\n",
        "        for node in self.gloss_seed_nodes:\n",
        "            # you may want to add heuristics only to one side depending on how seeds were created.\n",
        "            self.src_allowed.add(node)\n",
        "            self.tgt_allowed.add(node)\n",
        "\n",
        "        # Base heuristics from embedding beams\n",
        "        self.h_forward = {}\n",
        "        self.h_backward = {}\n",
        "        for (s_pair, t_pair, sim) in beams:\n",
        "            s_node = s_pair[0]\n",
        "            t_node = t_pair[0]\n",
        "            h_val = max(0.0, 1.0 - float(sim))\n",
        "            # keep smallest h (best sim)\n",
        "            if s_node not in self.h_forward or h_val < self.h_forward[s_node]:\n",
        "                self.h_forward[s_node] = h_val\n",
        "            if t_node not in self.h_backward or h_val < self.h_backward[t_node]:\n",
        "                self.h_backward[t_node] = h_val\n",
        "\n",
        "        # Ensure src/tgt have at least default heuristic\n",
        "        self.h_forward.setdefault(self.src, 0.0)\n",
        "        self.h_backward.setdefault(self.tgt, 0.0)\n",
        "\n",
        "        # Apply gloss bonus: reduce heuristic for gloss seeds (they look more promising)\n",
        "        for node in self.gloss_seed_nodes:\n",
        "            # subtract GLOSS_BONUS but keep >= 0\n",
        "            if node in self.h_forward:\n",
        "                self.h_forward[node] = max(0.0, self.h_forward[node] - self.GLOSS_BONUS)\n",
        "            else:\n",
        "                self.h_forward[node] = max(0.0, 0.5 - self.GLOSS_BONUS)  # default h=0.5 if not in beams\n",
        "\n",
        "            if node in self.h_backward:\n",
        "                self.h_backward[node] = max(0.0, self.h_backward[node] - self.GLOSS_BONUS)\n",
        "            else:\n",
        "                self.h_backward[node] = max(0.0, 0.5 - self.GLOSS_BONUS)\n",
        "\n",
        "    # -------------------------\n",
        "    # Initialization of queues\n",
        "    # -------------------------\n",
        "    def _init_search_state(self):\n",
        "        self._counter = itertools.count()\n",
        "        self.open_f = []\n",
        "        self.open_b = []\n",
        "        self.g_f = {self.src: 0.0}\n",
        "        self.g_b = {self.tgt: 0.0}\n",
        "        self.depth_f = {self.src: 0}\n",
        "        self.depth_b = {self.tgt: 0}\n",
        "        self.parent_f = {self.src: None}\n",
        "        self.parent_b = {self.tgt: None}\n",
        "        self.closed_f = set()\n",
        "        self.closed_b = set()\n",
        "\n",
        "        heapq.heappush(self.open_f, (self.h_forward.get(self.src, 0.0), next(self._counter), self.src))\n",
        "        heapq.heappush(self.open_b, (self.h_backward.get(self.tgt, 0.0), next(self._counter), self.tgt))\n",
        "\n",
        "    # -------------------------\n",
        "    # Utilities\n",
        "    # -------------------------\n",
        "    def _edge_weight(self, u: SynsetName, v: SynsetName) -> float:\n",
        "        try:\n",
        "            return float(self.g[u][v].get(\"weight\", 1.0))\n",
        "        except Exception:\n",
        "            return 1.0\n",
        "\n",
        "    def _allowed_forward(self, node: SynsetName) -> bool:\n",
        "        return self.relax_beam or node in self.src_allowed or node in self.tgt_allowed or node == self.tgt or node == self.src\n",
        "\n",
        "    def _allowed_backward(self, node: SynsetName) -> bool:\n",
        "        return self.relax_beam or node in self.tgt_allowed or node in self.src_allowed or node == self.src or node == self.tgt\n",
        "\n",
        "    # -------------------------\n",
        "    # Expand one node from forward/back\n",
        "    # -------------------------\n",
        "    def _expand_forward_once(self) -> Optional[SynsetName]:\n",
        "        \"\"\"Pop one element from forward open and expand. Return meeting node if found.\"\"\"\n",
        "        while self.open_f:\n",
        "            _, _, current = heapq.heappop(self.open_f)\n",
        "            if current in self.closed_f:\n",
        "                continue\n",
        "            self.closed_f.add(current)\n",
        "\n",
        "            # if already settled by backward:\n",
        "            if current in self.closed_b:\n",
        "                return current\n",
        "\n",
        "            curr_depth = self.depth_f.get(current, 0)\n",
        "            if curr_depth >= self.max_depth:\n",
        "                continue\n",
        "\n",
        "            for nbr in self.g.neighbors(current):\n",
        "                if not self._allowed_forward(nbr):\n",
        "                    continue\n",
        "                tentative_g = self.g_f[current] + self._edge_weight(current, nbr)\n",
        "                tentative_depth = curr_depth + 1\n",
        "                if tentative_depth > self.max_depth:\n",
        "                    continue\n",
        "                if tentative_g < self.g_f.get(nbr, float(\"inf\")):\n",
        "                    self.g_f[nbr] = tentative_g\n",
        "                    self.depth_f[nbr] = tentative_depth\n",
        "                    self.parent_f[nbr] = current\n",
        "                    f_score = tentative_g + self.h_forward.get(nbr, 0.0)\n",
        "                    heapq.heappush(self.open_f, (f_score, next(self._counter), nbr))\n",
        "                    if nbr in self.closed_b:\n",
        "                        return nbr\n",
        "            return None\n",
        "        return None\n",
        "\n",
        "    def _expand_backward_once(self) -> Optional[SynsetName]:\n",
        "        \"\"\"Pop one element from backward open and expand predecessors. Return meeting node if found.\"\"\"\n",
        "        while self.open_b:\n",
        "            _, _, current = heapq.heappop(self.open_b)\n",
        "            if current in self.closed_b:\n",
        "                continue\n",
        "            self.closed_b.add(current)\n",
        "\n",
        "            if current in self.closed_f:\n",
        "                return current\n",
        "\n",
        "            curr_depth = self.depth_b.get(current, 0)\n",
        "            if curr_depth >= self.max_depth:\n",
        "                continue\n",
        "\n",
        "            for nbr in self.g.predecessors(current):\n",
        "                if not self._allowed_backward(nbr):\n",
        "                    continue\n",
        "                tentative_g = self.g_b[current] + self._edge_weight(nbr, current)\n",
        "                tentative_depth = curr_depth + 1\n",
        "                if tentative_depth > self.max_depth:\n",
        "                    continue\n",
        "                if tentative_g < self.g_b.get(nbr, float(\"inf\")):\n",
        "                    self.g_b[nbr] = tentative_g\n",
        "                    self.depth_b[nbr] = tentative_depth\n",
        "                    self.parent_b[nbr] = current\n",
        "                    f_score = tentative_g + self.h_backward.get(nbr, 0.0)\n",
        "                    heapq.heappush(self.open_b, (f_score, next(self._counter), nbr))\n",
        "                    if nbr in self.closed_f:\n",
        "                        return nbr\n",
        "            return None\n",
        "        return None\n",
        "\n",
        "    # -------------------------\n",
        "    # Path reconstruction\n",
        "    # -------------------------\n",
        "    def _reconstruct_path(self, meet: SynsetName) -> Path:\n",
        "        # forward part\n",
        "        path_f: Path = []\n",
        "        n = meet\n",
        "        while n is not None:\n",
        "            path_f.append(n)\n",
        "            n = self.parent_f.get(n)\n",
        "        path_f.reverse()\n",
        "\n",
        "        # backward part (exclude meet to avoid dup)\n",
        "        path_b: Path = []\n",
        "        n = self.parent_b.get(meet)\n",
        "        while n is not None:\n",
        "            path_b.append(n)\n",
        "            n = self.parent_b.get(n)\n",
        "\n",
        "        return path_f + path_b\n",
        "\n",
        "    # -------------------------\n",
        "    # Core: find multiple paths\n",
        "    # -------------------------\n",
        "    def find_paths(self, max_results: int = 3, len_tolerance: int = 0) -> List[Tuple[Path, float]]:\n",
        "        \"\"\"\n",
        "        Run bidirectional beam+depth constrained search and return up to max_results unique paths.\n",
        "        Paths returned have total cost (sum of g_f + g_b at meet) and are kept while <= best_cost + len_tolerance.\n",
        "\n",
        "        len_tolerance: integer extra hops allowed beyond the best (shortest) path length.\n",
        "        \"\"\"\n",
        "        # Setup\n",
        "        self._build_allowed_and_heuristics()\n",
        "        self._init_search_state()\n",
        "\n",
        "        results: List[Tuple[Path, float]] = []\n",
        "        seen_paths: Set[Tuple[SynsetName, ...]] = set()\n",
        "        best_cost: Optional[float] = None\n",
        "\n",
        "        # helper to compute current lower bound on any next path cost\n",
        "        def current_lower_bound() -> float:\n",
        "            min_f_f = self.open_f[0][0] if self.open_f else float(\"inf\")\n",
        "            min_f_b = self.open_b[0][0] if self.open_b else float(\"inf\")\n",
        "            return min_f_f + min_f_b\n",
        "\n",
        "        # main loop\n",
        "        while (self.open_f or self.open_b) and len(results) < max_results:\n",
        "            # stopping condition: if we have a best_cost and the conservative lower bound\n",
        "            # exceeds best_cost + len_tolerance, we can stop.\n",
        "            if best_cost is not None and current_lower_bound() > best_cost + float(len_tolerance):\n",
        "                break\n",
        "\n",
        "            # expand side with smaller top f\n",
        "            top_f = self.open_f[0][0] if self.open_f else float(\"inf\")\n",
        "            top_b = self.open_b[0][0] if self.open_b else float(\"inf\")\n",
        "\n",
        "            meet = None\n",
        "            if top_f <= top_b:\n",
        "                meet = self._expand_forward_once()\n",
        "            else:\n",
        "                meet = self._expand_backward_once()\n",
        "\n",
        "            if meet is None:\n",
        "                continue\n",
        "\n",
        "            # When meet occurs, reconstruct path and compute cost.\n",
        "            path = self._reconstruct_path(meet)\n",
        "            path_key = tuple(path)\n",
        "            # compute cost: if both g maps contain meet, sum them; otherwise try to compute from edges\n",
        "            cost_f = self.g_f.get(meet, float(\"inf\"))\n",
        "            cost_b = self.g_b.get(meet, float(\"inf\"))\n",
        "            total_cost = cost_f + cost_b if (cost_f < float(\"inf\") and cost_b < float(\"inf\")) else float(\"inf\")\n",
        "\n",
        "            # Hop-based cost fallback if edge weights are all 1 or inexact: length-1 equals hops\n",
        "            if total_cost == float(\"inf\"):\n",
        "                # fallback to hop count\n",
        "                total_cost = len(path) - 1\n",
        "\n",
        "            if path_key not in seen_paths:\n",
        "                seen_paths.add(path_key)\n",
        "                # Accept path if within tolerance of current best\n",
        "                if best_cost is None or total_cost <= best_cost + float(len_tolerance):\n",
        "                    results.append((path, total_cost))\n",
        "                    if best_cost is None or total_cost < best_cost:\n",
        "                        best_cost = total_cost\n",
        "\n",
        "            # continue searching for more meets until stopping condition triggers\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "pBK1W1I-Eqsv"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helpers"
      ],
      "metadata": {
        "id": "FRn74UEhEhB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Refactored top_k_branch_wrapper using new embedding functions\n",
        "# -------------------------\n",
        "def top_k_branch_wrapper(\n",
        "    candidates_lists: List[List],\n",
        "    target_synset_or_list,\n",
        "    beam_width: int = 3,\n",
        "    model=None,  # embedding model\n",
        "    wn_module=None  # WordNet module\n",
        ") -> List[BeamElement]:\n",
        "    \"\"\"\n",
        "    Adaptation so that find_connected_paths can call a branch-ranking function\n",
        "    that uses the new embedding-based alignment functions.\n",
        "\n",
        "    Args:\n",
        "      candidates_lists: list of lists of wn.synset objects (from gloss tokens)\n",
        "      target_synset_or_list: either a single wn.synset or a list of target synsets\n",
        "      beam_width: number of top pairs to return\n",
        "      model: embedding model (e.g., Word2Vec, GloVe)\n",
        "      wn_module: WordNet module\n",
        "    Returns:\n",
        "      List of beam elements ((synset_name, rel), (target_syn_name, rel), sim)\n",
        "    \"\"\"\n",
        "    if model is None or wn_module is None:\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Choose a single representative target synset to align against.\n",
        "    # If a list is provided, pick the first (you can change this strategy).\n",
        "    if isinstance(target_synset_or_list, (list, tuple)):\n",
        "        if not target_synset_or_list:\n",
        "            return []\n",
        "        target_syn = target_synset_or_list[0]\n",
        "    else:\n",
        "        target_syn = target_synset_or_list\n",
        "\n",
        "    if target_syn is None:\n",
        "        return []\n",
        "\n",
        "    # Import the embedding functions (assuming they're available)\n",
        "    from embedding_helpers import (\n",
        "        embed_lexical_relations,\n",
        "        get_top_k_aligned_lex_rel_pairs\n",
        "    )\n",
        "\n",
        "    # Precompute target synset relation embeddings\n",
        "    tgt_emb_dict = embed_lexical_relations(target_syn, model)\n",
        "\n",
        "    # Relation maps (same defaults used by the embedding-beam adapter)\n",
        "    asymm_map = {\n",
        "        \"hypernyms\": \"hyponyms\",\n",
        "        \"hyponyms\": \"hypernyms\",\n",
        "        \"part_meronyms\": \"part_holonyms\",\n",
        "        \"member_meronyms\": \"member_holonyms\",\n",
        "        \"substance_meronyms\": \"substance_holonyms\",\n",
        "        \"entailments\": \"causes\",\n",
        "        \"causes\": \"entailments\",\n",
        "    }\n",
        "    symm_map = {\n",
        "        \"hypernyms\": \"hypernyms\",\n",
        "        \"hyponyms\": \"hyponyms\",\n",
        "        \"part_meronyms\": \"part_meronyms\",\n",
        "        \"member_meronyms\": \"member_meronyms\",\n",
        "        \"also_sees\": \"also_sees\",\n",
        "        \"verb_groups\": \"verb_groups\",\n",
        "    }\n",
        "\n",
        "    # For each candidate synset in every candidate list, compute its lexical-relation embeddings\n",
        "    # and align to the target's relation embeddings using get_top_k_aligned_lex_rel_pairs\n",
        "    for cand_list in candidates_lists:\n",
        "        for cand_syn in cand_list:\n",
        "            try:\n",
        "                src_emb_dict = embed_lexical_relations(cand_syn, model)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            # get top pairs from both maps\n",
        "            try:\n",
        "                asymm_results = get_top_k_aligned_lex_rel_pairs(\n",
        "                    asymm_map, src_emb_dict, tgt_emb_dict, beam_width=beam_width\n",
        "                )\n",
        "                results.extend(asymm_results)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            try:\n",
        "                symm_results = get_top_k_aligned_lex_rel_pairs(\n",
        "                    symm_map, src_emb_dict, tgt_emb_dict, beam_width=beam_width\n",
        "                )\n",
        "                results.extend(symm_results)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # Sort aggregated results by similarity and return top beam_width\n",
        "    results.sort(key=lambda x: x[2], reverse=True)\n",
        "    return results[:beam_width]\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# High-level triple search (subject -> predicate -> object)\n",
        "# -------------------------\n",
        "def find_connected_paths(\n",
        "    g: nx.DiGraph,\n",
        "    subject_word: str,\n",
        "    predicate_word: str,\n",
        "    object_word: str,\n",
        "    wn_module,                # your WordNet interface (e.g., nltk.corpus.wordnet or custom)\n",
        "    nlp_func,                 # spaCy NLP call (callable that takes a string -> doc)\n",
        "    get_new_beams_fn=None,\n",
        "    top_k_branch_fn: Optional[TopKBranchFn] = None,\n",
        "    extract_subjects_from_gloss=None,\n",
        "    extract_objects_from_gloss=None,\n",
        "    beam_width: int = 3,\n",
        "    max_depth: int = 8,\n",
        "    max_self_intersection: int = 5,\n",
        "    max_results_per_pair: int = 3,\n",
        "    len_tolerance: int = 1,\n",
        "    relax_beam: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Find connected subject->predicate->object paths.\n",
        "\n",
        "    Strategy:\n",
        "      - Get synsets for subject (nouns), predicate (verbs), object (nouns).\n",
        "      - For each candidate predicate synset:\n",
        "          - Build gloss seeds for subject->predicate side (use predicate gloss to extract candidate subject tokens)\n",
        "          - Build gloss seeds for predicate->object side (use predicate gloss to extract object tokens)\n",
        "          - Run PairwiseBidirectionalAStar twice (subj->pred and pred->obj), requesting multiple paths each\n",
        "          - Combine pairs of returned paths that share the predicate synset and meet intersection constraints\n",
        "      - Return sorted list of connected path triples\n",
        "    \"\"\"\n",
        "\n",
        "    # get word synsets\n",
        "    subject_synsets = wn_module.synsets(subject_word, pos=wn_module.NOUN)\n",
        "    predicate_synsets = wn_module.synsets(predicate_word, pos=wn_module.VERB)\n",
        "    object_synsets = wn_module.synsets(object_word, pos=wn_module.NOUN)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # helper: safe top_k_branch function fallback (if not provided)\n",
        "    def _default_top_k_branch_fn(candidates_lists, target_synset, beam_width_inner=3):\n",
        "        # fallback: return empty (user should inject a real one)\n",
        "        return []\n",
        "\n",
        "    top_k_branch_fn_used = top_k_branch_fn or _default_top_k_branch_fn\n",
        "\n",
        "    for pred_syn in predicate_synsets:\n",
        "        pred_name = pred_syn.name()\n",
        "\n",
        "        # ------------------------\n",
        "        # Build gloss seeds for subj->pred\n",
        "        # ------------------------\n",
        "        pred_gloss_doc = nlp_func(pred_syn.definition())\n",
        "\n",
        "        # Extract subject tokens from predicate gloss\n",
        "        active_subject_tokens = []\n",
        "        if extract_subjects_from_gloss:\n",
        "            try:\n",
        "                active_subject_tokens, _ = extract_subjects_from_gloss(pred_gloss_doc)\n",
        "            except Exception:\n",
        "                active_subject_tokens = []\n",
        "\n",
        "        # Convert tokens to candidate synset lists and get top_k branches\n",
        "        subject_candidate_synsets = []\n",
        "        for t in active_subject_tokens:\n",
        "            try:\n",
        "                synsets = wn_module.synsets(t.text, pos=wn_module.NOUN)\n",
        "                if synsets:\n",
        "                    subject_candidate_synsets.append(synsets)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Get top-k synsets that best align with subject synsets\n",
        "        subj_gloss_seed_nodes = set()\n",
        "        if subject_candidate_synsets and top_k_branch_fn_used:\n",
        "            try:\n",
        "                subj_top_k = top_k_branch_fn_used(subject_candidate_synsets, subject_synsets, beam_width)\n",
        "                # Extract synset names from the results\n",
        "                for beam_elem in subj_top_k:\n",
        "                    s_pair = beam_elem[0]  # (synset_name, rel)\n",
        "                    if isinstance(s_pair[0], str):\n",
        "                        subj_gloss_seed_nodes.add(s_pair[0])\n",
        "                    elif hasattr(s_pair[0], 'name'):\n",
        "                        subj_gloss_seed_nodes.add(s_pair[0].name())\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # ------------------------\n",
        "        # Build gloss seeds for pred->obj\n",
        "        # ------------------------\n",
        "        objects_tokens = []\n",
        "        if extract_objects_from_gloss:\n",
        "            try:\n",
        "                objects_tokens = extract_objects_from_gloss(pred_gloss_doc)\n",
        "            except Exception:\n",
        "                objects_tokens = []\n",
        "\n",
        "        # Convert tokens to candidate synset lists\n",
        "        object_candidate_synsets = []\n",
        "        for t in objects_tokens:\n",
        "            try:\n",
        "                synsets = wn_module.synsets(t.text, pos=wn_module.NOUN)\n",
        "                if synsets:\n",
        "                    object_candidate_synsets.append(synsets)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Get top-k synsets that best align with object synsets\n",
        "        obj_gloss_seed_nodes = set()\n",
        "        if object_candidate_synsets and top_k_branch_fn_used:\n",
        "            try:\n",
        "                obj_top_k = top_k_branch_fn_used(object_candidate_synsets, object_synsets, beam_width)\n",
        "                # Extract synset names from the results\n",
        "                for beam_elem in obj_top_k:\n",
        "                    s_pair = beam_elem[0]  # (synset_name, rel)\n",
        "                    if isinstance(s_pair[0], str):\n",
        "                        obj_gloss_seed_nodes.add(s_pair[0])\n",
        "                    elif hasattr(s_pair[0], 'name'):\n",
        "                        obj_gloss_seed_nodes.add(s_pair[0].name())\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Now run pairwise searches:\n",
        "        # For subject->predicate: src = each subject synset, tgt = pred_syn (pred_name)\n",
        "        subject_paths_map = {}  # subj_syn_name -> list of (path, cost)\n",
        "        for subj_syn in subject_synsets:\n",
        "            # Check if cross-POS search\n",
        "            if subj_syn.pos() != pred_syn.pos():\n",
        "                # Cross-POS: rely more on gloss seeds, relax beam constraints\n",
        "                pair_search = PairwiseBidirectionalAStar(\n",
        "                    g=g,\n",
        "                    src=subj_syn.name(),\n",
        "                    tgt=pred_name,\n",
        "                    get_new_beams_fn=None,  # Don't use beam function for cross-POS\n",
        "                    gloss_seed_nodes=subj_gloss_seed_nodes,  # Use gloss seeds\n",
        "                    beam_width=beam_width,\n",
        "                    max_depth=max_depth,\n",
        "                    relax_beam=True  # Allow exploration outside beams\n",
        "                )\n",
        "            else:\n",
        "                # Same-POS: use normal beam search with gloss enrichment\n",
        "                pair_search = PairwiseBidirectionalAStar(\n",
        "                    g=g,\n",
        "                    src=subj_syn.name(),\n",
        "                    tgt=pred_name,\n",
        "                    get_new_beams_fn=get_new_beams_fn,\n",
        "                    gloss_seed_nodes=subj_gloss_seed_nodes,\n",
        "                    beam_width=beam_width,\n",
        "                    max_depth=max_depth,\n",
        "                    relax_beam=relax_beam\n",
        "                )\n",
        "\n",
        "            subj_paths = pair_search.find_paths(\n",
        "                max_results=max_results_per_pair,\n",
        "                len_tolerance=len_tolerance\n",
        "            )\n",
        "            if subj_paths:\n",
        "                subject_paths_map[subj_syn.name()] = subj_paths\n",
        "\n",
        "        # For predicate->object: src = pred_name, tgt = each object synset\n",
        "        object_paths_map = {}  # obj_syn_name -> list of (path, cost)\n",
        "        for obj_syn in object_synsets:\n",
        "            # Check if cross-POS search\n",
        "            if pred_syn.pos() != obj_syn.pos():\n",
        "                # Cross-POS: rely more on gloss seeds, relax beam constraints\n",
        "                pair_search = PairwiseBidirectionalAStar(\n",
        "                    g=g,\n",
        "                    src=pred_name,\n",
        "                    tgt=obj_syn.name(),\n",
        "                    get_new_beams_fn=None,  # Don't use beam function for cross-POS\n",
        "                    gloss_seed_nodes=obj_gloss_seed_nodes,  # Use gloss seeds\n",
        "                    beam_width=beam_width,\n",
        "                    max_depth=max_depth,\n",
        "                    relax_beam=True  # Allow exploration outside beams\n",
        "                )\n",
        "            else:\n",
        "                # Same-POS: use normal beam search with gloss enrichment\n",
        "                pair_search = PairwiseBidirectionalAStar(\n",
        "                    g=g,\n",
        "                    src=pred_name,\n",
        "                    tgt=obj_syn.name(),\n",
        "                    get_new_beams_fn=get_new_beams_fn,\n",
        "                    gloss_seed_nodes=obj_gloss_seed_nodes,\n",
        "                    beam_width=beam_width,\n",
        "                    max_depth=max_depth,\n",
        "                    relax_beam=relax_beam\n",
        "                )\n",
        "\n",
        "            obj_paths = pair_search.find_paths(\n",
        "                max_results=max_results_per_pair,\n",
        "                len_tolerance=len_tolerance\n",
        "            )\n",
        "            if obj_paths:\n",
        "                object_paths_map[obj_syn.name()] = obj_paths\n",
        "\n",
        "        # Combine: for any subj_path and obj_path that both go through `pred_name`,\n",
        "        # produce combined results if intersection is low enough.\n",
        "        for subj_syn_name, subj_paths in subject_paths_map.items():\n",
        "            for subj_path, subj_cost in subj_paths:\n",
        "                for obj_syn_name, obj_paths in object_paths_map.items():\n",
        "                    for obj_path, obj_cost in obj_paths:\n",
        "                        # Make sure paths connect through the predicate\n",
        "                        if subj_path[-1] == pred_name and obj_path[0] == pred_name:\n",
        "                            # Check intersection to avoid tautological results\n",
        "                            intersection_size = len(set(subj_path).intersection(set(obj_path)))\n",
        "                            if intersection_size <= max_self_intersection:\n",
        "                                # Combined path: subject_path + object_path[1:] (drop duplicate predicate)\n",
        "                                combined_path = subj_path + obj_path[1:]\n",
        "                                combined_cost = subj_cost + obj_cost\n",
        "                                combined_len = len(combined_path)\n",
        "                                results.append({\n",
        "                                    \"predicate_synset\": pred_name,\n",
        "                                    \"subject_path\": subj_path,\n",
        "                                    \"object_path\": obj_path,\n",
        "                                    \"combined_path\": combined_path,\n",
        "                                    \"combined_cost\": combined_cost,\n",
        "                                    \"combined_len\": combined_len,\n",
        "                                })\n",
        "\n",
        "    # sort results by combined_len then cost\n",
        "    results = sorted(results, key=lambda r: (r[\"combined_len\"], r[\"combined_cost\"]))\n",
        "    return results"
      ],
      "metadata": {
        "id": "Yar9RUwGqcJc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gloss Parsing"
      ],
      "metadata": {
        "id": "Wli_MPyM2OvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Gloss Analysis Helper Functions (Keep as-is, they work well)\n",
        "# ============================================================================\n",
        "\n",
        "def extract_subjects_from_gloss(gloss_doc):\n",
        "    \"\"\"Extract subject tokens from a parsed gloss.\"\"\"\n",
        "    subjects = []\n",
        "\n",
        "    # Direct subjects\n",
        "    subjects.extend([tok for tok in gloss_doc if tok.dep_ == \"nsubj\"])\n",
        "\n",
        "    # Passive subjects (which are actually objects semantically)\n",
        "    # Skip these for actor identification\n",
        "    passive_subjects = [tok for tok in gloss_doc if tok.dep_ == \"nsubjpass\"]\n",
        "\n",
        "    # Filter out passive subjects from the main list\n",
        "    subjects = [s for s in subjects if s not in passive_subjects]\n",
        "\n",
        "    return subjects, passive_subjects\n",
        "\n",
        "\n",
        "def extract_objects_from_gloss(gloss_doc):\n",
        "    \"\"\"Extract various types of object tokens from a parsed gloss.\"\"\"\n",
        "    objs = []\n",
        "\n",
        "    # Indirect objects\n",
        "    iobjs = [tok for tok in gloss_doc if tok.dep_ == \"iobj\"]\n",
        "    objs.extend(iobjs)\n",
        "\n",
        "    # Direct objects\n",
        "    # Only include if there were no indirect objects,\n",
        "    #   crude, but good for MVP\n",
        "    if not iobjs:\n",
        "        objs.extend([tok for tok in gloss_doc if tok.dep_ == \"dobj\"])\n",
        "\n",
        "    # Prepositional objects\n",
        "    objs.extend([tok for tok in gloss_doc if tok.dep_ == \"pobj\"])\n",
        "\n",
        "    # General objects\n",
        "    objs.extend([tok for tok in gloss_doc if tok.dep_ == \"obj\"])\n",
        "\n",
        "    # Check for noun chunks related to root verb\n",
        "    root_verbs = [tok for tok in gloss_doc if tok.dep_ == \"ROOT\" and tok.pos_ == \"VERB\"]\n",
        "    if root_verbs and not objs:\n",
        "        for noun_chunk in gloss_doc.noun_chunks:\n",
        "            if any(token.head == root_verbs[0] for token in noun_chunk):\n",
        "                objs.append(noun_chunk.root)\n",
        "\n",
        "    return objs\n",
        "\n",
        "\n",
        "def extract_verbs_from_gloss(gloss_doc, include_passive=False):\n",
        "    \"\"\"Extract verb tokens from a parsed gloss.\"\"\"\n",
        "    verbs = [tok for tok in gloss_doc if tok.pos_ == \"VERB\"]\n",
        "\n",
        "    if include_passive:\n",
        "        # Past participles used as adjectives or in relative clauses\n",
        "        passive_verbs = [tok for tok in gloss_doc if\n",
        "                        tok.tag_ in [\"VBN\", \"VBD\"] and\n",
        "                        tok.dep_ in [\"acl\", \"relcl\", \"amod\"]]\n",
        "        verbs.extend(passive_verbs)\n",
        "\n",
        "    return verbs\n",
        "\n",
        "\n",
        "def find_instrumental_verbs(gloss_doc):\n",
        "    \"\"\"Find verbs associated with instrumental use (e.g., 'used for').\"\"\"\n",
        "    instrumental_verbs = []\n",
        "\n",
        "    if \"used\" in gloss_doc.text.lower():\n",
        "        for i, token in enumerate(gloss_doc):\n",
        "            if token.text.lower() == \"used\":\n",
        "                # Check tokens after \"used\"\n",
        "                for j in range(i+1, min(i+4, len(gloss_doc))):\n",
        "                    if gloss_doc[j].pos_ == \"VERB\":\n",
        "                        instrumental_verbs.append(gloss_doc[j])\n",
        "\n",
        "    return instrumental_verbs\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Simple same-POS path finding (for backward compatibility)\n",
        "# ============================================================================\n",
        "\n",
        "# def get_all_neighbors(synset, wn_module=None):\n",
        "#     \"\"\"Get all lexically related neighbors of a synset.\"\"\"\n",
        "#     neighbors = set()\n",
        "\n",
        "#     # Add all types of relations\n",
        "#     relation_methods = [\n",
        "#         'hypernyms', 'hyponyms', 'holonyms', 'meronyms',\n",
        "#         'similar_tos', 'also_sees', 'verb_groups',\n",
        "#         'entailments', 'causes', 'attributes'\n",
        "#     ]\n",
        "\n",
        "#     for method_name in relation_methods:\n",
        "#         if hasattr(synset, method_name):\n",
        "#             try:\n",
        "#                 related = getattr(synset, method_name)()\n",
        "#                 neighbors.update(related)\n",
        "#             except:\n",
        "#                 pass\n",
        "\n",
        "#     return list(neighbors)\n",
        "\n",
        "\n",
        "def path_syn_to_syn(start_synset, end_synset, max_depth=6, wn_module=None):\n",
        "    \"\"\"\n",
        "    Find shortest path between synsets of the same POS using bidirectional BFS.\n",
        "    Returns a list of synset names (strings) forming the path, or None if no path found.\n",
        "    \"\"\"\n",
        "    # Convert to names for consistency\n",
        "    start_name = start_synset.name() if hasattr(start_synset, 'name') else str(start_synset)\n",
        "    end_name = end_synset.name() if hasattr(end_synset, 'name') else str(end_synset)\n",
        "\n",
        "    # Check if same POS (if we have synset objects)\n",
        "    if hasattr(start_synset, 'pos') and hasattr(end_synset, 'pos'):\n",
        "        if start_synset.pos() != end_synset.pos():\n",
        "            return None\n",
        "\n",
        "    # Handle the trivial case where start and end are the same\n",
        "    if start_name == end_name:\n",
        "        return [start_name]\n",
        "\n",
        "    # Initialize two search frontiers\n",
        "    forward_queue = deque([(start_synset, 0)])\n",
        "    forward_visited = {start_name: [start_name]}\n",
        "\n",
        "    backward_queue = deque([(end_synset, 0)])\n",
        "    backward_visited = {end_name: [end_name]}\n",
        "\n",
        "    def expand_frontier(queue, visited_from_this_side, visited_from_other_side, is_forward):\n",
        "        \"\"\"Expand one step of the search frontier.\"\"\"\n",
        "        if not queue:\n",
        "            return None\n",
        "\n",
        "        curr_synset, depth = queue.popleft()\n",
        "\n",
        "        if depth >= (max_depth + 1) // 2:\n",
        "            return None\n",
        "\n",
        "        curr_name = curr_synset.name() if hasattr(curr_synset, 'name') else str(curr_synset)\n",
        "        path_to_current = visited_from_this_side[curr_name]\n",
        "\n",
        "        for neighbor in get_all_neighbors(curr_synset, wn_module):\n",
        "            neighbor_name = neighbor.name() if hasattr(neighbor, 'name') else str(neighbor)\n",
        "\n",
        "            if neighbor_name in visited_from_this_side:\n",
        "                continue\n",
        "\n",
        "            if is_forward:\n",
        "                new_path = path_to_current + [neighbor_name]\n",
        "            else:\n",
        "                new_path = [neighbor_name] + path_to_current\n",
        "\n",
        "            if neighbor_name in visited_from_other_side:\n",
        "                other_path = visited_from_other_side[neighbor_name]\n",
        "\n",
        "                if is_forward:\n",
        "                    full_path = path_to_current + other_path\n",
        "                else:\n",
        "                    full_path = other_path + path_to_current\n",
        "\n",
        "                return full_path\n",
        "\n",
        "            visited_from_this_side[neighbor_name] = new_path\n",
        "            queue.append((neighbor, depth + 1))\n",
        "\n",
        "        return None\n",
        "\n",
        "    # Alternate between forward and backward search\n",
        "    while forward_queue or backward_queue:\n",
        "        if forward_queue:\n",
        "            result = expand_frontier(forward_queue, forward_visited, backward_visited, True)\n",
        "            if result:\n",
        "                return result\n",
        "\n",
        "        if backward_queue:\n",
        "            result = expand_frontier(backward_queue, backward_visited, forward_visited, False)\n",
        "            if result:\n",
        "                return result\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "3mblVDdWG3Fg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Main wrapper function to replace find_connected_shortest_paths\n",
        "# ============================================================================\n",
        "\n",
        "def find_connected_shortest_paths(\n",
        "    subject_word: str,\n",
        "    predicate_word: str,\n",
        "    object_word: str,\n",
        "    wn_module,\n",
        "    nlp_func,\n",
        "    model=None,  # embedding model\n",
        "    g: nx.DiGraph = None,  # synset graph\n",
        "    max_depth: int = 10,\n",
        "    max_self_intersection: int = 5,\n",
        "    beam_width: int = 3,\n",
        "    max_results_per_pair: int = 3,\n",
        "    len_tolerance: int = 1,\n",
        "    relax_beam: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Wrapper that uses the new find_connected_paths architecture.\n",
        "    Returns the best connected path in the old format for backward compatibility.\n",
        "    \"\"\"\n",
        "\n",
        "    # Import the necessary components\n",
        "    from pathfinding_core import (\n",
        "        PairwiseBidirectionalAStar,\n",
        "        find_connected_paths,\n",
        "        get_new_beams_from_embeddings\n",
        "    )\n",
        "\n",
        "    # Create the beam function if we have a model\n",
        "    get_new_beams_fn = None\n",
        "    if model is not None and g is not None:\n",
        "        get_new_beams_fn = lambda g, src, tgt: get_new_beams_from_embeddings(\n",
        "            g, src, tgt, wn_module, model, beam_width=beam_width\n",
        "        )\n",
        "\n",
        "    # Create the top_k_branch function\n",
        "    top_k_branch_fn = None\n",
        "    if model is not None:\n",
        "        top_k_branch_fn = lambda candidates, target, bw: get_top_k_synset_branch_pairs(\n",
        "            candidates, target, bw, model, wn_module\n",
        "        )\n",
        "\n",
        "    # Build the graph if not provided\n",
        "    if g is None:\n",
        "        g = build_synset_graph(wn_module)  # You'll need to implement this\n",
        "\n",
        "    # Call the new find_connected_paths function\n",
        "    results = find_connected_paths(\n",
        "        g=g,\n",
        "        subject_word=subject_word,\n",
        "        predicate_word=predicate_word,\n",
        "        object_word=object_word,\n",
        "        wn_module=wn_module,\n",
        "        nlp_func=nlp_func,\n",
        "        get_new_beams_fn=get_new_beams_fn,\n",
        "        top_k_branch_fn=top_k_branch_fn,\n",
        "        extract_subjects_from_gloss=extract_subjects_from_gloss,\n",
        "        extract_objects_from_gloss=extract_objects_from_gloss,\n",
        "        beam_width=beam_width,\n",
        "        max_depth=max_depth,\n",
        "        max_self_intersection=max_self_intersection,\n",
        "        max_results_per_pair=max_results_per_pair,\n",
        "        len_tolerance=len_tolerance,\n",
        "        relax_beam=relax_beam\n",
        "    )\n",
        "\n",
        "    # Convert results to old format (best_subject_path, best_object_path, best_predicate)\n",
        "    if results:\n",
        "        best_result = results[0]  # Take the best result\n",
        "        # Convert synset names back to synset objects if needed\n",
        "        subject_path = [wn_module.synset(name) if isinstance(name, str) else name\n",
        "                       for name in best_result[\"subject_path\"]]\n",
        "        object_path = [wn_module.synset(name) if isinstance(name, str) else name\n",
        "                      for name in best_result[\"object_path\"]]\n",
        "        pred_synset = wn_module.synset(best_result[\"predicate_synset\"]) \\\n",
        "                     if isinstance(best_result[\"predicate_synset\"], str) \\\n",
        "                     else best_result[\"predicate_synset\"]\n",
        "\n",
        "        return subject_path, object_path, pred_synset\n",
        "\n",
        "    return None, None, None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Helper function to build synset graph (if needed)\n",
        "# ============================================================================\n",
        "\n",
        "def build_synset_graph(wn_module) -> nx.DiGraph:\n",
        "    \"\"\"\n",
        "    Build a directed graph of synsets with their lexical relations.\n",
        "    \"\"\"\n",
        "    g = nx.DiGraph()\n",
        "\n",
        "    # Get all synsets (you may want to limit this for performance)\n",
        "    all_synsets = list(wn_module.all_synsets())\n",
        "\n",
        "    # Add nodes\n",
        "    for synset in all_synsets:\n",
        "        g.add_node(synset.name())\n",
        "\n",
        "    # Add edges based on relations\n",
        "    for synset in all_synsets:\n",
        "        synset_name = synset.name()\n",
        "\n",
        "        # Add various relation types as edges\n",
        "        relations = {\n",
        "            'hypernyms': synset.hypernyms(),\n",
        "            'hyponyms': synset.hyponyms(),\n",
        "            'holonyms': synset.holonyms(),\n",
        "            'meronyms': synset.meronyms(),\n",
        "            'similar_tos': synset.similar_tos() if hasattr(synset, 'similar_tos') else [],\n",
        "            'also_sees': synset.also_sees() if hasattr(synset, 'also_sees') else [],\n",
        "            'verb_groups': synset.verb_groups() if hasattr(synset, 'verb_groups') else [],\n",
        "            'entailments': synset.entailments() if hasattr(synset, 'entailments') else [],\n",
        "            'causes': synset.causes() if hasattr(synset, 'causes') else [],\n",
        "        }\n",
        "\n",
        "        for rel_type, related_synsets in relations.items():\n",
        "            for related in related_synsets:\n",
        "                if related.name() in g:\n",
        "                    g.add_edge(synset_name, related.name(),\n",
        "                              relation=rel_type, weight=1.0)\n",
        "\n",
        "    return g\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Display Functions (keep as-is for backward compatibility)\n",
        "# ============================================================================\n",
        "\n",
        "def show_path(label, path):\n",
        "    \"\"\"Pretty print a path of synsets.\"\"\"\n",
        "    if path:\n",
        "        print(f\"{label}:\")\n",
        "        # Handle both synset objects and name strings\n",
        "        path_str = []\n",
        "        for s in path:\n",
        "            if hasattr(s, 'name'):\n",
        "                path_str.append(f\"{s.name()} ({s.definition()})\")\n",
        "            else:\n",
        "                path_str.append(str(s))\n",
        "        print(\" -> \".join(path_str))\n",
        "        print(f\"Path length: {len(path)}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"{label}: No path found\")\n",
        "        print()\n",
        "\n",
        "\n",
        "def show_connected_paths(subject_path, object_path, predicate):\n",
        "    \"\"\"Display the connected paths with their shared predicate.\"\"\"\n",
        "    if subject_path and object_path and predicate:\n",
        "        print(\"=\" * 70)\n",
        "        pred_name = predicate.name() if hasattr(predicate, 'name') else str(predicate)\n",
        "        print(f\"CONNECTED PATH through predicate: {pred_name}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        show_path(\"Subject -> Predicate path\", subject_path)\n",
        "        show_path(\"Predicate -> Object path\", object_path)\n",
        "\n",
        "        # Show the complete connected path\n",
        "        complete_path = subject_path + object_path[1:]  # Avoid duplicating the predicate\n",
        "        print(\"Complete connected path:\")\n",
        "        path_names = []\n",
        "        for s in complete_path:\n",
        "            if hasattr(s, 'name'):\n",
        "                path_names.append(s.name())\n",
        "            else:\n",
        "                path_names.append(str(s))\n",
        "        print(\" -> \".join(path_names))\n",
        "        print(f\"Total path length: {len(complete_path)}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(\"No connected path found through any predicate synset.\")"
      ],
      "metadata": {
        "id": "B5FzdxGNHUlR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "GZ7mH1XuhtBN"
      },
      "outputs": [],
      "source": [
        "# # ============================================================================\n",
        "# # Core Path Finding Functions\n",
        "# # ============================================================================\n",
        "\n",
        "# def path_syn_to_syn(start_synset, end_synset, max_depth=6):\n",
        "#     \"\"\"\n",
        "#     Find shortest path between synsets of the same POS using bidirectional BFS.\n",
        "#     Returns a list of synsets forming the path, or None if no path found.\n",
        "#     \"\"\"\n",
        "\n",
        "#     if not (start_synset.pos() == end_synset.pos() and start_synset.pos() in {'n', 'v'}):\n",
        "#       raise ValueError(f\"{start_synset.name()} POS tag != {end_synset.name()}. Synsets must be of the same POS (noun or verb).\")\n",
        "\n",
        "#     # Handle the trivial case where start and end are the same\n",
        "#     if start_synset.name() == end_synset.name():\n",
        "#         return [start_synset]\n",
        "\n",
        "#     # Initialize two search frontiers\n",
        "#     forward_queue = deque([(start_synset, 0)])\n",
        "#     forward_visited = {start_synset.name(): [start_synset]}\n",
        "\n",
        "#     backward_queue = deque([(end_synset, 0)])\n",
        "#     backward_visited = {end_synset.name(): [end_synset]}\n",
        "\n",
        "#     def expand_frontier(queue, visited_from_this_side, visited_from_other_side, is_forward):\n",
        "#         \"\"\"Expand one step of the search frontier.\"\"\"\n",
        "#         if not queue:\n",
        "#             return None\n",
        "\n",
        "#         curr_synset, depth = queue.popleft()\n",
        "\n",
        "#         if depth >= (max_depth + 1) // 2:\n",
        "#             return None\n",
        "\n",
        "#         path_to_current = visited_from_this_side[curr_synset.name()]\n",
        "\n",
        "#         for neighbor in get_all_neighbors(curr_synset):\n",
        "#             neighbor_name = neighbor.name()\n",
        "\n",
        "#             if neighbor_name in visited_from_this_side:\n",
        "#                 continue\n",
        "\n",
        "#             if is_forward:\n",
        "#                 new_path = path_to_current + [neighbor]\n",
        "#             else:\n",
        "#                 new_path = [neighbor] + path_to_current\n",
        "\n",
        "#             if neighbor_name in visited_from_other_side:\n",
        "#                 other_path = visited_from_other_side[neighbor_name]\n",
        "\n",
        "#                 if is_forward:\n",
        "#                     full_path = path_to_current + other_path\n",
        "#                 else:\n",
        "#                     full_path = other_path + path_to_current\n",
        "\n",
        "#                 return full_path\n",
        "\n",
        "#             visited_from_this_side[neighbor_name] = new_path\n",
        "#             queue.append((neighbor, depth + 1))\n",
        "\n",
        "#         return None\n",
        "\n",
        "#     # Alternate between forward and backward search\n",
        "#     while forward_queue or backward_queue:\n",
        "#         if forward_queue:\n",
        "#             result = expand_frontier(forward_queue, forward_visited, backward_visited, True)\n",
        "#             if result:\n",
        "#                 return result\n",
        "\n",
        "#         if backward_queue:\n",
        "#             result = expand_frontier(backward_queue, backward_visited, forward_visited, False)\n",
        "#             if result:\n",
        "#                 return result\n",
        "\n",
        "#     return None\n",
        "\n",
        "\n",
        "# # ============================================================================\n",
        "# # Gloss Analysis Helper Functions\n",
        "# # ============================================================================\n",
        "\n",
        "# def extract_subjects_from_gloss(gloss_doc):\n",
        "#     \"\"\"Extract subject tokens from a parsed gloss.\"\"\"\n",
        "#     subjects = []\n",
        "\n",
        "#     # Direct subjects\n",
        "#     subjects.extend([tok for tok in gloss_doc if tok.dep_ == \"nsubj\"])\n",
        "\n",
        "#     # Passive subjects (which are actually objects semantically)\n",
        "#     # Skip these for actor identification\n",
        "#     passive_subjects = [tok for tok in gloss_doc if tok.dep_ == \"nsubjpass\"]\n",
        "\n",
        "#     # Filter out passive subjects from the main list\n",
        "#     subjects = [s for s in subjects if s not in passive_subjects]\n",
        "\n",
        "#     return subjects, passive_subjects\n",
        "\n",
        "\n",
        "# def extract_objects_from_gloss(gloss_doc):\n",
        "#     \"\"\"Extract various types of object tokens from a parsed gloss.\"\"\"\n",
        "#     objs = []\n",
        "\n",
        "#     # Indirect objects\n",
        "#     iobjs = [tok for tok in gloss_doc if tok.dep_ == \"iobj\"]\n",
        "#     objs.extend(iobjs)\n",
        "\n",
        "#     # Direct objects\n",
        "#     # Only include if there were no indirect objects,\n",
        "#     #   crude, but good for MVP\n",
        "#     if not iobjs:\n",
        "#         objs.extend([tok for tok in gloss_doc if tok.dep_ == \"dobj\"])\n",
        "\n",
        "#     # Prepositional objects\n",
        "#     objs.extend([tok for tok in gloss_doc if tok.dep_ == \"pobj\"])\n",
        "\n",
        "#     # General objects\n",
        "#     objs.extend([tok for tok in gloss_doc if tok.dep_ == \"obj\"])\n",
        "\n",
        "#     # Check for noun chunks related to root verb\n",
        "#     root_verbs = [tok for tok in gloss_doc if tok.dep_ == \"ROOT\" and tok.pos_ == \"VERB\"]\n",
        "#     if root_verbs and not objs:\n",
        "#         for noun_chunk in gloss_doc.noun_chunks:\n",
        "#             if any(token.head == root_verbs[0] for token in noun_chunk):\n",
        "#                 objs.append(noun_chunk.root)\n",
        "\n",
        "#     return objs\n",
        "\n",
        "\n",
        "# def extract_verbs_from_gloss(gloss_doc, include_passive=False):\n",
        "#     \"\"\"Extract verb tokens from a parsed gloss.\"\"\"\n",
        "#     verbs = [tok for tok in gloss_doc if tok.pos_ == \"VERB\"]\n",
        "\n",
        "#     if include_passive:\n",
        "#         # Past participles used as adjectives or in relative clauses\n",
        "#         passive_verbs = [tok for tok in gloss_doc if\n",
        "#                         tok.tag_ in [\"VBN\", \"VBD\"] and\n",
        "#                         tok.dep_ in [\"acl\", \"relcl\", \"amod\"]]\n",
        "#         verbs.extend(passive_verbs)\n",
        "\n",
        "#     return verbs\n",
        "\n",
        "\n",
        "# def find_instrumental_verbs(gloss_doc):\n",
        "#     \"\"\"Find verbs associated with instrumental use (e.g., 'used for').\"\"\"\n",
        "#     instrumental_verbs = []\n",
        "\n",
        "#     if \"used\" in gloss_doc.text.lower():\n",
        "#         for i, token in enumerate(gloss_doc):\n",
        "#             if token.text.lower() == \"used\":\n",
        "#                 # Check tokens after \"used\"\n",
        "#                 for j in range(i+1, min(i+4, len(gloss_doc))):\n",
        "#                     if gloss_doc[j].pos_ == \"VERB\":\n",
        "#                         instrumental_verbs.append(gloss_doc[j])\n",
        "\n",
        "#     return instrumental_verbs\n",
        "\n",
        "\n",
        "# # ============================================================================\n",
        "# # Cross-POS Path Finding Functions\n",
        "# # ============================================================================\n",
        "# def get_top_k_synset_branch_pairs(\n",
        "#       candidates: List[List[wn.synset]],\n",
        "#       target_synset: wn.synset,\n",
        "#       beam_width=3\n",
        "#     ) -> List[Tuple[\n",
        "#         Tuple[wn.synset, str],\n",
        "#         Tuple[wn.synset, str],\n",
        "#         float\n",
        "#     ]]:\n",
        "#     \"\"\"\n",
        "#     Given a list of candidate tokens and a target synset,\n",
        "\n",
        "\n",
        "#     Return the k synset subrelation pairs most similar to the target of the form:\n",
        "#       ((synset, lexical_rel), (name, lexical_rel), relatedness)\n",
        "#     \"\"\"\n",
        "#     top_k_asymm_branches = list()\n",
        "#     top_k_symm_branches = list()\n",
        "#     beam = list()\n",
        "#     # for each list of possible synsets for a candidate token\n",
        "#     for synsets in candidates:\n",
        "#         # # filter to subjects based on whether they reside in the same sub-category\n",
        "#         # #   where the subcategory != 'entity.n.01' or a similar top-level\n",
        "#         # synsets = [\n",
        "#         #     s for s in synsets\n",
        "#         #     if s.root_hypernyms() != s.lowest_common_hypernyms(target_synset)\n",
        "#         # ]\n",
        "#         # # if there are synsets left for the candidate token after pruning\n",
        "#         if synsets:\n",
        "#             # # if the target is a verb,\n",
        "#             # #   filter out any synsets with no lemma frames matching the target\n",
        "#             # #   frame patterns: (Somebody [v] something), (Somebody [v]), ...\n",
        "#             # if target_synset.pos() == 'v':\n",
        "#             #     synsets = [\n",
        "#             #         s for s in synsets\n",
        "#             #         if any(\n",
        "#             #             frame in s.frame_ids()\n",
        "#             #             for frame in target_synset.frame_ids()\n",
        "#             #         )\n",
        "#             #     ]\n",
        "#             for synset in synsets:\n",
        "#               beam += get_synset_relatedness(synset, target_synset)\n",
        "#               beam += get_synset_relatedness(synset, target_synset)\n",
        "#     beam = sorted(\n",
        "#         beam,\n",
        "#         key=lambda x: x[2],\n",
        "#         reverse=True\n",
        "#     )[:beam_width]\n",
        "#     return beam\n",
        "\n",
        "\n",
        "# def find_subject_to_predicate_path(\n",
        "#       subject_synset: wn.synset,\n",
        "#       predicate_synset: wn.synset,\n",
        "#       max_depth:int,\n",
        "#       visited=set(),\n",
        "#       max_sample_size=5,\n",
        "#     ):\n",
        "#     \"\"\"Find path from subject (noun) to predicate (verb).\"\"\"\n",
        "#     if subject_synset.name() in visited or predicate_synset.name() in visited:\n",
        "#       return None\n",
        "\n",
        "#     paths = []\n",
        "#     print()\n",
        "#     print(f\"Finding path from {subject_synset.name()} to {predicate_synset.name()}\")\n",
        "\n",
        "#     # Strategy 1: Look for active subjects in verb's gloss\n",
        "#     pred_gloss_doc = nlp(predicate_synset.definition())\n",
        "#     # passive subjects are semantically equivalent to objects\n",
        "#     active_subjects, _ = extract_subjects_from_gloss(pred_gloss_doc)\n",
        "#     # convert spacy tokens to lists of synsets\n",
        "#     subjects = [wn.synsets(s.text, pos=subject_synset.pos()) for s in active_subjects]\n",
        "#     # of the remaining subjects, get the most similar\n",
        "#     top_k = get_top_k_synset_branches(active_subjects[:max_sample_size], subject_synset)\n",
        "#     if top_k:\n",
        "#       print(f\"Found best matches for {subject_synset.name()}: {top_k} using strategy 1\")\n",
        "#       for matched_synset, _ in top_k:\n",
        "#         path = path_syn_to_syn(subject_synset, matched_synset, max_depth-1)\n",
        "#         if path:\n",
        "#             paths.append(path + [predicate_synset])\n",
        "\n",
        "#     # Strategy 2: Look for verbs in the noun's gloss\n",
        "#     subj_gloss_doc = nlp(subject_synset.definition())\n",
        "#     verbs = extract_verbs_from_gloss(subj_gloss_doc, include_passive=False)\n",
        "#     # convert spacy tokens to lists of synsets\n",
        "#     verbs = [wn.synsets(v.text, pos=predicate_synset.pos()) for v in verbs]\n",
        "#     # of the remaining subjects, get the most similar\n",
        "#     top_k = get_top_k_synset_branches(verbs[:max_sample_size], predicate_synset)\n",
        "#     if top_k:\n",
        "#       print(f\"Found best matches for {predicate_synset.name()}: {top_k} using strategy 2\")\n",
        "#       for matched_synset, _ in top_k:\n",
        "#         path = path_syn_to_syn(matched_synset, predicate_synset, max_depth-1)\n",
        "#         if path:\n",
        "#             paths.append([subject_synset] + path)\n",
        "\n",
        "#     # Strategy 3: Explore the 3 most promising pairs of neighbors\n",
        "#     subject_neighbors = get_all_neighbors(subject_synset)\n",
        "#     predicate_neighbors = get_all_neighbors(predicate_synset)\n",
        "#     top_k = get_k_closest_synset_pairs(subject_neighbors, predicate_neighbors)\n",
        "#     if top_k:\n",
        "#       print(f\"Most promising pairs for bidirectional exploration: {top_k} using strategy 3\")\n",
        "#       for s, p, _ in top_k:\n",
        "#         visited.add(subject_synset.name())\n",
        "#         visited.add(predicate_synset.name())\n",
        "#         path = find_subject_to_predicate_path(s, p, max_depth-1, visited)\n",
        "#         if path:\n",
        "#             paths.append([subject_synset] + path + [predicate_synset])\n",
        "\n",
        "\n",
        "#     # Return shortest path if any found\n",
        "#     return min(paths, key=len) if paths else None\n",
        "\n",
        "\n",
        "# def find_predicate_to_object_path(\n",
        "#       predicate_synset: wn.synset,\n",
        "#       object_synset: wn.synset,\n",
        "#       max_depth:int,\n",
        "#       visited=set(),\n",
        "#       max_sample_size=5,\n",
        "#     ):\n",
        "#     \"\"\"Find path from predicate (verb) to object (noun).\"\"\"\n",
        "\n",
        "#     if predicate_synset.name() in visited or object_synset.name() in visited:\n",
        "#       return None\n",
        "\n",
        "#     paths = []\n",
        "#     print()\n",
        "#     print(f\"Finding path from {predicate_synset.name()} to {object_synset.name()}\")\n",
        "\n",
        "#     # === Strategy 1: Objects in predicate gloss (incl. passive subjects) ===\n",
        "#     pred_gloss_doc = nlp(predicate_synset.definition())\n",
        "#     objects = extract_objects_from_gloss(pred_gloss_doc)\n",
        "#     _, passive_subjects = extract_subjects_from_gloss(pred_gloss_doc)\n",
        "#     objects.extend(passive_subjects)\n",
        "#     # convert spacy tokens to lists of synsets\n",
        "#     objects = [wn.synsets(o.text, pos=object_synset.pos()) for o in objects]\n",
        "#     top_k = get_top_k_synset_branches(objects[:max_sample_size], object_synset)\n",
        "#     if top_k:\n",
        "#       print(f\"Found best matches for {object_synset.name()}: {top_k} using strategy 1\")\n",
        "#       for matched_synset, _ in top_k:\n",
        "#         path = path_syn_to_syn(matched_synset, object_synset, max_depth-1)\n",
        "#         if path:\n",
        "#             paths.append([predicate_synset] + path)\n",
        "\n",
        "#     # === Strategy 2: Verbs in object's gloss ===\n",
        "#     obj_gloss_doc = nlp(object_synset.definition())\n",
        "#     verbs = extract_verbs_from_gloss(obj_gloss_doc, include_passive=True)\n",
        "#     # Use instrumental verbs in object's gloss as backup\n",
        "#     verbs.extend(find_instrumental_verbs(obj_gloss_doc))\n",
        "#     # convert spacy tokens to lists of synsets\n",
        "#     verbs = [wn.synsets(v.text, pos=predicate_synset.pos()) for v in verbs]\n",
        "#     top_k = get_top_k_synset_branches(verbs[:max_sample_size], predicate_synset)\n",
        "#     if top_k:\n",
        "#       print(f\"Found best matches for {predicate_synset.name()}: {top_k} using strategy 2\")\n",
        "#       for matched_synset, _ in top_k:\n",
        "#         path = path_syn_to_syn(predicate_synset, matched_synset, max_depth-1)\n",
        "#         if path:\n",
        "#             paths.append(path + [object_synset])\n",
        "\n",
        "#     # Strategy 3: Explore the 3 most promising neighbors\n",
        "#     predicate_neighbors = get_all_neighbors(predicate_synset)\n",
        "#     object_neighbors = get_all_neighbors(object_synset)\n",
        "#     top_k = get_k_closest_synset_pairs(predicate_neighbors, object_neighbors)\n",
        "#     if top_k:\n",
        "#       print(f\"Most promising pairs for bidirectional exploration: {top_k} using strategy 3\")\n",
        "#       for p, o, _ in top_k:\n",
        "#         visited.add(predicate_synset.name())\n",
        "#         visited.add(object_synset.name())\n",
        "#         path = find_predicate_to_object_path(p, o, max_depth-1, visited)\n",
        "#         if path:\n",
        "#             paths.append([predicate_synset] + path + [object_synset])\n",
        "\n",
        "\n",
        "#     # Return shortest path if any found\n",
        "#     return min(paths, key=len) if paths else None\n",
        "\n",
        "\n",
        "# # ============================================================================\n",
        "# # Main Connected Path Finding Function\n",
        "# # ============================================================================\n",
        "\n",
        "# def find_connected_shortest_paths(\n",
        "#       subject_word,\n",
        "#       predicate_word,\n",
        "#       object_word,\n",
        "#       max_depth=10,\n",
        "#       max_self_intersection=5\n",
        "#     ):\n",
        "#     \"\"\"\n",
        "#     Find shortest connected paths from subject through predicate to object.\n",
        "#     Ensures that the same predicate synset connects both paths.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Get synsets for each word\n",
        "#     subject_synsets = wn.synsets(subject_word, pos=wn.NOUN)\n",
        "#     predicate_synsets = wn.synsets(predicate_word, pos=wn.VERB)\n",
        "#     object_synsets = wn.synsets(object_word, pos=wn.NOUN)\n",
        "\n",
        "#     best_combined_path_length = float('inf')\n",
        "#     best_subject_path = None\n",
        "#     best_object_path = None\n",
        "#     best_predicate = None\n",
        "\n",
        "#     # Try each predicate synset as the connector\n",
        "#     for pred in predicate_synsets:\n",
        "#         # Find paths from all subjects to this specific predicate\n",
        "#         subject_paths = []\n",
        "#         for subj in subject_synsets:\n",
        "#             path = find_subject_to_predicate_path(subj, pred, max_depth)\n",
        "#             if path:\n",
        "#                 subject_paths.append(path)\n",
        "\n",
        "#         # Find paths from this specific predicate to all objects\n",
        "#         object_paths = []\n",
        "#         for obj in object_synsets:\n",
        "#             path = find_predicate_to_object_path(pred, obj, max_depth)\n",
        "#             if path:\n",
        "#                 object_paths.append(path)\n",
        "\n",
        "#         # If we have both paths through this predicate, check if it's the best\n",
        "#         if subject_paths and object_paths:\n",
        "#             # find pairs of paths that don't intersect with eachother\n",
        "#             #   i.e. burglar > break_in > attack > strike > shoot > strike > attack > woman\n",
        "#             #   would not be allowed, since tautological statements are uninformative\n",
        "#             valid_pairs = list()\n",
        "#             for subj_path in subject_paths:\n",
        "#               for obj_path in object_paths:\n",
        "#                 if len(set(subj_path).intersection(set(obj_path))) <= max_self_intersection:\n",
        "#                   valid_pairs.append((\n",
        "#                       subj_path,\n",
        "#                       obj_path,\n",
        "#                       # Calculate combined length (subtract 1 to avoid counting predicate twice)\n",
        "#                       len(subj_path) + len(obj_path) - 1\n",
        "#                   ))\n",
        "\n",
        "#             if not valid_pairs:\n",
        "#               print(f\"No valid pairs of subj, obj paths found for {pred.name()}\")\n",
        "#               break\n",
        "\n",
        "#             shortest_comb_path = min(valid_pairs, key=lambda x: x[2])\n",
        "\n",
        "#             if shortest_comb_path[2] < best_combined_path_length:\n",
        "#                 best_combined_path_length = shortest_comb_path[2]\n",
        "#                 best_subject_path = shortest_comb_path[0]\n",
        "#                 best_object_path = shortest_comb_path[1]\n",
        "#                 best_predicate = pred\n",
        "\n",
        "#     return best_subject_path, best_object_path, best_predicate\n",
        "\n",
        "\n",
        "# # ============================================================================\n",
        "# # Display Functions\n",
        "# # ============================================================================\n",
        "\n",
        "# def show_path(label, path):\n",
        "#     \"\"\"Pretty print a path of synsets.\"\"\"\n",
        "#     if path:\n",
        "#         print(f\"{label}:\")\n",
        "#         print(\" -> \".join(f\"{s.name()} ({s.definition()})\" for s in path))\n",
        "#         print(f\"Path length: {len(path)}\")\n",
        "#         print()\n",
        "#     else:\n",
        "#         print(f\"{label}: No path found\")\n",
        "#         print()\n",
        "\n",
        "\n",
        "# def show_connected_paths(subject_path, object_path, predicate):\n",
        "#     \"\"\"Display the connected paths with their shared predicate.\"\"\"\n",
        "#     if subject_path and object_path and predicate:\n",
        "#         print(\"=\" * 70)\n",
        "#         print(f\"CONNECTED PATH through predicate: {predicate.name()}\")\n",
        "#         print(\"=\" * 70)\n",
        "\n",
        "#         show_path(\"Subject -> Predicate path\", subject_path)\n",
        "#         show_path(\"Predicate -> Object path\", object_path)\n",
        "\n",
        "#         # Show the complete connected path\n",
        "#         complete_path = subject_path + object_path[1:]  # Avoid duplicating the predicate\n",
        "#         print(\"Complete connected path:\")\n",
        "#         print(\" -> \".join(f\"{s.name()}\" for s in complete_path))\n",
        "#         print(f\"Total path length: {len(complete_path)}\")\n",
        "#         print()\n",
        "#     else:\n",
        "#         print(\"No connected path found through any predicate synset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "bwShzrxtIhOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = wn_to_nx()"
      ],
      "metadata": {
        "id": "c1XSDI7phuvN"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example"
      ],
      "metadata": {
        "id": "9qMzn_CMzk0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Example call of find_connected_paths\n",
        "# ------------------------------\n",
        "results = find_connected_paths(\n",
        "    g=g,\n",
        "    subject_word=\"dog\",\n",
        "    predicate_word=\"chase\",\n",
        "    object_word=\"cat\",\n",
        "    wn_module=wn,\n",
        "    nlp_func=nlp,\n",
        "    get_new_beams_fn=lambda G, s, t: get_new_beams_from_embeddings(\n",
        "        G, s, t, wn_module=wn, model=embedding_model, beam_width=4\n",
        "    ),\n",
        "    top_k_branch_fn=lambda candidates, target, bw: top_k_branch_wrapper(\n",
        "        candidates, target, beam_width=bw, model=embedding_model, wn_module=wn\n",
        "    ),\n",
        "    extract_subjects_from_gloss=extract_subjects_from_gloss,\n",
        "    extract_objects_from_gloss=extract_objects_from_gloss,\n",
        "    extract_verbs_from_gloss=extract_verbs_from_gloss,  # This parameter is now included\n",
        "    beam_width=10,\n",
        "    max_depth=10,\n",
        "    max_self_intersection=5,\n",
        "    max_results_per_pair=10,\n",
        "    len_tolerance=5,\n",
        "    relax_beam=True,\n",
        "    max_sample_size=5  # Also added this parameter for controlling gloss token sampling\n",
        ")\n",
        "\n",
        "# Print the top results\n",
        "for idx, r in enumerate(results[:10], start=1):\n",
        "    print(f\"Result #{idx}\")\n",
        "    print(\" Predicate synset:\", r[\"predicate_synset\"])\n",
        "    print(\" Combined length:\", r[\"combined_len\"])\n",
        "    print(\" Combined cost:\", r[\"combined_cost\"])\n",
        "    print(\" Combined path:\")\n",
        "    print(\"  -> \".join(r[\"combined_path\"]))\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "mDjnq9s_w29o",
        "outputId": "6a013290-7f4c-49e8-be8c-1bcb056051cd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "find_connected_paths() got an unexpected keyword argument 'extract_verbs_from_gloss'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-456310539.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Example call of find_connected_paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m results = find_connected_paths(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msubject_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dog\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: find_connected_paths() got an unexpected keyword argument 'extract_verbs_from_gloss'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfiM249sw_1s",
        "outputId": "b99d62a1-99ba-4009-93ad-a841bd348391"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diagnostic_run(\n",
        "    g=g,\n",
        "    src_name=\"dog.n.01\",\n",
        "    tgt_name=\"cat.n.01\",\n",
        "    wn_module=wn,\n",
        "    nlp_func=nlp,\n",
        "    embedding_model=embedding_model,\n",
        "    get_new_beams_fn_wrapped=lambda G,s,t: get_new_beams_from_embeddings(G,s,t, wn_module=wn, model=embedding_model, beam_width=6),\n",
        "    build_gloss_seed_nodes_fn=lambda pred, wn_mod, nlpfn: build_gloss_seed_nodes_from_predicate(pred, wn_mod, nlpfn,\n",
        "                                                                                              extract_subjects_fn=globals().get('extract_subjects_from_gloss'),\n",
        "                                                                                              extract_objects_fn=globals().get('extract_objects_from_gloss'),\n",
        "                                                                                              extract_verbs_fn=globals().get('extract_verbs_from_gloss'),\n",
        "                                                                                              top_k_branch_fn=lambda cand, target, bw: top_k_branch_wrapper(cand, target, bw, model=embedding_model, wn_module=wn),\n",
        "                                                                                              mode=\"subjects\",\n",
        "                                                                                              max_sample_size=6,\n",
        "                                                                                              beam_width=6),\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dvVuRcGtJdN",
        "outputId": "cd4edd85-b41b-42dc-976a-56d4e7452bee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Diagnostic start ===\n",
            "src: dog.n.01\n",
            "tgt: cat.n.01\n",
            "\n",
            "1) Node presence checks:\n",
            " g has src node? True\n",
            " g has tgt node? True\n",
            "\n",
            "2) Connectivity check using networkx:\n",
            " Undirected connectivity between src and tgt? True\n",
            " Example undirected shortest path (hop count): 3 hops. path sample: ['dog.n.01', 'domestic_animal.n.01', 'domestic_cat.n.01', 'cat.n.01']\n",
            "\n",
            "3) Node neighborhood:\n",
            " src out-neighbors (sample): ['canis.n.01', 'pack.n.06', 'flag.n.07', 'domestic_animal.n.01', 'canine.n.02', 'corgi.n.01', 'leonberg.n.01', 'cur.n.01', 'poodle.n.01', 'pug.n.01']\n",
            " tgt in-neighbors (sample): ['feline.n.01', 'domestic_cat.n.01', 'wildcat.n.03']\n",
            "\n",
            "4) Embedding beam seeding (get_new_beams_fn):\n",
            " beams count: 6\n",
            "[(('canine.n.02', 'hypernyms'),\n",
            "  ('feline.n.01', 'hypernyms'),\n",
            "  0.6772804990703681),\n",
            " (('domestic_animal.n.01', 'hypernyms'),\n",
            "  ('domestic_cat.n.01', 'hyponyms'),\n",
            "  0.6423323762382909),\n",
            " (('pooch.n.01', 'hyponyms'), ('feline.n.01', 'hypernyms'), 0.634097330718018),\n",
            " (('canine.n.02', 'hypernyms'),\n",
            "  ('domestic_cat.n.01', 'hyponyms'),\n",
            "  0.6148699834455494),\n",
            " (('pug.n.01', 'hyponyms'), ('feline.n.01', 'hypernyms'), 0.5972350530420178),\n",
            " (('newfoundland.n.01', 'hyponyms'),\n",
            "  ('feline.n.01', 'hypernyms'),\n",
            "  0.5958320114102831)]\n",
            "\n",
            "5) Gloss seeds (from predicate gloss):\n",
            " gloss seeds count: 1\n",
            " seeds sample: ['mammal.n.01']\n",
            "\n",
            "6) Run PairwiseBidirectionalAStar with relaxed settings (beam_width=10, relax_beam=True, max_depth=10)\n",
            " found paths (count): 4\n",
            " cost: 3.0  hops: 3  path: ['dog.n.01', 'domestic_animal.n.01', 'domestic_cat.n.01', 'cat.n.01']\n",
            " cost: 4.0  hops: 4  path: ['dog.n.01', 'canine.n.02', 'paw.n.01', 'feline.n.01', 'cat.n.01']\n",
            " cost: 5.0  hops: 5  path: ['dog.n.01', 'canis.n.01', 'mammal_genus.n.01', 'felis.n.01', 'wildcat.n.03', 'cat.n.01']\n",
            " cost: 4.0  hops: 4  path: ['dog.n.01', 'canine.n.02', 'carnivore.n.01', 'feline.n.01', 'cat.n.01']\n",
            "\n",
            "=== Diagnostic end ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1h0Rj1Ohva9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diagnostics"
      ],
      "metadata": {
        "id": "KsdJQGddzhq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug / diagnostic script\n",
        "from pprint import pprint\n",
        "import networkx as nx\n",
        "\n",
        "def diagnostic_run(\n",
        "    g, src_name, tgt_name, wn_module, nlp_func, embedding_model,\n",
        "    get_new_beams_fn_wrapped, build_gloss_seed_nodes_fn,\n",
        "    pairwise_class=PairwiseBidirectionalAStar\n",
        "):\n",
        "    print(\"=== Diagnostic start ===\")\n",
        "    print(\"src:\", src_name)\n",
        "    print(\"tgt:\", tgt_name)\n",
        "    print()\n",
        "\n",
        "    # 1) Node presence\n",
        "    print(\"1) Node presence checks:\")\n",
        "    print(\" g has src node?\", g.has_node(src_name))\n",
        "    print(\" g has tgt node?\", g.has_node(tgt_name))\n",
        "    if not g.has_node(src_name) or not g.has_node(tgt_name):\n",
        "        print(\" -> Node name mismatch likely. Check g.nodes() samples:\")\n",
        "        print(\"  sample nodes:\", list(g.nodes)[:20])\n",
        "        print(\"Stopping diagnostics early (node mismatch).\")\n",
        "        return\n",
        "\n",
        "    # 2) Connectivity check (fast)\n",
        "    print(\"\\n2) Connectivity check using networkx:\")\n",
        "    try:\n",
        "        has_path = nx.has_path(g.to_undirected(), src_name, tgt_name)\n",
        "    except Exception:\n",
        "        # graceful fallback for directed graphs if conversion fails\n",
        "        has_path = nx.has_path(g, src_name, tgt_name) if nx.is_directed(g) else False\n",
        "    print(\" Undirected connectivity between src and tgt?\", has_path)\n",
        "    if not has_path:\n",
        "        print(\" -> Graph disconnected between src and tgt. Try increasing graph connectivity or check edges.\")\n",
        "    else:\n",
        "        try:\n",
        "            sp = nx.shortest_path(g.to_undirected(), src_name, tgt_name)\n",
        "            print(\" Example undirected shortest path (hop count):\", len(sp)-1, \"hops. path sample:\", sp[:10])\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # 3) print node degrees and first neighbors\n",
        "    print(\"\\n3) Node neighborhood:\")\n",
        "    try:\n",
        "        nbrs_src = list(g.neighbors(src_name))[:10]\n",
        "    except Exception:\n",
        "        nbrs_src = []\n",
        "    try:\n",
        "        preds_tgt = list(g.predecessors(tgt_name))[:10]\n",
        "    except Exception:\n",
        "        preds_tgt = []\n",
        "    print(\" src out-neighbors (sample):\", nbrs_src)\n",
        "    print(\" tgt in-neighbors (sample):\", preds_tgt)\n",
        "\n",
        "    # 4) Beam seeding via embeddings\n",
        "    print(\"\\n4) Embedding beam seeding (get_new_beams_fn):\")\n",
        "    try:\n",
        "        beams = get_new_beams_fn_wrapped(g, src_name, tgt_name)\n",
        "        print(\" beams count:\", len(beams))\n",
        "        pprint(beams[:10])\n",
        "    except Exception as e:\n",
        "        print(\" get_new_beams_fn raised exception:\", e)\n",
        "        beams = []\n",
        "\n",
        "    # 5) Gloss seed extraction for predicate (if available)\n",
        "    print(\"\\n5) Gloss seeds (from predicate gloss):\")\n",
        "    try:\n",
        "        # attempt to get a predicate synset if tgt is a predicate; otherwise try wn.synset(tgt)\n",
        "        try:\n",
        "            pred_syn = wn_module.synset(tgt_name)\n",
        "        except Exception:\n",
        "            pred_syn = None\n",
        "        seeds = set()\n",
        "        if pred_syn:\n",
        "            seeds = build_gloss_seed_nodes_fn(pred_syn, wn_module, nlp_func)\n",
        "        print(\" gloss seeds count:\", len(seeds))\n",
        "        print(\" seeds sample:\", list(seeds)[:10])\n",
        "    except Exception as e:\n",
        "        print(\" gloss seeding raised exception:\", e)\n",
        "        seeds = set()\n",
        "\n",
        "    # 6) Run pairwise search with relaxed settings (wider beam, relax_beam True)\n",
        "    print(\"\\n6) Run PairwiseBidirectionalAStar with relaxed settings (beam_width=10, relax_beam=True, max_depth=10)\")\n",
        "    try:\n",
        "        search = pairwise_class(\n",
        "            g=g,\n",
        "            src=src_name,\n",
        "            tgt=tgt_name,\n",
        "            get_new_beams_fn=lambda G, s, t: get_new_beams_fn_wrapped(G, s, t),\n",
        "            gloss_seed_nodes=seeds,\n",
        "            beam_width=10,\n",
        "            max_depth=10,\n",
        "            relax_beam=True\n",
        "        )\n",
        "        paths = search.find_paths(max_results=5, len_tolerance=2)\n",
        "        print(\" found paths (count):\", len(paths))\n",
        "        for p, cost in paths:\n",
        "            print(\" cost:\", cost, \" hops:\", len(p)-1, \" path:\", p)\n",
        "    except Exception as e:\n",
        "        print(\" pairwise search raised exception:\", e)\n",
        "\n",
        "    print(\"\\n=== Diagnostic end ===\")\n"
      ],
      "metadata": {
        "id": "_s3EqGiVvmhC"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}
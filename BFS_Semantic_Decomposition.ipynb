{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbxT8xxo09iO31zqzHBHzm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/SMIED/blob/main/BFS_Semantic_Decomposition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Semantic decomposition of (\"cat\", \"eats\", \"mouse\") using WordNet + spaCy + depth-limited GBFS.\n",
        "- Uses spaCy to parse verb synset glosses and detect subject/object dependencies.\n",
        "- If both subject and object tokens are present, branches directly toward original triple synsets.\n",
        "- Otherwise falls back to WordNet relations.\n",
        "\"\"\"\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.wsd import lesk\n",
        "from heapq import heappush, heappop"
      ],
      "metadata": {
        "id": "Jb_gtRCvh-DF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "# Load spaCy English model for dependency parsing\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj4iZlAfh_t8",
        "outputId": "93488893-eda4-41f3-9b0f-9e4c3eecc62d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GZ7mH1XuhtBN"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.wsd import lesk\n",
        "import spacy\n",
        "\n",
        "# Initialize spaCy (assuming you have it loaded)\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def path_syn_to_syn(start_synset, end_synset, max_depth=6):\n",
        "    \"\"\"\n",
        "    Find shortest path between synsets of the same POS using bidirectional BFS.\n",
        "    Returns a list of synsets forming the path, or None if no path found.\n",
        "    \"\"\"\n",
        "\n",
        "    assert start_synset.pos() == end_synset.pos() and start_synset.pos() in {'n', 'v'}\n",
        "\n",
        "    # Handle the trivial case where start and end are the same\n",
        "    if start_synset.name() == end_synset.name():\n",
        "        return [start_synset]\n",
        "\n",
        "    # Initialize two search frontiers\n",
        "    # Forward search (from start)\n",
        "    forward_queue = deque([(start_synset, 0)])  # (synset, depth)\n",
        "    forward_visited = {start_synset.name(): [start_synset]}  # Maps synset name to path from start\n",
        "\n",
        "    # Backward search (from end)\n",
        "    backward_queue = deque([(end_synset, 0)])  # (synset, depth)\n",
        "    backward_visited = {end_synset.name(): [end_synset]}  # Maps synset name to path from end\n",
        "\n",
        "    def get_neighbors(synset):\n",
        "        \"\"\"Helper function to get all neighbors of a synset.\"\"\"\n",
        "        neighbors = []\n",
        "\n",
        "        # Add hypernyms and hyponyms\n",
        "        neighbors.extend(synset.hypernyms())\n",
        "        neighbors.extend(synset.hyponyms())\n",
        "\n",
        "        # Add POS-specific neighbors\n",
        "        if synset.pos() == 'n':\n",
        "            neighbors.extend(get_noun_neighbors(synset))\n",
        "        else:\n",
        "            neighbors.extend(get_verb_neighbors(synset))\n",
        "\n",
        "        return neighbors\n",
        "\n",
        "    def expand_frontier(queue, visited_from_this_side, visited_from_other_side, is_forward):\n",
        "        \"\"\"\n",
        "        Expand one step of the search frontier.\n",
        "        Returns the complete path if intersection found, None otherwise.\n",
        "        \"\"\"\n",
        "        if not queue:\n",
        "            return None\n",
        "\n",
        "        curr_synset, depth = queue.popleft()\n",
        "\n",
        "        # Don't expand beyond max_depth/2 for each direction\n",
        "        if depth >= (max_depth + 1) // 2:\n",
        "            return None\n",
        "\n",
        "        # Get the path to current synset\n",
        "        path_to_current = visited_from_this_side[curr_synset.name()]\n",
        "\n",
        "        # Explore neighbors\n",
        "        for neighbor in get_neighbors(curr_synset):\n",
        "            neighbor_name = neighbor.name()\n",
        "\n",
        "            # If we've already visited this node from this direction, skip it\n",
        "            if neighbor_name in visited_from_this_side:\n",
        "                continue\n",
        "\n",
        "            # Create the path to this neighbor\n",
        "            if is_forward:\n",
        "                new_path = path_to_current + [neighbor]\n",
        "            else:\n",
        "                new_path = [neighbor] + path_to_current\n",
        "\n",
        "            # Check if we've found an intersection with the other search\n",
        "            if neighbor_name in visited_from_other_side:\n",
        "                # We found a meeting point! Reconstruct the full path\n",
        "                other_path = visited_from_other_side[neighbor_name]\n",
        "\n",
        "                if is_forward:\n",
        "                    # Forward path + backward path (reversed, excluding the meeting point)\n",
        "                    full_path = path_to_current + other_path\n",
        "                else:\n",
        "                    # Forward path + backward path (excluding the meeting point)\n",
        "                    full_path = other_path + path_to_current\n",
        "\n",
        "                return full_path\n",
        "\n",
        "            # Add to visited and queue\n",
        "            visited_from_this_side[neighbor_name] = new_path\n",
        "            queue.append((neighbor, depth + 1))\n",
        "\n",
        "        return None\n",
        "\n",
        "    # Alternate between forward and backward search\n",
        "    while forward_queue or backward_queue:\n",
        "        # Expand forward frontier\n",
        "        if forward_queue:\n",
        "            result = expand_frontier(forward_queue, forward_visited, backward_visited, is_forward=True)\n",
        "            if result:\n",
        "                return result\n",
        "\n",
        "        # Expand backward frontier\n",
        "        if backward_queue:\n",
        "            result = expand_frontier(backward_queue, backward_visited, forward_visited, is_forward=False)\n",
        "            if result:\n",
        "                return result\n",
        "\n",
        "    return None  # No path found\n",
        "\n",
        "\n",
        "def get_noun_neighbors(syn):\n",
        "    \"\"\"\n",
        "    Get neighbors for a noun synset.\n",
        "    \"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.part_meronyms())\n",
        "    nbrs.update(syn.substance_meronyms())\n",
        "    nbrs.update(syn.member_meronyms())\n",
        "    nbrs.update(syn.part_holonyms())\n",
        "    nbrs.update(syn.substance_holonyms())\n",
        "    nbrs.update(syn.member_holonyms())\n",
        "    return list(nbrs)\n",
        "\n",
        "\n",
        "def get_verb_neighbors(syn):\n",
        "    \"\"\"\n",
        "    Get neighbors for a verb synset.\n",
        "    \"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.entailments())\n",
        "    nbrs.update(syn.causes())\n",
        "    nbrs.update(syn.also_sees())\n",
        "    nbrs.update(syn.verb_groups())\n",
        "    return list(nbrs)\n",
        "\n",
        "\n",
        "def cross_pos_path(start_synset, end_synset, max_depth=6):\n",
        "    \"\"\"\n",
        "    Find path between synsets of different POS using gloss analysis.\n",
        "    Returns a list of synsets forming the path, or None if no path found.\n",
        "    \"\"\"\n",
        "\n",
        "    assert start_synset.pos() != end_synset.pos() and start_synset.pos() in {'n', 'v'}\n",
        "\n",
        "    # If start is a noun and end is a verb (subject -> predicate)\n",
        "    if start_synset.pos() == 'n':\n",
        "        # Strategy 1: Look for subject references in the verb's gloss\n",
        "        pred_gloss_doc = nlp(end_synset.definition())\n",
        "        subjs = [tok for tok in pred_gloss_doc if tok.dep_ == \"nsubj\"]\n",
        "\n",
        "        if subjs:\n",
        "            try:\n",
        "                subject_synset = lesk(pred_gloss_doc.text, subjs[0].text, pos='n')\n",
        "                if subject_synset:\n",
        "                    # Find path from start to the subject mentioned in verb's gloss\n",
        "                    path = path_syn_to_syn(start_synset, subject_synset, max_depth=max_depth)\n",
        "                    if path:\n",
        "                        # Add the verb at the end to complete the cross-POS path\n",
        "                        return path + [end_synset]\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Strategy 2: Look for verb references in the noun's gloss\n",
        "        subj_gloss_doc = nlp(start_synset.definition())\n",
        "        preds = [tok for tok in subj_gloss_doc if tok.pos_ == \"VERB\"]\n",
        "\n",
        "        if preds:\n",
        "            try:\n",
        "                subj_pred_synset = lesk(subj_gloss_doc.text, preds[0].text, pos='v')\n",
        "                if subj_pred_synset:\n",
        "                    # Find path from verb found in subj gloss to the pred synset\n",
        "                    path = path_syn_to_syn(subj_pred_synset, end_synset, max_depth=max_depth)\n",
        "                    if path:\n",
        "                        # Prepend the noun to complete the cross-POS path\n",
        "                        return [start_synset] + path\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    # If start is a verb and end is a noun (predicate -> object)\n",
        "    elif start_synset.pos() == 'v':\n",
        "        # Strategy 1: Look for direct objects in the verb's gloss\n",
        "        pred_gloss_doc = nlp(start_synset.definition())\n",
        "\n",
        "        # Expand to look for multiple types of objects\n",
        "        objs = []\n",
        "        # Direct objects\n",
        "        objs.extend([tok for tok in pred_gloss_doc if tok.dep_ == \"dobj\"])\n",
        "        # Prepositional objects (e.g., \"eat at a table\")\n",
        "        objs.extend([tok for tok in pred_gloss_doc if tok.dep_ == \"pobj\"])\n",
        "        # Indirect objects (e.g., \"give someone something\")\n",
        "        objs.extend([tok for tok in pred_gloss_doc if tok.dep_ == \"iobj\"])\n",
        "        # Objects of prepositions that relate to the main verb\n",
        "        objs.extend([tok for tok in pred_gloss_doc if tok.dep_ == \"obj\"])\n",
        "\n",
        "        # Also look for noun chunks that might be objects\n",
        "        if not objs:\n",
        "            # Look for nouns that are children of the root verb\n",
        "            root_verbs = [tok for tok in pred_gloss_doc if tok.dep_ == \"ROOT\" and tok.pos_ == \"VERB\"]\n",
        "            if root_verbs:\n",
        "                for noun_chunk in pred_gloss_doc.noun_chunks:\n",
        "                    # Check if this noun chunk is related to the main verb\n",
        "                    if any(token.head == root_verbs[0] for token in noun_chunk):\n",
        "                        objs.append(noun_chunk.root)\n",
        "\n",
        "        if objs:\n",
        "            # Try multiple object candidates\n",
        "            for obj in objs[:3]:  # Limit to first 3 to avoid excessive computation\n",
        "                try:\n",
        "                    object_synset = lesk(pred_gloss_doc.text, obj.text, pos='n')\n",
        "                    if object_synset:\n",
        "                        # Find path from object mentioned in verb's gloss to end\n",
        "                        path = path_syn_to_syn(object_synset, end_synset, max_depth=max_depth)\n",
        "                        if path:\n",
        "                            # Add the verb at the beginning to complete the cross-POS path\n",
        "                            return [start_synset] + path\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # Strategy 2: Look for verb references in the object's gloss\n",
        "        obj_gloss_doc = nlp(end_synset.definition())\n",
        "\n",
        "        # Look for verbs that might describe actions done to/with this object\n",
        "        verbs = [tok for tok in obj_gloss_doc if tok.pos_ == \"VERB\"]\n",
        "\n",
        "        # Also check if the object is described as something that gets verbed\n",
        "        # (e.g., \"food: something that is eaten\")\n",
        "        passive_verbs = [tok for tok in obj_gloss_doc if tok.tag_ in [\"VBN\", \"VBD\"] and tok.dep_ in [\"acl\", \"relcl\"]]\n",
        "        verbs.extend(passive_verbs)\n",
        "\n",
        "        if verbs:\n",
        "            # Try multiple verb candidates\n",
        "            for verb in verbs[:3]:  # Limit to first 3\n",
        "                try:\n",
        "                    obj_verb_synset = lesk(obj_gloss_doc.text, verb.text, pos='v')\n",
        "                    if obj_verb_synset:\n",
        "                        # Find path from start verb to verb found in object's gloss\n",
        "                        path = path_syn_to_syn(start_synset, obj_verb_synset, max_depth=max_depth)\n",
        "                        if path:\n",
        "                            # Append the noun to complete the cross-POS path\n",
        "                            return path + [end_synset]\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # Strategy 3: Check for \"used for\" or \"used in\" relationships in object's gloss\n",
        "        # This helps with instrumental objects (e.g., \"fork\" used for \"eating\")\n",
        "        if \"used\" in obj_gloss_doc.text.lower():\n",
        "            # Look for verbs following \"used for/to/in\"\n",
        "            for i, token in enumerate(obj_gloss_doc):\n",
        "                if token.text.lower() == \"used\":\n",
        "                    # Check tokens after \"used\"\n",
        "                    for j in range(i+1, min(i+4, len(obj_gloss_doc))):\n",
        "                        if obj_gloss_doc[j].pos_ == \"VERB\":\n",
        "                            try:\n",
        "                                use_verb_synset = lesk(obj_gloss_doc.text, obj_gloss_doc[j].text, pos='v')\n",
        "                                if use_verb_synset:\n",
        "                                    path = path_syn_to_syn(start_synset, use_verb_synset, max_depth=max_depth)\n",
        "                                    if path:\n",
        "                                        return path + [end_synset]\n",
        "                            except:\n",
        "                                continue\n",
        "\n",
        "    # If gloss-based approach fails, return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def find_shortest_paths(subject_word, predicate_word, object_word, max_depth=10):\n",
        "    \"\"\"\n",
        "    Find shortest paths from subject to predicate and predicate to object.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get synsets for each word\n",
        "    subject_synsets = wn.synsets(subject_word, pos=wn.NOUN)\n",
        "    predicate_synsets = wn.synsets(predicate_word, pos=wn.VERB)\n",
        "    object_synsets = wn.synsets(object_word, pos=wn.NOUN)\n",
        "\n",
        "    # Find all possible paths from subject to predicate\n",
        "    subject_paths = []\n",
        "    for subj in subject_synsets:\n",
        "        for pred in predicate_synsets:\n",
        "            path = cross_pos_path(subj, pred, max_depth=max_depth)\n",
        "            if path:\n",
        "                subject_paths.append(path)\n",
        "\n",
        "    # Find all possible paths from predicate to object\n",
        "    object_paths = []\n",
        "    for pred in predicate_synsets:\n",
        "        for obj in object_synsets:\n",
        "            path = cross_pos_path(pred, obj, max_depth=max_depth)\n",
        "            if path:\n",
        "                object_paths.append(path)\n",
        "\n",
        "    # Get the shortest paths (if any found)\n",
        "    shortest_subject_path = min(subject_paths, key=len) if subject_paths else None\n",
        "    shortest_object_path = min(object_paths, key=len) if object_paths else None\n",
        "\n",
        "    return shortest_subject_path, shortest_object_path\n",
        "\n",
        "\n",
        "def show_path(label, path):\n",
        "    \"\"\"\n",
        "    Pretty print a path of synsets.\n",
        "    \"\"\"\n",
        "    if path:\n",
        "        print(f\"{label}:\")\n",
        "        print(\" -> \".join(f\"{s.name()} ({s.definition()})\" for s in path))\n",
        "        print(f\"Path length: {len(path)}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"{label}: No path found\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Find shortest paths\n",
        "cat_path, mouse_path = find_shortest_paths(\"cat\", \"eat\", \"mouse\", max_depth=10)\n",
        "\n",
        "# Display results\n",
        "show_path(\"Path from 'cat' to 'eat' (subject -> predicate)\", cat_path)\n",
        "show_path(\"Path from 'eat' to 'mouse' (predicate -> object)\", mouse_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "valgVwQUh0SJ",
        "outputId": "c83e0da3-3266-4258-ff0a-98074273e972"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path from 'cat' to 'eat' (subject -> predicate):\n",
            "kat.n.01 (the leaves of the shrub Catha edulis which are chewed like tobacco or used to make tea; has the effect of a euphoric stimulant) -> chew.v.01 (chew (food); to bite and grind with the teeth) -> eat.v.01 (take in solid food)\n",
            "Path length: 3\n",
            "\n",
            "Path from 'eat' to 'mouse' (predicate -> object):\n",
            "corrode.v.01 (cause to deteriorate due to the action of water, air, or an acid) -> natural_process.n.01 (a process existing in or produced by nature (rather than by the intent of human beings)) -> process.n.06 (a sustained phenomenon or one marked by gradual changes through a series of states) -> physical_entity.n.01 (an entity that has physical existence) -> causal_agent.n.01 (any entity that produces an effect or is responsible for events or results) -> person.n.01 (a human being) -> mouse.n.03 (person who is quiet or timid)\n",
            "Path length: 7\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EMRfljk7h1no"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}
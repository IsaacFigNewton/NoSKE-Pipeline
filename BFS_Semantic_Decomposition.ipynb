{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHjXMKB6JmNo4zFZNTYbS3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/SMIED/blob/main/BFS_Semantic_Decomposition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Semantic decomposition of (\"cat\", \"eats\", \"mouse\") using WordNet + spaCy + depth-limited GBFS.\n",
        "- Uses spaCy to parse verb synset glosses and detect subject/object dependencies.\n",
        "- If both subject and object tokens are present, branches directly toward original triple synsets.\n",
        "- Otherwise falls back to WordNet relations.\n",
        "\"\"\"\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.wsd import lesk\n",
        "from heapq import heappush, heappop"
      ],
      "metadata": {
        "id": "Jb_gtRCvh-DF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "# Load spaCy English model for dependency parsing\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj4iZlAfh_t8",
        "outputId": "93488893-eda4-41f3-9b0f-9e4c3eecc62d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GZ7mH1XuhtBN"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.wsd import lesk\n",
        "import spacy\n",
        "\n",
        "# Initialize spaCy (assuming you have it loaded)\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# ============================================================================\n",
        "# Core Path Finding Functions\n",
        "# ============================================================================\n",
        "\n",
        "def path_syn_to_syn(start_synset, end_synset, max_depth=6):\n",
        "    \"\"\"\n",
        "    Find shortest path between synsets of the same POS using bidirectional BFS.\n",
        "    Returns a list of synsets forming the path, or None if no path found.\n",
        "    \"\"\"\n",
        "\n",
        "    assert start_synset.pos() == end_synset.pos() and start_synset.pos() in {'n', 'v'}\n",
        "\n",
        "    # Handle the trivial case where start and end are the same\n",
        "    if start_synset.name() == end_synset.name():\n",
        "        return [start_synset]\n",
        "\n",
        "    # Initialize two search frontiers\n",
        "    forward_queue = deque([(start_synset, 0)])\n",
        "    forward_visited = {start_synset.name(): [start_synset]}\n",
        "\n",
        "    backward_queue = deque([(end_synset, 0)])\n",
        "    backward_visited = {end_synset.name(): [end_synset]}\n",
        "\n",
        "    def expand_frontier(queue, visited_from_this_side, visited_from_other_side, is_forward):\n",
        "        \"\"\"Expand one step of the search frontier.\"\"\"\n",
        "        if not queue:\n",
        "            return None\n",
        "\n",
        "        curr_synset, depth = queue.popleft()\n",
        "\n",
        "        if depth >= (max_depth + 1) // 2:\n",
        "            return None\n",
        "\n",
        "        path_to_current = visited_from_this_side[curr_synset.name()]\n",
        "\n",
        "        for neighbor in get_all_neighbors(curr_synset):\n",
        "            neighbor_name = neighbor.name()\n",
        "\n",
        "            if neighbor_name in visited_from_this_side:\n",
        "                continue\n",
        "\n",
        "            if is_forward:\n",
        "                new_path = path_to_current + [neighbor]\n",
        "            else:\n",
        "                new_path = [neighbor] + path_to_current\n",
        "\n",
        "            if neighbor_name in visited_from_other_side:\n",
        "                other_path = visited_from_other_side[neighbor_name]\n",
        "\n",
        "                if is_forward:\n",
        "                    full_path = path_to_current + other_path\n",
        "                else:\n",
        "                    full_path = other_path + path_to_current\n",
        "\n",
        "                return full_path\n",
        "\n",
        "            visited_from_this_side[neighbor_name] = new_path\n",
        "            queue.append((neighbor, depth + 1))\n",
        "\n",
        "        return None\n",
        "\n",
        "    # Alternate between forward and backward search\n",
        "    while forward_queue or backward_queue:\n",
        "        if forward_queue:\n",
        "            result = expand_frontier(forward_queue, forward_visited, backward_visited, True)\n",
        "            if result:\n",
        "                return result\n",
        "\n",
        "        if backward_queue:\n",
        "            result = expand_frontier(backward_queue, backward_visited, forward_visited, False)\n",
        "            if result:\n",
        "                return result\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_all_neighbors(synset):\n",
        "    \"\"\"Get all neighbors of a synset based on its POS.\"\"\"\n",
        "    neighbors = []\n",
        "\n",
        "    # Add hypernyms and hyponyms\n",
        "    neighbors.extend(synset.hypernyms())\n",
        "    neighbors.extend(synset.hyponyms())\n",
        "\n",
        "    # Add POS-specific neighbors\n",
        "    if synset.pos() == 'n':\n",
        "        neighbors.extend(get_noun_neighbors(synset))\n",
        "    else:\n",
        "        neighbors.extend(get_verb_neighbors(synset))\n",
        "\n",
        "    return neighbors\n",
        "\n",
        "\n",
        "def get_noun_neighbors(syn):\n",
        "    \"\"\"Get neighbors for a noun synset.\"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.part_meronyms())\n",
        "    nbrs.update(syn.substance_meronyms())\n",
        "    nbrs.update(syn.member_meronyms())\n",
        "    nbrs.update(syn.part_holonyms())\n",
        "    nbrs.update(syn.substance_holonyms())\n",
        "    nbrs.update(syn.member_holonyms())\n",
        "    return list(nbrs)\n",
        "\n",
        "\n",
        "def get_verb_neighbors(syn):\n",
        "    \"\"\"Get neighbors for a verb synset.\"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.entailments())\n",
        "    nbrs.update(syn.causes())\n",
        "    nbrs.update(syn.also_sees())\n",
        "    nbrs.update(syn.verb_groups())\n",
        "    return list(nbrs)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Gloss Analysis Helper Functions\n",
        "# ============================================================================\n",
        "\n",
        "def extract_subjects_from_gloss(gloss_doc):\n",
        "    \"\"\"Extract subject tokens from a parsed gloss.\"\"\"\n",
        "    subjects = []\n",
        "\n",
        "    # Direct subjects\n",
        "    subjects.extend([tok for tok in gloss_doc if tok.dep_ == \"nsubj\"])\n",
        "\n",
        "    # Passive subjects (which are actually objects semantically)\n",
        "    # Skip these for actor identification\n",
        "    passive_subjects = [tok for tok in gloss_doc if tok.dep_ == \"nsubjpass\"]\n",
        "\n",
        "    # Filter out passive subjects from the main list\n",
        "    subjects = [s for s in subjects if s not in passive_subjects]\n",
        "\n",
        "    return subjects, passive_subjects\n",
        "\n",
        "\n",
        "def extract_objects_from_gloss(gloss_doc):\n",
        "    \"\"\"Extract various types of object tokens from a parsed gloss.\"\"\"\n",
        "    objs = []\n",
        "\n",
        "    # Direct objects\n",
        "    objs.extend([tok for tok in gloss_doc if tok.dep_ == \"dobj\"])\n",
        "    # Prepositional objects\n",
        "    objs.extend([tok for tok in gloss_doc if tok.dep_ == \"pobj\"])\n",
        "    # Indirect objects\n",
        "    objs.extend([tok for tok in gloss_doc if tok.dep_ == \"iobj\"])\n",
        "    # General objects\n",
        "    objs.extend([tok for tok in gloss_doc if tok.dep_ == \"obj\"])\n",
        "\n",
        "    # Check for noun chunks related to root verb\n",
        "    root_verbs = [tok for tok in gloss_doc if tok.dep_ == \"ROOT\" and tok.pos_ == \"VERB\"]\n",
        "    if root_verbs and not objs:\n",
        "        for noun_chunk in gloss_doc.noun_chunks:\n",
        "            if any(token.head == root_verbs[0] for token in noun_chunk):\n",
        "                objs.append(noun_chunk.root)\n",
        "\n",
        "    return objs\n",
        "\n",
        "\n",
        "def extract_verbs_from_gloss(gloss_doc, include_passive=False):\n",
        "    \"\"\"Extract verb tokens from a parsed gloss.\"\"\"\n",
        "    verbs = [tok for tok in gloss_doc if tok.pos_ == \"VERB\"]\n",
        "\n",
        "    if include_passive:\n",
        "        # Past participles used as adjectives or in relative clauses\n",
        "        passive_verbs = [tok for tok in gloss_doc if\n",
        "                        tok.tag_ in [\"VBN\", \"VBD\"] and\n",
        "                        tok.dep_ in [\"acl\", \"relcl\", \"amod\"]]\n",
        "        verbs.extend(passive_verbs)\n",
        "\n",
        "    return verbs\n",
        "\n",
        "\n",
        "def find_instrumental_verbs(gloss_doc):\n",
        "    \"\"\"Find verbs associated with instrumental use (e.g., 'used for').\"\"\"\n",
        "    instrumental_verbs = []\n",
        "\n",
        "    if \"used\" in gloss_doc.text.lower():\n",
        "        for i, token in enumerate(gloss_doc):\n",
        "            if token.text.lower() == \"used\":\n",
        "                # Check tokens after \"used\"\n",
        "                for j in range(i+1, min(i+4, len(gloss_doc))):\n",
        "                    if gloss_doc[j].pos_ == \"VERB\":\n",
        "                        instrumental_verbs.append(gloss_doc[j])\n",
        "\n",
        "    return instrumental_verbs\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Cross-POS Path Finding Functions\n",
        "# ============================================================================\n",
        "\n",
        "def find_subject_to_predicate_path(subject_synset, predicate_synset, max_depth=6):\n",
        "    \"\"\"Find path from subject (noun) to predicate (verb).\"\"\"\n",
        "    paths = []\n",
        "\n",
        "    # Strategy 1: Look for active subjects in verb's gloss\n",
        "    pred_gloss_doc = nlp(predicate_synset.definition())\n",
        "    active_subjects, passive_subjects = extract_subjects_from_gloss(pred_gloss_doc)\n",
        "\n",
        "    # Try active subjects (true actors)\n",
        "    for subj in active_subjects[:3]:\n",
        "        try:\n",
        "            subject_synset_from_gloss = lesk(pred_gloss_doc.text, subj.text, pos='n')\n",
        "            if subject_synset_from_gloss:\n",
        "                path = path_syn_to_syn(subject_synset, subject_synset_from_gloss, max_depth)\n",
        "                if path:\n",
        "                    paths.append(path + [predicate_synset])\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Strategy 2: Look for verbs in the noun's gloss\n",
        "    subj_gloss_doc = nlp(subject_synset.definition())\n",
        "    verbs = extract_verbs_from_gloss(subj_gloss_doc, include_passive=False)\n",
        "\n",
        "    for verb in verbs[:3]:\n",
        "        try:\n",
        "            verb_synset_from_gloss = lesk(subj_gloss_doc.text, verb.text, pos='v')\n",
        "            if verb_synset_from_gloss:\n",
        "                path = path_syn_to_syn(verb_synset_from_gloss, predicate_synset, max_depth)\n",
        "                if path:\n",
        "                    paths.append([subject_synset] + path)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Return shortest path if any found\n",
        "    return min(paths, key=len) if paths else None\n",
        "\n",
        "\n",
        "def find_predicate_to_object_path(predicate_synset, object_synset, max_depth=6):\n",
        "    \"\"\"Find path from predicate (verb) to object (noun).\"\"\"\n",
        "    paths = []\n",
        "\n",
        "    # Strategy 1: Look for objects in verb's gloss\n",
        "    pred_gloss_doc = nlp(predicate_synset.definition())\n",
        "    objects = extract_objects_from_gloss(pred_gloss_doc)\n",
        "\n",
        "    # Also check for passive subjects (which are semantic objects)\n",
        "    _, passive_subjects = extract_subjects_from_gloss(pred_gloss_doc)\n",
        "    objects.extend(passive_subjects)\n",
        "\n",
        "    for obj in objects[:3]:\n",
        "        try:\n",
        "            object_synset_from_gloss = lesk(pred_gloss_doc.text, obj.text, pos='n')\n",
        "            if object_synset_from_gloss:\n",
        "                path = path_syn_to_syn(object_synset_from_gloss, object_synset, max_depth)\n",
        "                if path:\n",
        "                    paths.append([predicate_synset] + path)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Strategy 2: Look for verbs in object's gloss\n",
        "    obj_gloss_doc = nlp(object_synset.definition())\n",
        "    verbs = extract_verbs_from_gloss(obj_gloss_doc, include_passive=True)\n",
        "\n",
        "    for verb in verbs[:3]:\n",
        "        try:\n",
        "            verb_synset_from_gloss = lesk(obj_gloss_doc.text, verb.text, pos='v')\n",
        "            if verb_synset_from_gloss:\n",
        "                path = path_syn_to_syn(predicate_synset, verb_synset_from_gloss, max_depth)\n",
        "                if path:\n",
        "                    paths.append(path + [object_synset])\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Strategy 3: Check for instrumental relationships\n",
        "    instrumental_verbs = find_instrumental_verbs(obj_gloss_doc)\n",
        "    for verb in instrumental_verbs[:2]:\n",
        "        try:\n",
        "            verb_synset = lesk(obj_gloss_doc.text, verb.text, pos='v')\n",
        "            if verb_synset:\n",
        "                path = path_syn_to_syn(predicate_synset, verb_synset, max_depth)\n",
        "                if path:\n",
        "                    paths.append(path + [object_synset])\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Return shortest path if any found\n",
        "    return min(paths, key=len) if paths else None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Main Connected Path Finding Function\n",
        "# ============================================================================\n",
        "\n",
        "def find_connected_shortest_paths(subject_word, predicate_word, object_word, max_depth=10):\n",
        "    \"\"\"\n",
        "    Find shortest connected paths from subject through predicate to object.\n",
        "    Ensures that the same predicate synset connects both paths.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get synsets for each word\n",
        "    subject_synsets = wn.synsets(subject_word, pos=wn.NOUN)\n",
        "    predicate_synsets = wn.synsets(predicate_word, pos=wn.VERB)\n",
        "    object_synsets = wn.synsets(object_word, pos=wn.NOUN)\n",
        "\n",
        "    best_combined_path_length = float('inf')\n",
        "    best_subject_path = None\n",
        "    best_object_path = None\n",
        "    best_predicate = None\n",
        "\n",
        "    # Try each predicate synset as the connector\n",
        "    for pred in predicate_synsets:\n",
        "        # Find paths from all subjects to this specific predicate\n",
        "        subject_paths = []\n",
        "        for subj in subject_synsets:\n",
        "            path = find_subject_to_predicate_path(subj, pred, max_depth)\n",
        "            if path:\n",
        "                subject_paths.append(path)\n",
        "\n",
        "        # Find paths from this specific predicate to all objects\n",
        "        object_paths = []\n",
        "        for obj in object_synsets:\n",
        "            path = find_predicate_to_object_path(pred, obj, max_depth)\n",
        "            if path:\n",
        "                object_paths.append(path)\n",
        "\n",
        "        # If we have both paths through this predicate, check if it's the best\n",
        "        if subject_paths and object_paths:\n",
        "            shortest_subj_path = min(subject_paths, key=len)\n",
        "            shortest_obj_path = min(object_paths, key=len)\n",
        "\n",
        "            # Calculate combined length (subtract 1 to avoid counting predicate twice)\n",
        "            combined_length = len(shortest_subj_path) + len(shortest_obj_path) - 1\n",
        "\n",
        "            if combined_length < best_combined_path_length:\n",
        "                best_combined_path_length = combined_length\n",
        "                best_subject_path = shortest_subj_path\n",
        "                best_object_path = shortest_obj_path\n",
        "                best_predicate = pred\n",
        "\n",
        "    return best_subject_path, best_object_path, best_predicate\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Display Functions\n",
        "# ============================================================================\n",
        "\n",
        "def show_path(label, path):\n",
        "    \"\"\"Pretty print a path of synsets.\"\"\"\n",
        "    if path:\n",
        "        print(f\"{label}:\")\n",
        "        print(\" -> \".join(f\"{s.name()} ({s.definition()})\" for s in path))\n",
        "        print(f\"Path length: {len(path)}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"{label}: No path found\")\n",
        "        print()\n",
        "\n",
        "\n",
        "def show_connected_paths(subject_path, object_path, predicate):\n",
        "    \"\"\"Display the connected paths with their shared predicate.\"\"\"\n",
        "    if subject_path and object_path and predicate:\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"CONNECTED PATH through predicate: {predicate.name()}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        show_path(\"Subject -> Predicate path\", subject_path)\n",
        "        show_path(\"Predicate -> Object path\", object_path)\n",
        "\n",
        "        # Show the complete connected path\n",
        "        complete_path = subject_path + object_path[1:]  # Avoid duplicating the predicate\n",
        "        print(\"Complete connected path:\")\n",
        "        print(\" -> \".join(f\"{s.name()}\" for s in complete_path))\n",
        "        print(f\"Total path length: {len(complete_path)}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(\"No connected path found through any predicate synset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Find shortest connected paths\n",
        "subject_path, object_path, connecting_predicate = find_connected_shortest_paths(\n",
        "    \"cat\", \"eat\", \"mouse\", max_depth=10\n",
        ")\n",
        "\n",
        "# Display results\n",
        "show_connected_paths(subject_path, object_path, connecting_predicate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "valgVwQUh0SJ",
        "outputId": "11aa0305-95d1-43fc-b3a5-6c9d7e8fdd4a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CONNECTED PATH through predicate: feed.v.06\n",
            "======================================================================\n",
            "Subject -> Predicate path:\n",
            "kat.n.01 (the leaves of the shrub Catha edulis which are chewed like tobacco or used to make tea; has the effect of a euphoric stimulant) -> chew.v.01 (chew (food); to bite and grind with the teeth) -> eat.v.01 (take in solid food) -> feed.v.06 (take in food; used of animals only)\n",
            "Path length: 4\n",
            "\n",
            "Predicate -> Object path:\n",
            "feed.v.06 (take in food; used of animals only) -> animal.n.01 (a living organism characterized by voluntary movement) -> organism.n.01 (a living thing that has (or can develop) the ability to act or function independently) -> person.n.01 (a human being) -> mouse.n.03 (person who is quiet or timid)\n",
            "Path length: 5\n",
            "\n",
            "Complete connected path:\n",
            "kat.n.01 -> chew.v.01 -> eat.v.01 -> feed.v.06 -> animal.n.01 -> organism.n.01 -> person.n.01 -> mouse.n.03\n",
            "Total path length: 8\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJjxiYEy9smM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
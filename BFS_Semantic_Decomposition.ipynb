{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2J9B-9RI5lRP",
        "H6Ahax9MciBc",
        "7cmuVbqxY6ot"
      ],
      "authorship_tag": "ABX9TyMmdkeD82WZITFoMQfbVM0j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/SMIED/blob/main/BFS_Semantic_Decomposition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "2J9B-9RI5lRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "GReqNuQt7wvb",
        "outputId": "ba87a752-fd3b-4499-816b-f6230809e798"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "5b5a9116ee884cbfb11c9a5d28295c1c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import, config stuff"
      ],
      "metadata": {
        "id": "dSlaJDUEPnfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Semantic decomposition of (\"cat\", \"eats\", \"mouse\") using WordNet + spaCy + depth-limited GBFS.\n",
        "- Uses spaCy to parse verb synset glosses and detect subject/object dependencies.\n",
        "- If both subject and object tokens are present, branches directly toward original triple synsets.\n",
        "- Otherwise falls back to WordNet relations.\n",
        "\"\"\"\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from heapq import heappush, heappop\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gensim.downloader as api\n",
        "from collections import deque\n",
        "import heapq\n",
        "import itertools\n",
        "from typing import Dict, List, Optional, Set, Tuple, Callable\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "Jb_gtRCvh-DF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "# Load spaCy English model for dependency parsing\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj4iZlAfh_t8",
        "outputId": "7d54f684-d807-4307-8bf9-8958cc3fe43b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5TGC7I88EvX",
        "outputId": "44cd4ac0-10b3-44d5-ef79-7f8f5767338a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpers"
      ],
      "metadata": {
        "id": "H6Ahax9MciBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wn_to_nx():\n",
        "    # Initialize directed graph\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    synset_rels = {\n",
        "        # holonyms\n",
        "        \"part_holonyms\": lambda x: x.part_holonyms(),\n",
        "        \"substance_holonyms\": lambda x: x.substance_holonyms(),\n",
        "        \"member_holonyms\": lambda x: x.member_holonyms(),\n",
        "\n",
        "        # meronyms\n",
        "        \"part_meronyms\": lambda x: x.part_meronyms(),\n",
        "        \"substance_meronyms\": lambda x: x.substance_meronyms(),\n",
        "        \"member_meronyms\": lambda x: x.member_meronyms(),\n",
        "\n",
        "        # other\n",
        "        \"hypernyms\": lambda x: x.hypernyms(),\n",
        "        \"hyponyms\": lambda x: x.hyponyms(),\n",
        "        \"entailments\": lambda x: x.entailments(),\n",
        "        \"causes\": lambda x: x.causes(),\n",
        "        \"also_sees\": lambda x: x.also_sees(),\n",
        "        \"verb_groups\": lambda x: x.verb_groups(),\n",
        "    }\n",
        "\n",
        "    # add nodes (synsets) and all their edges (lexical relations) to the nx graph\n",
        "    for synset in wn.all_synsets():\n",
        "        for rel_name, rel_func in synset_rels.items():\n",
        "            for target in rel_func(synset):\n",
        "                G.add_edge(\n",
        "                    synset.name(),\n",
        "                    target.name(),\n",
        "                    relation = rel_name[:-1]\n",
        "                )\n",
        "    return G"
      ],
      "metadata": {
        "id": "ZkUj-vdOivAP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_neighbors(synset: wn.synset) -> List[wn.synset]:\n",
        "    \"\"\"Get all neighbors of a synset based on its POS.\"\"\"\n",
        "    neighbors = []\n",
        "\n",
        "    # Add hypernyms and hyponyms\n",
        "    neighbors.extend(synset.hypernyms())\n",
        "    neighbors.extend(synset.hyponyms())\n",
        "\n",
        "    # Add POS-specific neighbors\n",
        "    if synset.pos() == 'n':\n",
        "        neighbors.extend(get_noun_neighbors(synset))\n",
        "    else:\n",
        "        neighbors.extend(get_verb_neighbors(synset))\n",
        "\n",
        "    return neighbors\n",
        "\n",
        "\n",
        "def get_noun_neighbors(syn: wn.synset):\n",
        "    \"\"\"Get neighbors for a noun synset.\"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.part_meronyms())\n",
        "    nbrs.update(syn.substance_meronyms())\n",
        "    nbrs.update(syn.member_meronyms())\n",
        "    nbrs.update(syn.part_holonyms())\n",
        "    nbrs.update(syn.substance_holonyms())\n",
        "    nbrs.update(syn.member_holonyms())\n",
        "    return list(nbrs)\n",
        "\n",
        "\n",
        "def get_verb_neighbors(syn: wn.synset):\n",
        "    \"\"\"Get neighbors for a verb synset.\"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.entailments())\n",
        "    nbrs.update(syn.causes())\n",
        "    nbrs.update(syn.also_sees())\n",
        "    nbrs.update(syn.verb_groups())\n",
        "    return list(nbrs)"
      ],
      "metadata": {
        "id": "xoGoYrlVcjiG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Helpers"
      ],
      "metadata": {
        "id": "oL_kkfMUY1bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synset_embedding_centroid(synset:wn.synset, model=embedding_model):\n",
        "    \"\"\"\n",
        "    Get the centroid (mean) of token embeddings for all lemmas in a synset.\n",
        "\n",
        "    Args:\n",
        "        synset: WordNet Synset object (e.g., 'dog.n.01')\n",
        "        model: Loaded token model\n",
        "\n",
        "    Returns:\n",
        "        numpy array representing the centroid, or an empty numpy array if no lemmas found\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get all lemma names from the synset\n",
        "        lemmas = [lemma.name().lower().replace('_', ' ') for lemma in synset.lemmas()]\n",
        "        # Collect embeddings for lemmas that exist in the model\n",
        "        embeddings = []\n",
        "        found_lemmas = []\n",
        "\n",
        "        for lemma in lemmas:\n",
        "            # Try the lemma as-is first\n",
        "            if lemma in model:\n",
        "                embeddings.append(model[lemma])\n",
        "                found_lemmas.append(lemma)\n",
        "            # Try with underscores replaced by spaces (for multi-word terms)\n",
        "            elif lemma.replace(' ', '_') in model:\n",
        "                embeddings.append(model[lemma.replace(' ', '_')])\n",
        "                found_lemmas.append(lemma)\n",
        "            # Try individual words if it's a multi-word term\n",
        "            elif ' ' in lemma:\n",
        "                words = lemma.split()\n",
        "                word_embeddings = []\n",
        "                for word in words:\n",
        "                    if word in model:\n",
        "                        word_embeddings.append(model[word])\n",
        "                if word_embeddings:\n",
        "                    # Average the embeddings of individual words\n",
        "                    embeddings.append(np.mean(word_embeddings, axis=0))\n",
        "                    found_lemmas.append(lemma)\n",
        "\n",
        "        if not embeddings:\n",
        "            # print(f\"Warning: No lemmas from {synset.name()} found in token model\")\n",
        "            # print(f\"  Attempted lemmas: {lemmas}\")\n",
        "            return np.array([])\n",
        "\n",
        "        # print(f\"Synset {synset.name()}: Found {len(found_lemmas)}/{len(lemmas)} lemmas in model\")\n",
        "        # print(f\"  Found: {found_lemmas}\")\n",
        "\n",
        "        # Return the mean of all embeddings\n",
        "        return np.mean(embeddings, axis=0)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing synset {synset.name()}: {e}\")\n",
        "        return np.array([]) # Return empty array on error\n",
        "\n",
        "\n",
        "def embed_lexical_relations(synset: wn.synset, model) -> Dict[str, List[Tuple[str, np.ndarray]]]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        synset: WordNet Synset object (e.g., 'dog.n.01')\n",
        "\n",
        "    Returns:\n",
        "        Dict of embeddings for lexical relations of the synset.\n",
        "    \"\"\"\n",
        "    def _rel_centroids(get_attr) -> List[Tuple[str, np.ndarray]]:\n",
        "      try:\n",
        "        centroids = [\n",
        "            (s.name(), get_synset_embedding_centroid(s))\n",
        "            for s in get_attr(synset)\n",
        "        ]\n",
        "        # Filter out empty arrays and return the mean\n",
        "        return [c for c in centroids if c[1].size > 0]\n",
        "      except Exception as e:\n",
        "        print(f\"Error processing relation for synset {synset.name()}: {e}\")\n",
        "        return np.array([])\n",
        "\n",
        "    return {\n",
        "        # holonyms\n",
        "        \"part_holonyms\": _rel_centroids(lambda x: x.part_holonyms()),\n",
        "        \"substance_holonyms\": _rel_centroids(lambda x: x.substance_holonyms()),\n",
        "        \"member_holonyms\": _rel_centroids(lambda x: x.member_holonyms()),\n",
        "\n",
        "        # meronyms\n",
        "        \"part_meronyms\": _rel_centroids(lambda x: x.part_meronyms()),\n",
        "        \"substance_meronyms\": _rel_centroids(lambda x: x.substance_meronyms()),\n",
        "        \"member_meronyms\": _rel_centroids(lambda x: x.member_meronyms()),\n",
        "\n",
        "        # other\n",
        "        \"hypernyms\": _rel_centroids(lambda x: x.hypernyms()),\n",
        "        \"hyponyms\": _rel_centroids(lambda x: x.hyponyms()),\n",
        "        \"entailments\": _rel_centroids(lambda x: x.entailments()),\n",
        "        \"causes\": _rel_centroids(lambda x: x.causes()),\n",
        "        \"also_sees\": _rel_centroids(lambda x: x.also_sees()),\n",
        "        \"verb_groups\": _rel_centroids(lambda x: x.verb_groups()),\n",
        "    }\n",
        "\n",
        "\n",
        "def get_embedding_similarities(\n",
        "      rel_embs_1: List[Tuple[str, np.ndarray]],\n",
        "      rel_embs_2: List[Tuple[str, np.ndarray]],\n",
        "    ) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    get similarities of all possible pairings\n",
        "      between elements of the asymmetric lexical categories\n",
        "    \"\"\"\n",
        "    # convert the lists of centroid embeddings into rank 2 tensors\n",
        "    # e1_rel_embs: shape (m, d)\n",
        "    embs_1 = np.array([e[1] for e in rel_embs_1])\n",
        "    # e2_rel_embs: shape (n, d)\n",
        "    embs_2 = np.array([e[1] for e in rel_embs_2])\n",
        "\n",
        "    # Normalize each embedding vector to unit length\n",
        "    e1_norm = embs_1 / np.linalg.norm(embs_1, axis=1, keepdims=True)\n",
        "    e2_norm = embs_2 / np.linalg.norm(embs_2, axis=1, keepdims=True)\n",
        "\n",
        "    # Compute cosine similarities via dot product\n",
        "    cosine_sims = np.dot(e1_norm, e2_norm.T)  # shape (m, n)\n",
        "    return cosine_sims\n",
        "\n",
        "\n",
        "def get_top_k_aligned_lex_rel_pairs(\n",
        "      src_tgt_rel_map:Dict[str, List[Tuple[str, np.ndarray]]],\n",
        "      src_emb_dict:Dict[str, List[Tuple[str, np.ndarray]]],\n",
        "      tgt_emb_dict:Dict[str, List[Tuple[str, np.ndarray]]],\n",
        "      model=embedding_model,\n",
        "      beam_width:int=-1\n",
        "    ) -> List[Tuple[\n",
        "        Tuple[str, str],\n",
        "        Tuple[str, str],\n",
        "        float\n",
        "    ]]:\n",
        "    \"\"\"\n",
        "    Get the k most similar symmetric/asymmetric lexical relationship pairs\n",
        "      based on the relations' associated synset embeddings.\n",
        "\n",
        "    Returns a list of tuples of the form:\n",
        "      (\n",
        "        (synset1, lexical_rel),\n",
        "        (synset2, lexical_rel),\n",
        "        relatedness\n",
        "      )\n",
        "    \"\"\"\n",
        "\n",
        "    # check similarity of relations\n",
        "    #   i.e. similarity of synset1's meronyms to synset2's meronyms\n",
        "    rel_sims = list()\n",
        "    for e1_rel, e2_rel in src_tgt_rel_map.items():\n",
        "        # get embedding lists for e1_rel and e2_rel in the associated embedding dicts\n",
        "        e1_rel_syn_embs = src_emb_dict.get(e1_rel)\n",
        "        e2_rel_syn_embs = tgt_emb_dict.get(e2_rel)\n",
        "\n",
        "        # if there are >0 embeddings in each list\n",
        "        if len(e1_rel_syn_embs) > 0 and len(e2_rel_syn_embs) > 0:\n",
        "            # get similarities of all possible pairings\n",
        "            sims = get_embedding_similarities(e1_rel_syn_embs, e2_rel_syn_embs)\n",
        "            # add tuples relating the base synset to the neighbors\n",
        "            #   and their similarities to the asymm_rel_sims list\n",
        "            for i, j in np.ndindex(sims.shape):\n",
        "                try:\n",
        "                  rel_sims.append(\n",
        "                      (\n",
        "                          (e1_rel_syn_embs[i][0], e1_rel),\n",
        "                          (e2_rel_syn_embs[j][0], e2_rel),\n",
        "                          sims[i, j]\n",
        "                      )\n",
        "                  )\n",
        "                except IndexError:\n",
        "                  raise IndexError(f\"IndexError: i={i}, j={j} \\nShape of sims: {sims.shape} \\nLength of e1_rel_syn_embs: {len(e1_rel_syn_embs)} \\nLength of e2_rel_syn_embs: {len(e2_rel_syn_embs)}\")\n",
        "\n",
        "    # it doesn't matter which one is contains/includes/etc. the other,\n",
        "    #   as long as they're closer than antonyms or unrelated terms\n",
        "    # i.e. a good hyponym-hypernym pair is just as important\n",
        "    #   as a good hypernym-hyponym pair\n",
        "    beam = sorted(\n",
        "        rel_sims,\n",
        "        key=lambda x: x[2],\n",
        "        reverse=True\n",
        "    )[:beam_width]\n",
        "    return beam"
      ],
      "metadata": {
        "id": "th4qyXLmYH4j"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam Construction"
      ],
      "metadata": {
        "id": "7cmuVbqxY6ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asymmetric_pairs_map = {\n",
        "    # holonyms\n",
        "    \"part_holonyms\": \"part_meronyms\",\n",
        "    \"substance_holonyms\": \"substance_meronyms\",\n",
        "    \"member_holonyms\": \"member_meronyms\",\n",
        "\n",
        "    # meronyms\n",
        "    \"part_meronyms\": \"part_holonyms\",\n",
        "    \"substance_meronyms\": \"substance_holonyms\",\n",
        "    \"member_meronyms\": \"member_holonyms\",\n",
        "\n",
        "    # other\n",
        "    \"hypernyms\": \"hyponyms\",\n",
        "    \"hyponyms\": \"hyponyms\"\n",
        "}\n",
        "\n",
        "\n",
        "symmetric_pairs_map = {\n",
        "    # holonyms\n",
        "    \"part_holonyms\": \"part_holonyms\",\n",
        "    \"substance_holonyms\": \"substance_holonyms\",\n",
        "    \"member_holonyms\": \"member_holonyms\",\n",
        "\n",
        "    # meronyms\n",
        "    \"part_meronyms\": \"part_meronyms\",\n",
        "    \"substance_meronyms\": \"substance_meronyms\",\n",
        "    \"member_meronyms\": \"member_meronyms\",\n",
        "\n",
        "    # other\n",
        "    \"hypernyms\": \"hypernyms\",\n",
        "    \"hyponyms\": \"hyponyms\",\n",
        "    \"entailments\": \"entailments\",\n",
        "    \"causes\": \"causes\",\n",
        "    \"also_sees\": \"also_sees\",\n",
        "    \"verb_groups\": \"verb_groups\"\n",
        "}"
      ],
      "metadata": {
        "id": "25u1o6wq5req"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_new_beams(\n",
        "      g: nx.DiGraph,\n",
        "      src: str,\n",
        "      tgt: str,\n",
        "      model=embedding_model,\n",
        "      beam_width=3\n",
        "    ) -> List[Tuple[\n",
        "        Tuple[str, str],\n",
        "        Tuple[str, str],\n",
        "        float\n",
        "    ]]:\n",
        "    \"\"\"\n",
        "    Get the k closest pairs of lexical relations between 2 synsets.\n",
        "\n",
        "    Args:\n",
        "        src: WordNet Synset object (e.g., 'dog.n.01')\n",
        "        tgt: WordNet Synset object (e.g., 'cat.n.01')\n",
        "        model: token model (if None, will load default)\n",
        "        beam_width: max number of pairs to return\n",
        "\n",
        "    Returns:\n",
        "        List of tuples of the form:\n",
        "          (\n",
        "            (synset1, lexical_rel),\n",
        "            (synset2, lexical_rel),\n",
        "            relatedness\n",
        "          )\n",
        "    \"\"\"\n",
        "\n",
        "    # Build a map of each synset's associated lexical relations\n",
        "    #   and the centroids of their associated synsets\n",
        "    src_lex_rel_embs = embed_lexical_relations(wn.synset(src), model)\n",
        "    tgt_lex_rel_embs = embed_lexical_relations(wn.synset(tgt), model)\n",
        "\n",
        "    # ensure the edges in the nx graph align with those in the embedding maps\n",
        "    src_neighbors = {n for n in g.neighbors(src)}\n",
        "    for rel, synset_list in src_lex_rel_embs.items():\n",
        "      if not all(s[0] in src_neighbors for s in synset_list):\n",
        "        raise ValueError(f\"Not all lexical properties of {src} ({[s[0] for s in synset_list]}) in graph for relation {rel}\")\n",
        "    tgt_neighbors = {n for n in g.neighbors(tgt)}\n",
        "    for rel, synset_list in tgt_lex_rel_embs.items():\n",
        "      if not all(s[0] in tgt_neighbors for s in synset_list):\n",
        "        raise ValueError(f\"Not all lexical properties of {tgt} ({[s[0] for s in synset_list]}) in graph for relation {rel}\")\n",
        "    # in the future, get neighbor relation in node metadata with g.adj[n]\n",
        "\n",
        "    # Get the asymmetric lexical relation pairings,\n",
        "    #   sorted in descending order of embedding similarity\n",
        "    #   e.x. similarity of synset1's hypernyms to synset2's hypernyms\n",
        "    asymm_lex_rel_sims = get_top_k_aligned_lex_rel_pairs(\n",
        "        asymmetric_pairs_map,\n",
        "        src_lex_rel_embs,\n",
        "        tgt_lex_rel_embs,\n",
        "        model,\n",
        "        beam_width\n",
        "    )\n",
        "    # Get the symmetric lexical relation pairings,\n",
        "    #   sorted in descending order of embedding similarity\n",
        "    #   e.x. similarity of synset1's hypernyms to synset2's hypernyms\n",
        "    symm_lex_rel_sims = get_top_k_aligned_lex_rel_pairs(\n",
        "        symmetric_pairs_map,\n",
        "        src_lex_rel_embs,\n",
        "        tgt_lex_rel_embs,\n",
        "        model,\n",
        "        beam_width\n",
        "    )\n",
        "    combined = asymm_lex_rel_sims + symm_lex_rel_sims\n",
        "    beam = sorted(combined, key=lambda x: x[2], reverse=True)[:beam_width]\n",
        "    return beam"
      ],
      "metadata": {
        "id": "iTYjzwULZCJd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pathing"
      ],
      "metadata": {
        "id": "X1WdFnm55oTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type aliases\n",
        "BeamElement = Tuple[Tuple[str, str], Tuple[str, str], float]\n",
        "GetNewBeamsFn = Callable[[nx.DiGraph, str, str], List[BeamElement]]\n",
        "\n",
        "\n",
        "class BidirectionalAStar:\n",
        "    def __init__(\n",
        "        self,\n",
        "        g: nx.DiGraph,\n",
        "        src: str,\n",
        "        tgt: str,\n",
        "        get_new_beams_fn: GetNewBeamsFn = get_new_beams,\n",
        "        beam_width: int = 10,\n",
        "        relax_beam: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Beam-constrained bidirectional A* encapsulated in a class.\n",
        "\n",
        "        Args:\n",
        "            g: directed graph (nx.DiGraph)\n",
        "            src: source node id\n",
        "            tgt: target node id\n",
        "            get_new_beams_fn: function(g, src, tgt) -> List[((src_node, rel),(tgt_node, rel), sim)]\n",
        "            beam_width: passed through to get_new_beams if that function uses it internally\n",
        "            relax_beam: if True, allow exploring nodes outside the beam sets\n",
        "        \"\"\"\n",
        "        self.g = g\n",
        "        self.src = src\n",
        "        self.tgt = tgt\n",
        "        self.get_new_beams_fn = get_new_beams_fn\n",
        "        self.beam_width = beam_width\n",
        "        self.relax_beam = relax_beam\n",
        "\n",
        "        # Will be filled during setup/search\n",
        "        self.src_allowed: Set[str] = set()\n",
        "        self.tgt_allowed: Set[str] = set()\n",
        "        self.h_f: Dict[str, float] = {}\n",
        "        self.h_b: Dict[str, float] = {}\n",
        "\n",
        "        # Search state\n",
        "        self.g_f: Dict[str, float] = {}\n",
        "        self.g_b: Dict[str, float] = {}\n",
        "        self.parent_f: Dict[str, Optional[str]] = {}\n",
        "        self.parent_b: Dict[str, Optional[str]] = {}\n",
        "        self.open_f = []\n",
        "        self.open_b = []\n",
        "        self.closed_f: Set[str] = set()\n",
        "        self.closed_b: Set[str] = set()\n",
        "        self._counter = itertools.count()\n",
        "\n",
        "        # Results\n",
        "        self.meet_node: Optional[str] = None\n",
        "        self.path_cost: Optional[float] = None\n",
        "\n",
        "    # ----------------------\n",
        "    # Setup / heuristics\n",
        "    # ----------------------\n",
        "    def build_allowed_sets_and_heuristics(self):\n",
        "        \"\"\"\n",
        "        Calls get_new_beams_fn and builds:\n",
        "          - src_allowed, tgt_allowed (sets of node ids)\n",
        "          - h_f, h_b : heuristic maps (node -> float)\n",
        "        \"\"\"\n",
        "        beams = self.get_new_beams_fn(self.g, self.src, self.tgt)\n",
        "        # beams are expected to be [((src_node, rel),(tgt_node, rel), sim), ...]\n",
        "        src_pairs = [b[0] for b in beams]\n",
        "        tgt_pairs = [b[1] for b in beams]\n",
        "\n",
        "        self.src_allowed = {p[0] for p in src_pairs}\n",
        "        self.tgt_allowed = {p[0] for p in tgt_pairs}\n",
        "        # always allow src and tgt explicitly\n",
        "        self.src_allowed.add(self.src)\n",
        "        self.tgt_allowed.add(self.tgt)\n",
        "\n",
        "        self.h_f = {}\n",
        "        self.h_b = {}\n",
        "        for s_pair, t_pair, sim in beams:\n",
        "            s_node = s_pair[0]\n",
        "            t_node = t_pair[0]\n",
        "            h_val = max(0.0, 1.0 - float(sim))\n",
        "            if s_node not in self.h_f or h_val < self.h_f[s_node]:\n",
        "                self.h_f[s_node] = h_val\n",
        "            if t_node not in self.h_b or h_val < self.h_b[t_node]:\n",
        "                self.h_b[t_node] = h_val\n",
        "\n",
        "    # ----------------------\n",
        "    # Initialization\n",
        "    # ----------------------\n",
        "    def _init_search_state(self):\n",
        "        \"\"\"Initialize open queues, g-scores, parents, closed sets and counters.\"\"\"\n",
        "        self._counter = itertools.count()\n",
        "        self.open_f = []\n",
        "        self.open_b = []\n",
        "        self.g_f = {self.src: 0.0}\n",
        "        self.g_b = {self.tgt: 0.0}\n",
        "        self.parent_f = {self.src: None}\n",
        "        self.parent_b = {self.tgt: None}\n",
        "        self.closed_f = set()\n",
        "        self.closed_b = set()\n",
        "\n",
        "        heapq.heappush(self.open_f, (self.h_f.get(self.src, 0.0), next(self._counter), self.src))\n",
        "        heapq.heappush(self.open_b, (self.h_b.get(self.tgt, 0.0), next(self._counter), self.tgt))\n",
        "\n",
        "    # ----------------------\n",
        "    # Utilities\n",
        "    # ----------------------\n",
        "    def _edge_weight(self, u: str, v: str) -> float:\n",
        "        data = self.g.get_edge_data(u, v, default={})\n",
        "        try:\n",
        "            return float(data.get(\"weight\", 1.0))\n",
        "        except Exception:\n",
        "            return 1.0\n",
        "\n",
        "    def _allowed_forward(self, node: str) -> bool:\n",
        "        return self.relax_beam or node in self.src_allowed or node in self.tgt_allowed or node == self.tgt or node == self.src\n",
        "\n",
        "    def _allowed_backward(self, node: str) -> bool:\n",
        "        return self.relax_beam or node in self.tgt_allowed or node in self.src_allowed or node == self.src or node == self.tgt\n",
        "\n",
        "    # ----------------------\n",
        "    # Expansion methods\n",
        "    # ----------------------\n",
        "    def _expand_forward(self) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Pop one entry from open_f, expand its outgoing neighbors.\n",
        "        Return meeting node if found (node in closed_b), else None.\n",
        "        \"\"\"\n",
        "        while self.open_f:\n",
        "            _, _, current = heapq.heappop(self.open_f)\n",
        "            if current in self.closed_f:\n",
        "                continue\n",
        "            self.closed_f.add(current)\n",
        "\n",
        "            if current in self.closed_b:\n",
        "                return current\n",
        "\n",
        "            for nbr in self.g.neighbors(current):\n",
        "                if not self._allowed_forward(nbr):\n",
        "                    continue\n",
        "                tentative = self.g_f[current] + self._edge_weight(current, nbr)\n",
        "                if tentative < self.g_f.get(nbr, float(\"inf\")):\n",
        "                    self.g_f[nbr] = tentative\n",
        "                    self.parent_f[nbr] = current\n",
        "                    f_score = tentative + self.h_f.get(nbr, 0.0)\n",
        "                    heapq.heappush(self.open_f, (f_score, next(self._counter), nbr))\n",
        "                    if nbr in self.closed_b:\n",
        "                        return nbr\n",
        "            # we expanded one node; break to let the other side move if needed\n",
        "            return None\n",
        "        return None\n",
        "\n",
        "    def _expand_backward(self) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Pop one entry from open_b, expand its predecessors (backward search).\n",
        "        Return meeting node if found, else None.\n",
        "        \"\"\"\n",
        "        while self.open_b:\n",
        "            _, _, current = heapq.heappop(self.open_b)\n",
        "            if current in self.closed_b:\n",
        "                continue\n",
        "            self.closed_b.add(current)\n",
        "\n",
        "            if current in self.closed_f:\n",
        "                return current\n",
        "\n",
        "            for nbr in self.g.predecessors(current):\n",
        "                if not self._allowed_backward(nbr):\n",
        "                    continue\n",
        "                tentative = self.g_b[current] + self._edge_weight(nbr, current)\n",
        "                if tentative < self.g_b.get(nbr, float(\"inf\")):\n",
        "                    self.g_b[nbr] = tentative\n",
        "                    self.parent_b[nbr] = current\n",
        "                    f_score = tentative + self.h_b.get(nbr, 0.0)\n",
        "                    heapq.heappush(self.open_b, (f_score, next(self._counter), nbr))\n",
        "                    if nbr in self.closed_f:\n",
        "                        return nbr\n",
        "            return None\n",
        "        return None\n",
        "\n",
        "    # ----------------------\n",
        "    # Path reconstruction\n",
        "    # ----------------------\n",
        "    def _reconstruct_path(self, meet: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Build path: src -> ... -> meet -> ... -> tgt using parent_f and parent_b.\n",
        "        parent_f maps node -> predecessor (toward src),\n",
        "        parent_b maps node -> successor (toward tgt).\n",
        "        \"\"\"\n",
        "        # forward: src ... meet\n",
        "        path_f: List[str] = []\n",
        "        n = meet\n",
        "        while n is not None:\n",
        "            path_f.append(n)\n",
        "            n = self.parent_f.get(n)\n",
        "        path_f.reverse()  # src -> ... -> meet\n",
        "\n",
        "        # backward: nodes after meet toward tgt\n",
        "        path_b: List[str] = []\n",
        "        n = self.parent_b.get(meet)\n",
        "        while n is not None:\n",
        "            path_b.append(n)\n",
        "            n = self.parent_b.get(n)\n",
        "\n",
        "        return path_f + path_b\n",
        "\n",
        "    # ----------------------\n",
        "    # Main search\n",
        "    # ----------------------\n",
        "    def search(self) -> Optional[List[str]]:\n",
        "        \"\"\"\n",
        "        Run the bidirectional beam-constrained A*. Returns path (list of node ids) or None.\n",
        "        Also sets .meet_node and .path_cost when a path is found.\n",
        "        \"\"\"\n",
        "        if self.src == self.tgt:\n",
        "            self.meet_node = self.src\n",
        "            self.path_cost = 0.0\n",
        "            return [self.src]\n",
        "\n",
        "        # prepare heuristics and allowed sets\n",
        "        self.build_allowed_sets_and_heuristics()\n",
        "        self._init_search_state()\n",
        "\n",
        "        while self.open_f and self.open_b:\n",
        "            top_f = self.open_f[0][0] if self.open_f else float(\"inf\")\n",
        "            top_b = self.open_b[0][0] if self.open_b else float(\"inf\")\n",
        "\n",
        "            if top_f <= top_b:\n",
        "                meet = self._expand_forward()\n",
        "            else:\n",
        "                meet = self._expand_backward()\n",
        "\n",
        "            if meet is not None:\n",
        "                self.meet_node = meet\n",
        "                # compute path and cost\n",
        "                path = self._reconstruct_path(meet)\n",
        "                cost_f = self.g_f.get(meet, float(\"inf\"))\n",
        "                cost_b = self.g_b.get(meet, float(\"inf\"))\n",
        "                # When meet is not exactly same node in both maps, attempt to compute more precise cost:\n",
        "                # if there exists a node in both g_f and g_b, we could choose minimum g_f[n] + g_b[n] across intersection.\n",
        "                # For now use the meeting node costs as reported.\n",
        "                self.path_cost = None if (cost_f == float(\"inf\") or cost_b == float(\"inf\")) else (cost_f + cost_b)\n",
        "                return path\n",
        "\n",
        "        # no path found\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "HpemJQhSZIJN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GZ7mH1XuhtBN"
      },
      "outputs": [],
      "source": [
        "# # ============================================================================\n",
        "# # Core Path Finding Functions\n",
        "# # ============================================================================\n",
        "\n",
        "# def path_syn_to_syn(start_synset, end_synset, max_depth=6):\n",
        "#     \"\"\"\n",
        "#     Find shortest path between synsets of the same POS using bidirectional BFS.\n",
        "#     Returns a list of synsets forming the path, or None if no path found.\n",
        "#     \"\"\"\n",
        "\n",
        "#     if not (start_synset.pos() == end_synset.pos() and start_synset.pos() in {'n', 'v'}):\n",
        "#       raise ValueError(f\"{start_synset.name()} POS tag != {end_synset.name()}. Synsets must be of the same POS (noun or verb).\")\n",
        "\n",
        "#     # Handle the trivial case where start and end are the same\n",
        "#     if start_synset.name() == end_synset.name():\n",
        "#         return [start_synset]\n",
        "\n",
        "#     # Initialize two search frontiers\n",
        "#     forward_queue = deque([(start_synset, 0)])\n",
        "#     forward_visited = {start_synset.name(): [start_synset]}\n",
        "\n",
        "#     backward_queue = deque([(end_synset, 0)])\n",
        "#     backward_visited = {end_synset.name(): [end_synset]}\n",
        "\n",
        "#     def expand_frontier(queue, visited_from_this_side, visited_from_other_side, is_forward):\n",
        "#         \"\"\"Expand one step of the search frontier.\"\"\"\n",
        "#         if not queue:\n",
        "#             return None\n",
        "\n",
        "#         curr_synset, depth = queue.popleft()\n",
        "\n",
        "#         if depth >= (max_depth + 1) // 2:\n",
        "#             return None\n",
        "\n",
        "#         path_to_current = visited_from_this_side[curr_synset.name()]\n",
        "\n",
        "#         for neighbor in get_all_neighbors(curr_synset):\n",
        "#             neighbor_name = neighbor.name()\n",
        "\n",
        "#             if neighbor_name in visited_from_this_side:\n",
        "#                 continue\n",
        "\n",
        "#             if is_forward:\n",
        "#                 new_path = path_to_current + [neighbor]\n",
        "#             else:\n",
        "#                 new_path = [neighbor] + path_to_current\n",
        "\n",
        "#             if neighbor_name in visited_from_other_side:\n",
        "#                 other_path = visited_from_other_side[neighbor_name]\n",
        "\n",
        "#                 if is_forward:\n",
        "#                     full_path = path_to_current + other_path\n",
        "#                 else:\n",
        "#                     full_path = other_path + path_to_current\n",
        "\n",
        "#                 return full_path\n",
        "\n",
        "#             visited_from_this_side[neighbor_name] = new_path\n",
        "#             queue.append((neighbor, depth + 1))\n",
        "\n",
        "#         return None\n",
        "\n",
        "#     # Alternate between forward and backward search\n",
        "#     while forward_queue or backward_queue:\n",
        "#         if forward_queue:\n",
        "#             result = expand_frontier(forward_queue, forward_visited, backward_visited, True)\n",
        "#             if result:\n",
        "#                 return result\n",
        "\n",
        "#         if backward_queue:\n",
        "#             result = expand_frontier(backward_queue, backward_visited, forward_visited, False)\n",
        "#             if result:\n",
        "#                 return result\n",
        "\n",
        "#     return None\n",
        "\n",
        "\n",
        "# # ============================================================================\n",
        "# # Gloss Analysis Helper Functions\n",
        "# # ============================================================================\n",
        "\n",
        "# def extract_subjects_from_gloss(gloss_doc):\n",
        "#     \"\"\"Extract subject tokens from a parsed gloss.\"\"\"\n",
        "#     subjects = []\n",
        "\n",
        "#     # Direct subjects\n",
        "#     subjects.extend([tok for tok in gloss_doc if tok.dep_ == \"nsubj\"])\n",
        "\n",
        "#     # Passive subjects (which are actually objects semantically)\n",
        "#     # Skip these for actor identification\n",
        "#     passive_subjects = [tok for tok in gloss_doc if tok.dep_ == \"nsubjpass\"]\n",
        "\n",
        "#     # Filter out passive subjects from the main list\n",
        "#     subjects = [s for s in subjects if s not in passive_subjects]\n",
        "\n",
        "#     return subjects, passive_subjects\n",
        "\n",
        "\n",
        "# def extract_objects_from_gloss(gloss_doc):\n",
        "#     \"\"\"Extract various types of object tokens from a parsed gloss.\"\"\"\n",
        "#     objs = []\n",
        "\n",
        "#     # Indirect objects\n",
        "#     iobjs = [tok for tok in gloss_doc if tok.dep_ == \"iobj\"]\n",
        "#     objs.extend(iobjs)\n",
        "\n",
        "#     # Direct objects\n",
        "#     # Only include if there were no indirect objects,\n",
        "#     #   crude, but good for MVP\n",
        "#     if not iobjs:\n",
        "#         objs.extend([tok for tok in gloss_doc if tok.dep_ == \"dobj\"])\n",
        "\n",
        "#     # Prepositional objects\n",
        "#     objs.extend([tok for tok in gloss_doc if tok.dep_ == \"pobj\"])\n",
        "\n",
        "#     # General objects\n",
        "#     objs.extend([tok for tok in gloss_doc if tok.dep_ == \"obj\"])\n",
        "\n",
        "#     # Check for noun chunks related to root verb\n",
        "#     root_verbs = [tok for tok in gloss_doc if tok.dep_ == \"ROOT\" and tok.pos_ == \"VERB\"]\n",
        "#     if root_verbs and not objs:\n",
        "#         for noun_chunk in gloss_doc.noun_chunks:\n",
        "#             if any(token.head == root_verbs[0] for token in noun_chunk):\n",
        "#                 objs.append(noun_chunk.root)\n",
        "\n",
        "#     return objs\n",
        "\n",
        "\n",
        "# def extract_verbs_from_gloss(gloss_doc, include_passive=False):\n",
        "#     \"\"\"Extract verb tokens from a parsed gloss.\"\"\"\n",
        "#     verbs = [tok for tok in gloss_doc if tok.pos_ == \"VERB\"]\n",
        "\n",
        "#     if include_passive:\n",
        "#         # Past participles used as adjectives or in relative clauses\n",
        "#         passive_verbs = [tok for tok in gloss_doc if\n",
        "#                         tok.tag_ in [\"VBN\", \"VBD\"] and\n",
        "#                         tok.dep_ in [\"acl\", \"relcl\", \"amod\"]]\n",
        "#         verbs.extend(passive_verbs)\n",
        "\n",
        "#     return verbs\n",
        "\n",
        "\n",
        "# def find_instrumental_verbs(gloss_doc):\n",
        "#     \"\"\"Find verbs associated with instrumental use (e.g., 'used for').\"\"\"\n",
        "#     instrumental_verbs = []\n",
        "\n",
        "#     if \"used\" in gloss_doc.text.lower():\n",
        "#         for i, token in enumerate(gloss_doc):\n",
        "#             if token.text.lower() == \"used\":\n",
        "#                 # Check tokens after \"used\"\n",
        "#                 for j in range(i+1, min(i+4, len(gloss_doc))):\n",
        "#                     if gloss_doc[j].pos_ == \"VERB\":\n",
        "#                         instrumental_verbs.append(gloss_doc[j])\n",
        "\n",
        "#     return instrumental_verbs\n",
        "\n",
        "\n",
        "# # ============================================================================\n",
        "# # Cross-POS Path Finding Functions\n",
        "# # ============================================================================\n",
        "# def get_top_k_synset_branch_pairs(\n",
        "#       candidates: List[List[wn.synset]],\n",
        "#       target_synset: wn.synset,\n",
        "#       beam_width=3\n",
        "#     ) -> List[Tuple[\n",
        "#         Tuple[wn.synset, str],\n",
        "#         Tuple[wn.synset, str],\n",
        "#         float\n",
        "#     ]]:\n",
        "#     \"\"\"\n",
        "#     Given a list of candidate tokens and a target synset,\n",
        "\n",
        "\n",
        "#     Return the k synset subrelation pairs most similar to the target of the form:\n",
        "#       ((synset, lexical_rel), (name, lexical_rel), relatedness)\n",
        "#     \"\"\"\n",
        "#     top_k_asymm_branches = list()\n",
        "#     top_k_symm_branches = list()\n",
        "#     beam = list()\n",
        "#     # for each list of possible synsets for a candidate token\n",
        "#     for synsets in candidates:\n",
        "#         # # filter to subjects based on whether they reside in the same sub-category\n",
        "#         # #   where the subcategory != 'entity.n.01' or a similar top-level\n",
        "#         # synsets = [\n",
        "#         #     s for s in synsets\n",
        "#         #     if s.root_hypernyms() != s.lowest_common_hypernyms(target_synset)\n",
        "#         # ]\n",
        "#         # # if there are synsets left for the candidate token after pruning\n",
        "#         if synsets:\n",
        "#             # # if the target is a verb,\n",
        "#             # #   filter out any synsets with no lemma frames matching the target\n",
        "#             # #   frame patterns: (Somebody [v] something), (Somebody [v]), ...\n",
        "#             # if target_synset.pos() == 'v':\n",
        "#             #     synsets = [\n",
        "#             #         s for s in synsets\n",
        "#             #         if any(\n",
        "#             #             frame in s.frame_ids()\n",
        "#             #             for frame in target_synset.frame_ids()\n",
        "#             #         )\n",
        "#             #     ]\n",
        "#             for synset in synsets:\n",
        "#               beam += get_synset_relatedness(synset, target_synset)\n",
        "#               beam += get_synset_relatedness(synset, target_synset)\n",
        "#     beam = sorted(\n",
        "#         beam,\n",
        "#         key=lambda x: x[2],\n",
        "#         reverse=True\n",
        "#     )[:beam_width]\n",
        "#     return beam\n",
        "\n",
        "\n",
        "# def find_subject_to_predicate_path(\n",
        "#       subject_synset: wn.synset,\n",
        "#       predicate_synset: wn.synset,\n",
        "#       max_depth:int,\n",
        "#       visited=set(),\n",
        "#       max_sample_size=5,\n",
        "#     ):\n",
        "#     \"\"\"Find path from subject (noun) to predicate (verb).\"\"\"\n",
        "#     if subject_synset.name() in visited or predicate_synset.name() in visited:\n",
        "#       return None\n",
        "\n",
        "#     paths = []\n",
        "#     print()\n",
        "#     print(f\"Finding path from {subject_synset.name()} to {predicate_synset.name()}\")\n",
        "\n",
        "#     # Strategy 1: Look for active subjects in verb's gloss\n",
        "#     pred_gloss_doc = nlp(predicate_synset.definition())\n",
        "#     # passive subjects are semantically equivalent to objects\n",
        "#     active_subjects, _ = extract_subjects_from_gloss(pred_gloss_doc)\n",
        "#     # convert spacy tokens to lists of synsets\n",
        "#     subjects = [wn.synsets(s.text, pos=subject_synset.pos()) for s in active_subjects]\n",
        "#     # of the remaining subjects, get the most similar\n",
        "#     top_k = get_top_k_synset_branches(active_subjects[:max_sample_size], subject_synset)\n",
        "#     if top_k:\n",
        "#       print(f\"Found best matches for {subject_synset.name()}: {top_k} using strategy 1\")\n",
        "#       for matched_synset, _ in top_k:\n",
        "#         path = path_syn_to_syn(subject_synset, matched_synset, max_depth-1)\n",
        "#         if path:\n",
        "#             paths.append(path + [predicate_synset])\n",
        "\n",
        "#     # Strategy 2: Look for verbs in the noun's gloss\n",
        "#     subj_gloss_doc = nlp(subject_synset.definition())\n",
        "#     verbs = extract_verbs_from_gloss(subj_gloss_doc, include_passive=False)\n",
        "#     # convert spacy tokens to lists of synsets\n",
        "#     verbs = [wn.synsets(v.text, pos=predicate_synset.pos()) for v in verbs]\n",
        "#     # of the remaining subjects, get the most similar\n",
        "#     top_k = get_top_k_synset_branches(verbs[:max_sample_size], predicate_synset)\n",
        "#     if top_k:\n",
        "#       print(f\"Found best matches for {predicate_synset.name()}: {top_k} using strategy 2\")\n",
        "#       for matched_synset, _ in top_k:\n",
        "#         path = path_syn_to_syn(matched_synset, predicate_synset, max_depth-1)\n",
        "#         if path:\n",
        "#             paths.append([subject_synset] + path)\n",
        "\n",
        "#     # Strategy 3: Explore the 3 most promising pairs of neighbors\n",
        "#     subject_neighbors = get_all_neighbors(subject_synset)\n",
        "#     predicate_neighbors = get_all_neighbors(predicate_synset)\n",
        "#     top_k = get_k_closest_synset_pairs(subject_neighbors, predicate_neighbors)\n",
        "#     if top_k:\n",
        "#       print(f\"Most promising pairs for bidirectional exploration: {top_k} using strategy 3\")\n",
        "#       for s, p, _ in top_k:\n",
        "#         visited.add(subject_synset.name())\n",
        "#         visited.add(predicate_synset.name())\n",
        "#         path = find_subject_to_predicate_path(s, p, max_depth-1, visited)\n",
        "#         if path:\n",
        "#             paths.append([subject_synset] + path + [predicate_synset])\n",
        "\n",
        "\n",
        "#     # Return shortest path if any found\n",
        "#     return min(paths, key=len) if paths else None\n",
        "\n",
        "\n",
        "# def find_predicate_to_object_path(\n",
        "#       predicate_synset: wn.synset,\n",
        "#       object_synset: wn.synset,\n",
        "#       max_depth:int,\n",
        "#       visited=set(),\n",
        "#       max_sample_size=5,\n",
        "#     ):\n",
        "#     \"\"\"Find path from predicate (verb) to object (noun).\"\"\"\n",
        "\n",
        "#     if predicate_synset.name() in visited or object_synset.name() in visited:\n",
        "#       return None\n",
        "\n",
        "#     paths = []\n",
        "#     print()\n",
        "#     print(f\"Finding path from {predicate_synset.name()} to {object_synset.name()}\")\n",
        "\n",
        "#     # === Strategy 1: Objects in predicate gloss (incl. passive subjects) ===\n",
        "#     pred_gloss_doc = nlp(predicate_synset.definition())\n",
        "#     objects = extract_objects_from_gloss(pred_gloss_doc)\n",
        "#     _, passive_subjects = extract_subjects_from_gloss(pred_gloss_doc)\n",
        "#     objects.extend(passive_subjects)\n",
        "#     # convert spacy tokens to lists of synsets\n",
        "#     objects = [wn.synsets(o.text, pos=object_synset.pos()) for o in objects]\n",
        "#     top_k = get_top_k_synset_branches(objects[:max_sample_size], object_synset)\n",
        "#     if top_k:\n",
        "#       print(f\"Found best matches for {object_synset.name()}: {top_k} using strategy 1\")\n",
        "#       for matched_synset, _ in top_k:\n",
        "#         path = path_syn_to_syn(matched_synset, object_synset, max_depth-1)\n",
        "#         if path:\n",
        "#             paths.append([predicate_synset] + path)\n",
        "\n",
        "#     # === Strategy 2: Verbs in object's gloss ===\n",
        "#     obj_gloss_doc = nlp(object_synset.definition())\n",
        "#     verbs = extract_verbs_from_gloss(obj_gloss_doc, include_passive=True)\n",
        "#     # Use instrumental verbs in object's gloss as backup\n",
        "#     verbs.extend(find_instrumental_verbs(obj_gloss_doc))\n",
        "#     # convert spacy tokens to lists of synsets\n",
        "#     verbs = [wn.synsets(v.text, pos=predicate_synset.pos()) for v in verbs]\n",
        "#     top_k = get_top_k_synset_branches(verbs[:max_sample_size], predicate_synset)\n",
        "#     if top_k:\n",
        "#       print(f\"Found best matches for {predicate_synset.name()}: {top_k} using strategy 2\")\n",
        "#       for matched_synset, _ in top_k:\n",
        "#         path = path_syn_to_syn(predicate_synset, matched_synset, max_depth-1)\n",
        "#         if path:\n",
        "#             paths.append(path + [object_synset])\n",
        "\n",
        "#     # Strategy 3: Explore the 3 most promising neighbors\n",
        "#     predicate_neighbors = get_all_neighbors(predicate_synset)\n",
        "#     object_neighbors = get_all_neighbors(object_synset)\n",
        "#     top_k = get_k_closest_synset_pairs(predicate_neighbors, object_neighbors)\n",
        "#     if top_k:\n",
        "#       print(f\"Most promising pairs for bidirectional exploration: {top_k} using strategy 3\")\n",
        "#       for p, o, _ in top_k:\n",
        "#         visited.add(predicate_synset.name())\n",
        "#         visited.add(object_synset.name())\n",
        "#         path = find_predicate_to_object_path(p, o, max_depth-1, visited)\n",
        "#         if path:\n",
        "#             paths.append([predicate_synset] + path + [object_synset])\n",
        "\n",
        "\n",
        "#     # Return shortest path if any found\n",
        "#     return min(paths, key=len) if paths else None\n",
        "\n",
        "\n",
        "# # ============================================================================\n",
        "# # Main Connected Path Finding Function\n",
        "# # ============================================================================\n",
        "\n",
        "# def find_connected_shortest_paths(\n",
        "#       subject_word,\n",
        "#       predicate_word,\n",
        "#       object_word,\n",
        "#       max_depth=10,\n",
        "#       max_self_intersection=5\n",
        "#     ):\n",
        "#     \"\"\"\n",
        "#     Find shortest connected paths from subject through predicate to object.\n",
        "#     Ensures that the same predicate synset connects both paths.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Get synsets for each word\n",
        "#     subject_synsets = wn.synsets(subject_word, pos=wn.NOUN)\n",
        "#     predicate_synsets = wn.synsets(predicate_word, pos=wn.VERB)\n",
        "#     object_synsets = wn.synsets(object_word, pos=wn.NOUN)\n",
        "\n",
        "#     best_combined_path_length = float('inf')\n",
        "#     best_subject_path = None\n",
        "#     best_object_path = None\n",
        "#     best_predicate = None\n",
        "\n",
        "#     # Try each predicate synset as the connector\n",
        "#     for pred in predicate_synsets:\n",
        "#         # Find paths from all subjects to this specific predicate\n",
        "#         subject_paths = []\n",
        "#         for subj in subject_synsets:\n",
        "#             path = find_subject_to_predicate_path(subj, pred, max_depth)\n",
        "#             if path:\n",
        "#                 subject_paths.append(path)\n",
        "\n",
        "#         # Find paths from this specific predicate to all objects\n",
        "#         object_paths = []\n",
        "#         for obj in object_synsets:\n",
        "#             path = find_predicate_to_object_path(pred, obj, max_depth)\n",
        "#             if path:\n",
        "#                 object_paths.append(path)\n",
        "\n",
        "#         # If we have both paths through this predicate, check if it's the best\n",
        "#         if subject_paths and object_paths:\n",
        "#             # find pairs of paths that don't intersect with eachother\n",
        "#             #   i.e. burglar > break_in > attack > strike > shoot > strike > attack > woman\n",
        "#             #   would not be allowed, since tautological statements are uninformative\n",
        "#             valid_pairs = list()\n",
        "#             for subj_path in subject_paths:\n",
        "#               for obj_path in object_paths:\n",
        "#                 if len(set(subj_path).intersection(set(obj_path))) <= max_self_intersection:\n",
        "#                   valid_pairs.append((\n",
        "#                       subj_path,\n",
        "#                       obj_path,\n",
        "#                       # Calculate combined length (subtract 1 to avoid counting predicate twice)\n",
        "#                       len(subj_path) + len(obj_path) - 1\n",
        "#                   ))\n",
        "\n",
        "#             if not valid_pairs:\n",
        "#               print(f\"No valid pairs of subj, obj paths found for {pred.name()}\")\n",
        "#               break\n",
        "\n",
        "#             shortest_comb_path = min(valid_pairs, key=lambda x: x[2])\n",
        "\n",
        "#             if shortest_comb_path[2] < best_combined_path_length:\n",
        "#                 best_combined_path_length = shortest_comb_path[2]\n",
        "#                 best_subject_path = shortest_comb_path[0]\n",
        "#                 best_object_path = shortest_comb_path[1]\n",
        "#                 best_predicate = pred\n",
        "\n",
        "#     return best_subject_path, best_object_path, best_predicate\n",
        "\n",
        "\n",
        "# # ============================================================================\n",
        "# # Display Functions\n",
        "# # ============================================================================\n",
        "\n",
        "# def show_path(label, path):\n",
        "#     \"\"\"Pretty print a path of synsets.\"\"\"\n",
        "#     if path:\n",
        "#         print(f\"{label}:\")\n",
        "#         print(\" -> \".join(f\"{s.name()} ({s.definition()})\" for s in path))\n",
        "#         print(f\"Path length: {len(path)}\")\n",
        "#         print()\n",
        "#     else:\n",
        "#         print(f\"{label}: No path found\")\n",
        "#         print()\n",
        "\n",
        "\n",
        "# def show_connected_paths(subject_path, object_path, predicate):\n",
        "#     \"\"\"Display the connected paths with their shared predicate.\"\"\"\n",
        "#     if subject_path and object_path and predicate:\n",
        "#         print(\"=\" * 70)\n",
        "#         print(f\"CONNECTED PATH through predicate: {predicate.name()}\")\n",
        "#         print(\"=\" * 70)\n",
        "\n",
        "#         show_path(\"Subject -> Predicate path\", subject_path)\n",
        "#         show_path(\"Predicate -> Object path\", object_path)\n",
        "\n",
        "#         # Show the complete connected path\n",
        "#         complete_path = subject_path + object_path[1:]  # Avoid duplicating the predicate\n",
        "#         print(\"Complete connected path:\")\n",
        "#         print(\" -> \".join(f\"{s.name()}\" for s in complete_path))\n",
        "#         print(f\"Total path length: {len(complete_path)}\")\n",
        "#         print()\n",
        "#     else:\n",
        "#         print(\"No connected path found through any predicate synset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "bwShzrxtIhOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = wn_to_nx()"
      ],
      "metadata": {
        "id": "c1XSDI7phuvN"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset(\"person.n.01\").hypernyms()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyUanbNslZWv",
        "outputId": "34257088-e2cf-4bb4-a13b-e1e1b4789a50"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('causal_agent.n.01'), Synset('organism.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "solver = BidirectionalAStar(\n",
        "    g,\n",
        "    \"dog.n.01\",\n",
        "    \"one.n.02\",\n",
        "    get_new_beams_fn=get_new_beams,\n",
        "    relax_beam=True\n",
        ")\n",
        "path = solver.search()\n",
        "if path:\n",
        "    print(\"found path:\", path)\n",
        "    print(\"meet:\", solver.meet_node, \"cost:\", solver.path_cost)\n",
        "else:\n",
        "    print(\"no path found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "valgVwQUh0SJ",
        "outputId": "e9e42bc9-2e4a-4669-ca18-4813afe766a5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found path: ['dog.n.01', 'pack.n.06', 'animal_group.n.01', 'biological_group.n.01', 'group.n.01', 'abstraction.n.06', 'measure.n.02', 'playing_period.n.01', 'chukker.n.01', 'part.n.09', 'whole.n.01', 'unit.n.04', 'one.n.02']\n",
            "meet: playing_period.n.01 cost: 12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJjxiYEy9smM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
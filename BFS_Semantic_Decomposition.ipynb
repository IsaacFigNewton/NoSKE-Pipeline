{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2J9B-9RI5lRP",
        "dSlaJDUEPnfT"
      ],
      "authorship_tag": "ABX9TyPJQCjDIrTpdPBdLfKZuibl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/SMIED/blob/main/BFS_Semantic_Decomposition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "2J9B-9RI5lRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "GReqNuQt7wvb",
        "outputId": "f80e9608-ae32-4d31-c65c-51589b8701e2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "679ad3197b3a400b93eedb820c7c4c25"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import, config stuff"
      ],
      "metadata": {
        "id": "dSlaJDUEPnfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Semantic decomposition of (\"cat\", \"eats\", \"mouse\") using WordNet + spaCy + depth-limited GBFS.\n",
        "- Uses spaCy to parse verb synset glosses and detect subject/object dependencies.\n",
        "- If both subject and object tokens are present, branches directly toward original triple synsets.\n",
        "- Otherwise falls back to WordNet relations.\n",
        "\"\"\"\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from heapq import heappush, heappop\n",
        "import numpy as np\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gensim.downloader as api\n",
        "from collections import deque\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "import spacy"
      ],
      "metadata": {
        "id": "Jb_gtRCvh-DF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "# Load spaCy English model for dependency parsing\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj4iZlAfh_t8",
        "outputId": "377d24dd-4665-4dbf-c571-a94162bb554d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5TGC7I88EvX",
        "outputId": "379866ee-eb76-473a-f540-3bfef1e64d24"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpers"
      ],
      "metadata": {
        "id": "H6Ahax9MciBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_neighbors(synset: wn.synset) -> List[wn.synset]:\n",
        "    \"\"\"Get all neighbors of a synset based on its POS.\"\"\"\n",
        "    neighbors = []\n",
        "\n",
        "    # Add hypernyms and hyponyms\n",
        "    neighbors.extend(synset.hypernyms())\n",
        "    neighbors.extend(synset.hyponyms())\n",
        "\n",
        "    # Add POS-specific neighbors\n",
        "    if synset.pos() == 'n':\n",
        "        neighbors.extend(get_noun_neighbors(synset))\n",
        "    else:\n",
        "        neighbors.extend(get_verb_neighbors(synset))\n",
        "\n",
        "    return neighbors\n",
        "\n",
        "\n",
        "def get_noun_neighbors(syn: wn.synset):\n",
        "    \"\"\"Get neighbors for a noun synset.\"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.part_meronyms())\n",
        "    nbrs.update(syn.substance_meronyms())\n",
        "    nbrs.update(syn.member_meronyms())\n",
        "    nbrs.update(syn.part_holonyms())\n",
        "    nbrs.update(syn.substance_holonyms())\n",
        "    nbrs.update(syn.member_holonyms())\n",
        "    return list(nbrs)\n",
        "\n",
        "\n",
        "def get_verb_neighbors(syn: wn.synset):\n",
        "    \"\"\"Get neighbors for a verb synset.\"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.entailments())\n",
        "    nbrs.update(syn.causes())\n",
        "    nbrs.update(syn.also_sees())\n",
        "    nbrs.update(syn.verb_groups())\n",
        "    return list(nbrs)"
      ],
      "metadata": {
        "id": "xoGoYrlVcjiG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding similarities"
      ],
      "metadata": {
        "id": "zJVKWnjy5qNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def wn_to_nx():\n",
        "    # Initialize directed graph\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    synset_rels = {\n",
        "        # holonyms\n",
        "        \"part_holonyms\": lambda x: x.part_holonyms(),\n",
        "        \"substance_holonyms\": lambda x: x.substance_holonyms(),\n",
        "        \"member_holonyms\": lambda x: x.member_holonyms(),\n",
        "\n",
        "        # meronyms\n",
        "        \"part_meronyms\": lambda x: x.part_meronyms(),\n",
        "        \"substance_meronyms\": lambda x: x.substance_meronyms(),\n",
        "        \"member_meronyms\": lambda x: x.member_meronyms(),\n",
        "\n",
        "        # other\n",
        "        \"hypernyms\": lambda x: x.hypernyms(),\n",
        "        \"hyponyms\": lambda x: x.hyponyms(),\n",
        "        \"entailments\": lambda x: x.entailments(),\n",
        "        \"causes\": lambda x: x.causes(),\n",
        "        \"also_sees\": lambda x: x.also_sees(),\n",
        "        \"verb_groups\": lambda x: x.verb_groups(),\n",
        "    }\n",
        "\n",
        "    # add nodes (synsets) and all their edges (lexical relations) to the nx graph\n",
        "    for synset in wn.all_synsets():\n",
        "        for rel_name, rel_func in synset_rels.items():\n",
        "            for target in rel_func(synset):\n",
        "                G.add_edge(\n",
        "                    synset.name(),\n",
        "                    target.name(),\n",
        "                    relation = rel_name[:-1]\n",
        "                )\n",
        "    return G"
      ],
      "metadata": {
        "id": "udDC5rrRAy6x"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src = wn.synset('dog.n.01')      # example\n",
        "tgt = wn.synset('mouse.n.01')   # example\n",
        "\n",
        "# 3. Use Yen's algorithm for k shortest simple paths\n",
        "paths = list(nx.shortest_simple_paths(G, src.name(), tgt.name()))\n",
        "\n",
        "# Get top k paths\n",
        "k = 3\n",
        "print(f\"Top {k} paths from {src.name()} to {tgt.name()}:\")\n",
        "for i, p in enumerate(paths[:k], 1):\n",
        "    print(f\"Path {i}: {p}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "wjd-le_TF2th",
        "outputId": "8c381e90-6c2d-4987-e769-fa8bbef76d83"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-367375838.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 3. Use Yen's algorithm for k shortest simple paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortest_simple_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Get top k paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/networkx/algorithms/simple_paths.py\u001b[0m in \u001b[0;36mshortest_simple_paths\u001b[0;34m(G, source, target, weight)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0mroot_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlength_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m                         \u001b[0mignore_edges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Helpers"
      ],
      "metadata": {
        "id": "oL_kkfMUY1bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synset_embedding_centroid(synset:wn.synset, model=embedding_model):\n",
        "    \"\"\"\n",
        "    Get the centroid (mean) of token embeddings for all lemmas in a synset.\n",
        "\n",
        "    Args:\n",
        "        synset: WordNet Synset object (e.g., 'dog.n.01')\n",
        "        model: Loaded token model\n",
        "\n",
        "    Returns:\n",
        "        numpy array representing the centroid, or an empty numpy array if no lemmas found\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get all lemma names from the synset\n",
        "        lemmas = [lemma.name().lower().replace('_', ' ') for lemma in synset.lemmas()]\n",
        "        # Collect embeddings for lemmas that exist in the model\n",
        "        embeddings = []\n",
        "        found_lemmas = []\n",
        "\n",
        "        for lemma in lemmas:\n",
        "            # Try the lemma as-is first\n",
        "            if lemma in model:\n",
        "                embeddings.append(model[lemma])\n",
        "                found_lemmas.append(lemma)\n",
        "            # Try with underscores replaced by spaces (for multi-word terms)\n",
        "            elif lemma.replace(' ', '_') in model:\n",
        "                embeddings.append(model[lemma.replace(' ', '_')])\n",
        "                found_lemmas.append(lemma)\n",
        "            # Try individual words if it's a multi-word term\n",
        "            elif ' ' in lemma:\n",
        "                words = lemma.split()\n",
        "                word_embeddings = []\n",
        "                for word in words:\n",
        "                    if word in model:\n",
        "                        word_embeddings.append(model[word])\n",
        "                if word_embeddings:\n",
        "                    # Average the embeddings of individual words\n",
        "                    embeddings.append(np.mean(word_embeddings, axis=0))\n",
        "                    found_lemmas.append(lemma)\n",
        "\n",
        "        if not embeddings:\n",
        "            # print(f\"Warning: No lemmas from {synset.name()} found in token model\")\n",
        "            # print(f\"  Attempted lemmas: {lemmas}\")\n",
        "            return np.array([])\n",
        "\n",
        "        # print(f\"Synset {synset.name()}: Found {len(found_lemmas)}/{len(lemmas)} lemmas in model\")\n",
        "        # print(f\"  Found: {found_lemmas}\")\n",
        "\n",
        "        # Return the mean of all embeddings\n",
        "        return np.mean(embeddings, axis=0)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing synset {synset.name()}: {e}\")\n",
        "        return np.array([]) # Return empty array on error\n",
        "\n",
        "\n",
        "def embed_lexical_relations(synset: wn.synset, model) -> Dict[str, List[Tuple[str, np.ndarray]]]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        synset: WordNet Synset object (e.g., 'dog.n.01')\n",
        "\n",
        "    Returns:\n",
        "        Dict of embeddings for lexical relations of the synset.\n",
        "    \"\"\"\n",
        "    def _rel_centroids(get_attr) -> List[Tuple[str, np.ndarray]]:\n",
        "      try:\n",
        "        centroids = [\n",
        "            (s.name(), get_synset_embedding_centroid(s))\n",
        "            for s in get_attr(synset)\n",
        "        ]\n",
        "        # Filter out empty arrays and return the mean\n",
        "        return [c for c in centroids if c.size > 0]\n",
        "      except Exception as e:\n",
        "        print(f\"Error processing relation for synset {synset.name()}: {e}\")\n",
        "        return np.array([])\n",
        "\n",
        "    return {\n",
        "        # holonyms\n",
        "        \"part_holonyms\": _rel_centroids(lambda x: x.part_holonyms()),\n",
        "        \"substance_holonyms\": _rel_centroids(lambda x: x.substance_holonyms()),\n",
        "        \"member_holonyms\": _rel_centroids(lambda x: x.member_holonyms()),\n",
        "\n",
        "        # meronyms\n",
        "        \"part_meronyms\": _rel_centroids(lambda x: x.part_meronyms()),\n",
        "        \"substance_meronyms\": _rel_centroids(lambda x: x.substance_meronyms()),\n",
        "        \"member_meronyms\": _rel_centroids(lambda x: x.member_meronyms()),\n",
        "\n",
        "        # other\n",
        "        \"hypernyms\": _rel_centroids(lambda x: x.hypernyms()),\n",
        "        \"hyponyms\": _rel_centroids(lambda x: x.hyponyms()),\n",
        "        \"entailments\": _rel_centroids(lambda x: x.entailments()),\n",
        "        \"causes\": _rel_centroids(lambda x: x.causes()),\n",
        "        \"also_sees\": _rel_centroids(lambda x: x.also_sees()),\n",
        "        \"verb_groups\": _rel_centroids(lambda x: x.verb_groups()),\n",
        "    }\n",
        "\n",
        "\n",
        "def get_embedding_similarities(\n",
        "      rel_embs_1: List[Tuple[str, np.ndarray]],\n",
        "      rel_embs_2: List[Tuple[str, np.ndarray]],\n",
        "    ) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    get similarities of all possible pairings\n",
        "      between elements of the asymmetric lexical categories\n",
        "    \"\"\"\n",
        "    # convert the lists of centroid embeddings into rank 2 tensors\n",
        "    # e1_rel_embs: shape (m, d)\n",
        "    embs_1 = np.array([e[1] for e in rel_embs_1])\n",
        "    # e2_rel_embs: shape (n, d)\n",
        "    embs_2 = np.array([e[1] for e in rel_embs_2])\n",
        "\n",
        "    # Normalize each embedding vector to unit length\n",
        "    e1_norm = embs_1 / np.linalg.norm(embs_1, axis=1, keepdims=True)\n",
        "    e2_norm = embs_1 / np.linalg.norm(embs_1, axis=1, keepdims=True)\n",
        "\n",
        "    # Compute cosine similarities via dot product\n",
        "    cosine_sims = np.dot(e1_norm, e2_norm.T)  # shape (m, n)\n",
        "    return cosine_sims\n",
        "\n",
        "\n",
        "def get_top_k_aligned_lex_rel_pairs(\n",
        "      src_tgt_rel_map:Dict[str, List[Tuple[str, np.ndarray]]],\n",
        "      src_emb_dict:Dict[str, List[Tuple[str, np.ndarray]]],\n",
        "      tgt_emb_dict:Dict[str, List[Tuple[str, np.ndarray]]],\n",
        "      model=embedding_model,\n",
        "      beam_width:int=-1\n",
        "    ) -> List[Tuple[\n",
        "        Tuple[str, str],\n",
        "        Tuple[str, str],\n",
        "        float\n",
        "    ]]:\n",
        "    \"\"\"\n",
        "    Get the k most similar symmetric/asymmetric lexical relationship pairs\n",
        "      based on the relations' associated synset embeddings.\n",
        "\n",
        "    Returns a list of tuples of the form:\n",
        "      (\n",
        "        (synset1, lexical_rel),\n",
        "        (synset2, lexical_rel),\n",
        "        relatedness\n",
        "      )\n",
        "    \"\"\"\n",
        "\n",
        "    # check similarity of relations\n",
        "    #   i.e. similarity of synset1's meronyms to synset2's meronyms\n",
        "    rel_sims = list()\n",
        "    for e1_rel, e2_rel in src_tgt_rel_map.items():\n",
        "        # get embedding lists for e1_rel and e2_rel in the associated embedding dicts\n",
        "        e1_rel_syn_embs = src_emb_dict.get(e1_rel)\n",
        "        e2_rel_syn_embs = tgt_emb_dict.get(e2_rel)\n",
        "\n",
        "        # if there are >0 embeddings in each list\n",
        "        if len(e1_rel_syn_embs) > 0 and len(e2_rel_syn_embs) > 0:\n",
        "            # get similarities of all possible pairings\n",
        "            sims = get_embedding_similarities(e1_rel_syn_embs, e2_rel_syn_embs)\n",
        "            # add tuples relating the base synset to the neighbors\n",
        "            #   and their similarities to the asymm_rel_sims list\n",
        "            for i, j in np.ndindex(sims.shape):\n",
        "                rel_sims.append(\n",
        "                    (\n",
        "                        (e1_rel_syn_embs[i][0], e1_rel),\n",
        "                        (e2_rel_syn_embs[j][0], e2_rel),\n",
        "                        sims[i, j]\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    # it doesn't matter which one is contains/includes/etc. the other,\n",
        "    #   as long as they're closer than antonyms or unrelated terms\n",
        "    # i.e. a good hyponym-hypernym pair is just as important\n",
        "    #   as a good hypernym-hyponym pair\n",
        "    beam = sorted(\n",
        "        rel_sims,\n",
        "        key=lambda x: x[2],\n",
        "        reverse=True\n",
        "    )[:beam_width]\n",
        "    return beam"
      ],
      "metadata": {
        "id": "th4qyXLmYH4j"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam Construction"
      ],
      "metadata": {
        "id": "7cmuVbqxY6ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asymmetric_pairs_map = {\n",
        "    # holonyms\n",
        "    \"part_holonyms\": \"part_meronyms\",\n",
        "    \"substance_holonyms\": \"substance_meronyms\",\n",
        "    \"member_holonyms\": \"member_meronyms\",\n",
        "\n",
        "    # meronyms\n",
        "    \"part_meronyms\": \"part_holonyms\",\n",
        "    \"substance_meronyms\": \"substance_holonyms\",\n",
        "    \"member_meronyms\": \"member_holonyms\",\n",
        "\n",
        "    # other\n",
        "    \"hypernyms\": \"hyponyms\",\n",
        "    \"hyponyms\": \"hyponyms\"\n",
        "}\n",
        "\n",
        "\n",
        "symmetric_pairs_map = {\n",
        "    # holonyms\n",
        "    \"part_holonyms\": \"part_holonyms\",\n",
        "    \"substance_holonyms\": \"substance_holonyms\",\n",
        "    \"member_holonyms\": \"member_holonyms\",\n",
        "\n",
        "    # meronyms\n",
        "    \"part_meronyms\": \"part_meronyms\",\n",
        "    \"substance_meronyms\": \"substance_meronyms\",\n",
        "    \"member_meronyms\": \"member_meronyms\",\n",
        "\n",
        "    # other\n",
        "    \"hypernyms\": \"hypernyms\",\n",
        "    \"hyponyms\": \"hyponyms\",\n",
        "    \"entailments\": \"entailments\",\n",
        "    \"causes\": \"causes\",\n",
        "    \"also_sees\": \"also_sees\",\n",
        "    \"verb_groups\": \"verb_groups\"\n",
        "}"
      ],
      "metadata": {
        "id": "25u1o6wq5req"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_new_beams(\n",
        "      g: nx.DiGraph,\n",
        "      src: str,\n",
        "      tgt: str,,\n",
        "      model=embedding_model,\n",
        "      beam_width=3\n",
        "    ) -> List[Tuple[\n",
        "        Tuple[str, str],\n",
        "        Tuple[str, str],\n",
        "        float\n",
        "    ]]:\n",
        "    \"\"\"\n",
        "    Get the k closest pairs of lexical relations between 2 synsets.\n",
        "\n",
        "    Args:\n",
        "        src: WordNet Synset object (e.g., 'dog.n.01')\n",
        "        tgt: WordNet Synset object (e.g., 'cat.n.01')\n",
        "        model: token model (if None, will load default)\n",
        "        beam_width: max number of pairs to return\n",
        "\n",
        "    Returns:\n",
        "        List of tuples of the form:\n",
        "          (\n",
        "            (synset1, lexical_rel),\n",
        "            (synset2, lexical_rel),\n",
        "            relatedness\n",
        "          )\n",
        "    \"\"\"\n",
        "\n",
        "    # Build a map of each synset's associated lexical relations\n",
        "    #   and the centroids of their associated synsets\n",
        "    src_lex_rel_embs = embed_lexical_relations(wn.synset(src), model)\n",
        "    tgt_lex_rel_embs = embed_lexical_relations(wn.synset(tgt), model)\n",
        "\n",
        "    # ensure the edges in the nx graph align with those in the embedding maps\n",
        "    src_neighbors = {n for n in g.neighbors(src)}\n",
        "    for synset_list in src_lex_rel_embs:\n",
        "      assert all(n[0] in src_neighbors for n in synset_list)\n",
        "    tgt_neighbors = {n for n in g.neighbors(tgt)}\n",
        "    for synset_list in tgt_lex_rel_embs:\n",
        "      assert all(n[0] in src_neighbors for n in synset_list)\n",
        "    # in the future, get neighbor relation in node metadata with g.adj[n]\n",
        "\n",
        "    # Get the asymmetric lexical relation pairings,\n",
        "    #   sorted in descending order of embedding similarity\n",
        "    #   e.x. similarity of synset1's hypernyms to synset2's hypernyms\n",
        "    asymm_lex_rel_sims = get_top_k_aligned_lex_rel_pairs(\n",
        "        asymmetric_pairs_map,\n",
        "        src_lex_rel_embs,\n",
        "        tgt_lex_rel_embs,\n",
        "        model,\n",
        "        beam_width\n",
        "    )\n",
        "    # Get the symmetric lexical relation pairings,\n",
        "    #   sorted in descending order of embedding similarity\n",
        "    #   e.x. similarity of synset1's hypernyms to synset2's hypernyms\n",
        "    symm_lex_rel_sims = get_top_k_aligned_lex_rel_pairs(\n",
        "        symmetric_pairs_map,\n",
        "        src_lex_rel_embs,\n",
        "        tgt_lex_rel_embs,\n",
        "        model,\n",
        "        beam_width\n",
        "    )\n",
        "    combined = asymm_lex_rel_sims + symm_lex_rel_sims\n",
        "    beam = sorted(combined, key=lambda x: x[2], reverse=True)[:beam_width]\n",
        "    return beam"
      ],
      "metadata": {
        "id": "iTYjzwULZCJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity([[-1]], [[1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjWIoM4MyzNV",
        "outputId": "0ab65e15-dbb5-4dd3-c0be-014f08fec99e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pathing"
      ],
      "metadata": {
        "id": "X1WdFnm55oTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bidirectional_a_star(g: nx.DiGraph, src: str, tgt: str):\n",
        "  \"\"\"\n",
        "  Perform bidirectional A* search on a graph.\n",
        "  \"\"\"\n",
        "  # get the k best pairs of neighbors from g.neighbors(src) and g.neighbors(tgt)\n",
        "  # use the each pair's similarity as\n",
        "  new_beams = get_new_beams(g, src, tgt)\n",
        "  src_beam = [(n[0], n[2]) for n in new_beams]\n",
        "  tgt_beam = [(n[1], n[2]) for n in new_beams]\n",
        "\n",
        "  # explore each node with an index in the associated beams\n",
        "  pass"
      ],
      "metadata": {
        "id": "HpemJQhSZIJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ7mH1XuhtBN"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Core Path Finding Functions\n",
        "# ============================================================================\n",
        "\n",
        "def path_syn_to_syn(start_synset, end_synset, max_depth=6):\n",
        "    \"\"\"\n",
        "    Find shortest path between synsets of the same POS using bidirectional BFS.\n",
        "    Returns a list of synsets forming the path, or None if no path found.\n",
        "    \"\"\"\n",
        "\n",
        "    if not (start_synset.pos() == end_synset.pos() and start_synset.pos() in {'n', 'v'}):\n",
        "      raise ValueError(f\"{start_synset.name()} POS tag != {end_synset.name()}. Synsets must be of the same POS (noun or verb).\")\n",
        "\n",
        "    # Handle the trivial case where start and end are the same\n",
        "    if start_synset.name() == end_synset.name():\n",
        "        return [start_synset]\n",
        "\n",
        "    # Initialize two search frontiers\n",
        "    forward_queue = deque([(start_synset, 0)])\n",
        "    forward_visited = {start_synset.name(): [start_synset]}\n",
        "\n",
        "    backward_queue = deque([(end_synset, 0)])\n",
        "    backward_visited = {end_synset.name(): [end_synset]}\n",
        "\n",
        "    def expand_frontier(queue, visited_from_this_side, visited_from_other_side, is_forward):\n",
        "        \"\"\"Expand one step of the search frontier.\"\"\"\n",
        "        if not queue:\n",
        "            return None\n",
        "\n",
        "        curr_synset, depth = queue.popleft()\n",
        "\n",
        "        if depth >= (max_depth + 1) // 2:\n",
        "            return None\n",
        "\n",
        "        path_to_current = visited_from_this_side[curr_synset.name()]\n",
        "\n",
        "        for neighbor in get_all_neighbors(curr_synset):\n",
        "            neighbor_name = neighbor.name()\n",
        "\n",
        "            if neighbor_name in visited_from_this_side:\n",
        "                continue\n",
        "\n",
        "            if is_forward:\n",
        "                new_path = path_to_current + [neighbor]\n",
        "            else:\n",
        "                new_path = [neighbor] + path_to_current\n",
        "\n",
        "            if neighbor_name in visited_from_other_side:\n",
        "                other_path = visited_from_other_side[neighbor_name]\n",
        "\n",
        "                if is_forward:\n",
        "                    full_path = path_to_current + other_path\n",
        "                else:\n",
        "                    full_path = other_path + path_to_current\n",
        "\n",
        "                return full_path\n",
        "\n",
        "            visited_from_this_side[neighbor_name] = new_path\n",
        "            queue.append((neighbor, depth + 1))\n",
        "\n",
        "        return None\n",
        "\n",
        "    # Alternate between forward and backward search\n",
        "    while forward_queue or backward_queue:\n",
        "        if forward_queue:\n",
        "            result = expand_frontier(forward_queue, forward_visited, backward_visited, True)\n",
        "            if result:\n",
        "                return result\n",
        "\n",
        "        if backward_queue:\n",
        "            result = expand_frontier(backward_queue, backward_visited, forward_visited, False)\n",
        "            if result:\n",
        "                return result\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Gloss Analysis Helper Functions\n",
        "# ============================================================================\n",
        "\n",
        "def extract_subjects_from_gloss(gloss_doc):\n",
        "    \"\"\"Extract subject tokens from a parsed gloss.\"\"\"\n",
        "    subjects = []\n",
        "\n",
        "    # Direct subjects\n",
        "    subjects.extend([tok for tok in gloss_doc if tok.dep_ == \"nsubj\"])\n",
        "\n",
        "    # Passive subjects (which are actually objects semantically)\n",
        "    # Skip these for actor identification\n",
        "    passive_subjects = [tok for tok in gloss_doc if tok.dep_ == \"nsubjpass\"]\n",
        "\n",
        "    # Filter out passive subjects from the main list\n",
        "    subjects = [s for s in subjects if s not in passive_subjects]\n",
        "\n",
        "    return subjects, passive_subjects\n",
        "\n",
        "\n",
        "def extract_objects_from_gloss(gloss_doc):\n",
        "    \"\"\"Extract various types of object tokens from a parsed gloss.\"\"\"\n",
        "    objs = []\n",
        "\n",
        "    # Indirect objects\n",
        "    iobjs = [tok for tok in gloss_doc if tok.dep_ == \"iobj\"]\n",
        "    objs.extend(iobjs)\n",
        "\n",
        "    # Direct objects\n",
        "    # Only include if there were no indirect objects,\n",
        "    #   crude, but good for MVP\n",
        "    if not iobjs:\n",
        "        objs.extend([tok for tok in gloss_doc if tok.dep_ == \"dobj\"])\n",
        "\n",
        "    # Prepositional objects\n",
        "    objs.extend([tok for tok in gloss_doc if tok.dep_ == \"pobj\"])\n",
        "\n",
        "    # General objects\n",
        "    objs.extend([tok for tok in gloss_doc if tok.dep_ == \"obj\"])\n",
        "\n",
        "    # Check for noun chunks related to root verb\n",
        "    root_verbs = [tok for tok in gloss_doc if tok.dep_ == \"ROOT\" and tok.pos_ == \"VERB\"]\n",
        "    if root_verbs and not objs:\n",
        "        for noun_chunk in gloss_doc.noun_chunks:\n",
        "            if any(token.head == root_verbs[0] for token in noun_chunk):\n",
        "                objs.append(noun_chunk.root)\n",
        "\n",
        "    return objs\n",
        "\n",
        "\n",
        "def extract_verbs_from_gloss(gloss_doc, include_passive=False):\n",
        "    \"\"\"Extract verb tokens from a parsed gloss.\"\"\"\n",
        "    verbs = [tok for tok in gloss_doc if tok.pos_ == \"VERB\"]\n",
        "\n",
        "    if include_passive:\n",
        "        # Past participles used as adjectives or in relative clauses\n",
        "        passive_verbs = [tok for tok in gloss_doc if\n",
        "                        tok.tag_ in [\"VBN\", \"VBD\"] and\n",
        "                        tok.dep_ in [\"acl\", \"relcl\", \"amod\"]]\n",
        "        verbs.extend(passive_verbs)\n",
        "\n",
        "    return verbs\n",
        "\n",
        "\n",
        "def find_instrumental_verbs(gloss_doc):\n",
        "    \"\"\"Find verbs associated with instrumental use (e.g., 'used for').\"\"\"\n",
        "    instrumental_verbs = []\n",
        "\n",
        "    if \"used\" in gloss_doc.text.lower():\n",
        "        for i, token in enumerate(gloss_doc):\n",
        "            if token.text.lower() == \"used\":\n",
        "                # Check tokens after \"used\"\n",
        "                for j in range(i+1, min(i+4, len(gloss_doc))):\n",
        "                    if gloss_doc[j].pos_ == \"VERB\":\n",
        "                        instrumental_verbs.append(gloss_doc[j])\n",
        "\n",
        "    return instrumental_verbs\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Cross-POS Path Finding Functions\n",
        "# ============================================================================\n",
        "def get_top_k_synset_branch_pairs(\n",
        "      candidates: List[List[wn.synset]],\n",
        "      target_synset: wn.synset,\n",
        "      beam_width=3\n",
        "    ) -> List[Tuple[\n",
        "        Tuple[wn.synset, str],\n",
        "        Tuple[wn.synset, str],\n",
        "        float\n",
        "    ]]:\n",
        "    \"\"\"\n",
        "    Given a list of candidate tokens and a target synset,\n",
        "\n",
        "\n",
        "    Return the k synset subrelation pairs most similar to the target of the form:\n",
        "      ((synset, lexical_rel), (name, lexical_rel), relatedness)\n",
        "    \"\"\"\n",
        "    top_k_asymm_branches = list()\n",
        "    top_k_symm_branches = list()\n",
        "    beam = list()\n",
        "    # for each list of possible synsets for a candidate token\n",
        "    for synsets in candidates:\n",
        "        # # filter to subjects based on whether they reside in the same sub-category\n",
        "        # #   where the subcategory != 'entity.n.01' or a similar top-level\n",
        "        # synsets = [\n",
        "        #     s for s in synsets\n",
        "        #     if s.root_hypernyms() != s.lowest_common_hypernyms(target_synset)\n",
        "        # ]\n",
        "        # # if there are synsets left for the candidate token after pruning\n",
        "        if synsets:\n",
        "            # # if the target is a verb,\n",
        "            # #   filter out any synsets with no lemma frames matching the target\n",
        "            # #   frame patterns: (Somebody [v] something), (Somebody [v]), ...\n",
        "            # if target_synset.pos() == 'v':\n",
        "            #     synsets = [\n",
        "            #         s for s in synsets\n",
        "            #         if any(\n",
        "            #             frame in s.frame_ids()\n",
        "            #             for frame in target_synset.frame_ids()\n",
        "            #         )\n",
        "            #     ]\n",
        "            for synset in synsets:\n",
        "              beam += get_synset_relatedness(synset, target_synset)\n",
        "              beam += get_synset_relatedness(synset, target_synset)\n",
        "    beam = sorted(\n",
        "        beam,\n",
        "        key=lambda x: x[2],\n",
        "        reverse=True\n",
        "    )[:beam_width]\n",
        "    return beam\n",
        "\n",
        "\n",
        "def find_subject_to_predicate_path(\n",
        "      subject_synset: wn.synset,\n",
        "      predicate_synset: wn.synset,\n",
        "      max_depth:int,\n",
        "      visited=set(),\n",
        "      max_sample_size=5,\n",
        "    ):\n",
        "    \"\"\"Find path from subject (noun) to predicate (verb).\"\"\"\n",
        "    if subject_synset.name() in visited or predicate_synset.name() in visited:\n",
        "      return None\n",
        "\n",
        "    paths = []\n",
        "    print()\n",
        "    print(f\"Finding path from {subject_synset.name()} to {predicate_synset.name()}\")\n",
        "\n",
        "    # Strategy 1: Look for active subjects in verb's gloss\n",
        "    pred_gloss_doc = nlp(predicate_synset.definition())\n",
        "    # passive subjects are semantically equivalent to objects\n",
        "    active_subjects, _ = extract_subjects_from_gloss(pred_gloss_doc)\n",
        "    # convert spacy tokens to lists of synsets\n",
        "    subjects = [wn.synsets(s.text, pos=subject_synset.pos()) for s in active_subjects]\n",
        "    # of the remaining subjects, get the most similar\n",
        "    top_k = get_top_k_synset_branches(active_subjects[:max_sample_size], subject_synset)\n",
        "    if top_k:\n",
        "      print(f\"Found best matches for {subject_synset.name()}: {top_k} using strategy 1\")\n",
        "      for matched_synset, _ in top_k:\n",
        "        path = path_syn_to_syn(subject_synset, matched_synset, max_depth-1)\n",
        "        if path:\n",
        "            paths.append(path + [predicate_synset])\n",
        "\n",
        "    # Strategy 2: Look for verbs in the noun's gloss\n",
        "    subj_gloss_doc = nlp(subject_synset.definition())\n",
        "    verbs = extract_verbs_from_gloss(subj_gloss_doc, include_passive=False)\n",
        "    # convert spacy tokens to lists of synsets\n",
        "    verbs = [wn.synsets(v.text, pos=predicate_synset.pos()) for v in verbs]\n",
        "    # of the remaining subjects, get the most similar\n",
        "    top_k = get_top_k_synset_branches(verbs[:max_sample_size], predicate_synset)\n",
        "    if top_k:\n",
        "      print(f\"Found best matches for {predicate_synset.name()}: {top_k} using strategy 2\")\n",
        "      for matched_synset, _ in top_k:\n",
        "        path = path_syn_to_syn(matched_synset, predicate_synset, max_depth-1)\n",
        "        if path:\n",
        "            paths.append([subject_synset] + path)\n",
        "\n",
        "    # Strategy 3: Explore the 3 most promising pairs of neighbors\n",
        "    subject_neighbors = get_all_neighbors(subject_synset)\n",
        "    predicate_neighbors = get_all_neighbors(predicate_synset)\n",
        "    top_k = get_k_closest_synset_pairs(subject_neighbors, predicate_neighbors)\n",
        "    if top_k:\n",
        "      print(f\"Most promising pairs for bidirectional exploration: {top_k} using strategy 3\")\n",
        "      for s, p, _ in top_k:\n",
        "        visited.add(subject_synset.name())\n",
        "        visited.add(predicate_synset.name())\n",
        "        path = find_subject_to_predicate_path(s, p, max_depth-1, visited)\n",
        "        if path:\n",
        "            paths.append([subject_synset] + path + [predicate_synset])\n",
        "\n",
        "\n",
        "    # Return shortest path if any found\n",
        "    return min(paths, key=len) if paths else None\n",
        "\n",
        "\n",
        "def find_predicate_to_object_path(\n",
        "      predicate_synset: wn.synset,\n",
        "      object_synset: wn.synset,\n",
        "      max_depth:int,\n",
        "      visited=set(),\n",
        "      max_sample_size=5,\n",
        "    ):\n",
        "    \"\"\"Find path from predicate (verb) to object (noun).\"\"\"\n",
        "\n",
        "    if predicate_synset.name() in visited or object_synset.name() in visited:\n",
        "      return None\n",
        "\n",
        "    paths = []\n",
        "    print()\n",
        "    print(f\"Finding path from {predicate_synset.name()} to {object_synset.name()}\")\n",
        "\n",
        "    # === Strategy 1: Objects in predicate gloss (incl. passive subjects) ===\n",
        "    pred_gloss_doc = nlp(predicate_synset.definition())\n",
        "    objects = extract_objects_from_gloss(pred_gloss_doc)\n",
        "    _, passive_subjects = extract_subjects_from_gloss(pred_gloss_doc)\n",
        "    objects.extend(passive_subjects)\n",
        "    # convert spacy tokens to lists of synsets\n",
        "    objects = [wn.synsets(o.text, pos=object_synset.pos()) for o in objects]\n",
        "    top_k = get_top_k_synset_branches(objects[:max_sample_size], object_synset)\n",
        "    if top_k:\n",
        "      print(f\"Found best matches for {object_synset.name()}: {top_k} using strategy 1\")\n",
        "      for matched_synset, _ in top_k:\n",
        "        path = path_syn_to_syn(matched_synset, object_synset, max_depth-1)\n",
        "        if path:\n",
        "            paths.append([predicate_synset] + path)\n",
        "\n",
        "    # === Strategy 2: Verbs in object's gloss ===\n",
        "    obj_gloss_doc = nlp(object_synset.definition())\n",
        "    verbs = extract_verbs_from_gloss(obj_gloss_doc, include_passive=True)\n",
        "    # Use instrumental verbs in object's gloss as backup\n",
        "    verbs.extend(find_instrumental_verbs(obj_gloss_doc))\n",
        "    # convert spacy tokens to lists of synsets\n",
        "    verbs = [wn.synsets(v.text, pos=predicate_synset.pos()) for v in verbs]\n",
        "    top_k = get_top_k_synset_branches(verbs[:max_sample_size], predicate_synset)\n",
        "    if top_k:\n",
        "      print(f\"Found best matches for {predicate_synset.name()}: {top_k} using strategy 2\")\n",
        "      for matched_synset, _ in top_k:\n",
        "        path = path_syn_to_syn(predicate_synset, matched_synset, max_depth-1)\n",
        "        if path:\n",
        "            paths.append(path + [object_synset])\n",
        "\n",
        "    # Strategy 3: Explore the 3 most promising neighbors\n",
        "    predicate_neighbors = get_all_neighbors(predicate_synset)\n",
        "    object_neighbors = get_all_neighbors(object_synset)\n",
        "    top_k = get_k_closest_synset_pairs(predicate_neighbors, object_neighbors)\n",
        "    if top_k:\n",
        "      print(f\"Most promising pairs for bidirectional exploration: {top_k} using strategy 3\")\n",
        "      for p, o, _ in top_k:\n",
        "        visited.add(predicate_synset.name())\n",
        "        visited.add(object_synset.name())\n",
        "        path = find_predicate_to_object_path(p, o, max_depth-1, visited)\n",
        "        if path:\n",
        "            paths.append([predicate_synset] + path + [object_synset])\n",
        "\n",
        "\n",
        "    # Return shortest path if any found\n",
        "    return min(paths, key=len) if paths else None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Main Connected Path Finding Function\n",
        "# ============================================================================\n",
        "\n",
        "def find_connected_shortest_paths(\n",
        "      subject_word,\n",
        "      predicate_word,\n",
        "      object_word,\n",
        "      max_depth=10,\n",
        "      max_self_intersection=5\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Find shortest connected paths from subject through predicate to object.\n",
        "    Ensures that the same predicate synset connects both paths.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get synsets for each word\n",
        "    subject_synsets = wn.synsets(subject_word, pos=wn.NOUN)\n",
        "    predicate_synsets = wn.synsets(predicate_word, pos=wn.VERB)\n",
        "    object_synsets = wn.synsets(object_word, pos=wn.NOUN)\n",
        "\n",
        "    best_combined_path_length = float('inf')\n",
        "    best_subject_path = None\n",
        "    best_object_path = None\n",
        "    best_predicate = None\n",
        "\n",
        "    # Try each predicate synset as the connector\n",
        "    for pred in predicate_synsets:\n",
        "        # Find paths from all subjects to this specific predicate\n",
        "        subject_paths = []\n",
        "        for subj in subject_synsets:\n",
        "            path = find_subject_to_predicate_path(subj, pred, max_depth)\n",
        "            if path:\n",
        "                subject_paths.append(path)\n",
        "\n",
        "        # Find paths from this specific predicate to all objects\n",
        "        object_paths = []\n",
        "        for obj in object_synsets:\n",
        "            path = find_predicate_to_object_path(pred, obj, max_depth)\n",
        "            if path:\n",
        "                object_paths.append(path)\n",
        "\n",
        "        # If we have both paths through this predicate, check if it's the best\n",
        "        if subject_paths and object_paths:\n",
        "            # find pairs of paths that don't intersect with eachother\n",
        "            #   i.e. burglar > break_in > attack > strike > shoot > strike > attack > woman\n",
        "            #   would not be allowed, since tautological statements are uninformative\n",
        "            valid_pairs = list()\n",
        "            for subj_path in subject_paths:\n",
        "              for obj_path in object_paths:\n",
        "                if len(set(subj_path).intersection(set(obj_path))) <= max_self_intersection:\n",
        "                  valid_pairs.append((\n",
        "                      subj_path,\n",
        "                      obj_path,\n",
        "                      # Calculate combined length (subtract 1 to avoid counting predicate twice)\n",
        "                      len(subj_path) + len(obj_path) - 1\n",
        "                  ))\n",
        "\n",
        "            if not valid_pairs:\n",
        "              print(f\"No valid pairs of subj, obj paths found for {pred.name()}\")\n",
        "              break\n",
        "\n",
        "            shortest_comb_path = min(valid_pairs, key=lambda x: x[2])\n",
        "\n",
        "            if shortest_comb_path[2] < best_combined_path_length:\n",
        "                best_combined_path_length = shortest_comb_path[2]\n",
        "                best_subject_path = shortest_comb_path[0]\n",
        "                best_object_path = shortest_comb_path[1]\n",
        "                best_predicate = pred\n",
        "\n",
        "    return best_subject_path, best_object_path, best_predicate\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Display Functions\n",
        "# ============================================================================\n",
        "\n",
        "def show_path(label, path):\n",
        "    \"\"\"Pretty print a path of synsets.\"\"\"\n",
        "    if path:\n",
        "        print(f\"{label}:\")\n",
        "        print(\" -> \".join(f\"{s.name()} ({s.definition()})\" for s in path))\n",
        "        print(f\"Path length: {len(path)}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"{label}: No path found\")\n",
        "        print()\n",
        "\n",
        "\n",
        "def show_connected_paths(subject_path, object_path, predicate):\n",
        "    \"\"\"Display the connected paths with their shared predicate.\"\"\"\n",
        "    if subject_path and object_path and predicate:\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"CONNECTED PATH through predicate: {predicate.name()}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        show_path(\"Subject -> Predicate path\", subject_path)\n",
        "        show_path(\"Predicate -> Object path\", object_path)\n",
        "\n",
        "        # Show the complete connected path\n",
        "        complete_path = subject_path + object_path[1:]  # Avoid duplicating the predicate\n",
        "        print(\"Complete connected path:\")\n",
        "        print(\" -> \".join(f\"{s.name()}\" for s in complete_path))\n",
        "        print(f\"Total path length: {len(complete_path)}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(\"No connected path found through any predicate synset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "bwShzrxtIhOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Find shortest connected paths\n",
        "subject_path, object_path, connecting_predicate = find_connected_shortest_paths(\n",
        "    \"man\",\n",
        "    \"shot\",\n",
        "    \"gun\",\n",
        "    max_depth=10,\n",
        "    max_self_intersection=5\n",
        ")\n",
        "\n",
        "# Display results\n",
        "show_connected_paths(subject_path, object_path, connecting_predicate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "valgVwQUh0SJ",
        "outputId": "0f64609a-441c-4bbe-9b73-045b1a55e14c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finding path from man.n.01 to shoot.v.01\n",
            "Found best matches for shoot.v.01: [(Synset('pit.v.01'), 0.22008430119603872), (Synset('fight.v.02'), 0.21860437840223312), (Synset('react.v.02'), 0.17705296725034714)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('grass_widower.n.01'), Synset('grass.v.01'), 0.24251817166805267), (Synset('bull.n.02'), Synset('shoot.v.02'), 0.16149000637233257), (Synset('dandy.n.01'), Synset('shoot.v.02'), 0.15834155026823282)] using strategy 3\n",
            "\n",
            "Finding path from grass_widower.n.01 to grass.v.01\n",
            "Found best matches for grass.v.01: [(Synset('divorce.v.02'), 0.036589786410331726), (Synset('disassociate.v.01'), -0.015623902902007103)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('man.n.01'), Synset('shoot.v.01'), 0.10713894478976727)] using strategy 3\n",
            "\n",
            "Finding path from bull.n.02 to shoot.v.02\n",
            "Most promising pairs for bidirectional exploration: [(Synset('man.n.01'), Synset('shoot.v.01'), 0.10713894478976727), (Synset('man.n.01'), Synset('flight.v.01'), 0.1053353026509285), (Synset('man.n.01'), Synset('kill.v.01'), 0.10454683471471071)] using strategy 3\n",
            "\n",
            "Finding path from shoot.v.01 to gun.n.01\n",
            "Found best matches for gun.n.01: [(Synset('projectile.n.01'), 0.2414008118212223), (Synset('missile.n.01'), 0.19290614314377308)] using strategy 1\n",
            "Found best matches for shoot.v.01: [(Synset('drop.v.08'), 0.21583104133605957), (Synset('exhaust.v.05'), 0.14289074391126633), (Synset('discharge.v.10'), 0.14058300852775574)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('gun_down.v.01'), Synset('set_gun.n.01'), 0.39754974842071533), (Synset('gun_down.v.01'), Synset('quaker_gun.n.01'), 0.39293190836906433), (Synset('gun_down.v.01'), Synset('minute_gun.n.01'), 0.361138254404068)] using strategy 3\n",
            "\n",
            "Finding path from gun_down.v.01 to set_gun.n.01\n",
            "Found best matches for gun_down.v.01: [(Synset('set.v.10'), 0.18283559381961823), (Synset('put.v.01'), 0.15346331894397736), (Synset('specify.v.02'), 0.11585843563079834)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('shoot.v.01'), Synset('gun.n.01'), 0.1749589554965496)] using strategy 3\n",
            "\n",
            "Finding path from shoot.v.02 to artillery.n.01\n",
            "Found best matches for artillery.n.01: [(Synset('projectile.n.01'), 0.3530864752829075), (Synset('missile.n.01'), 0.345676027238369)] using strategy 1\n",
            "Most promising pairs for bidirectional exploration: [(Synset('shoot.v.01'), Synset('field_artillery.n.01'), 0.19987302273511887), (Synset('shoot.v.01'), Synset('cannon.n.01'), 0.1659548357129097), (Synset('kill.v.01'), Synset('field_artillery.n.01'), 0.14842836186289787)] using strategy 3\n",
            "\n",
            "Finding path from kill.v.01 to field_artillery.n.01\n",
            "Found best matches for field_artillery.n.01: [(Synset('death.n.02'), 0.07083729468286037)] using strategy 1\n",
            "Found best matches for kill.v.01: [(Synset('use.v.02'), 0.18218636140227318), (Synset('use.v.03'), 0.1245257705450058), (Synset('use.v.04'), 0.12084228172898293)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('saber.v.02'), Synset('artillery.n.01'), 0.17074326798319817), (Synset('tomahawk.v.02'), Synset('artillery.n.01'), 0.1644812785089016), (Synset('murder.v.01'), Synset('artillery.n.01'), 0.15665879240259528)] using strategy 3\n",
            "\n",
            "Finding path from serviceman.n.01 to blast.v.07\n",
            "Found best matches for blast.v.07: [(Synset('serve.v.11'), 0.12850171700119972), (Synset('suffice.v.01'), 0.12208053842186928), (Synset('serve.v.06'), 0.11885928484844044)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('artilleryman.n.01'), Synset('fire.v.02'), 0.1845763511955738), (Synset('marine.n.01'), Synset('fire.v.02'), 0.16663408651947975), (Synset('air_force_officer.n.01'), Synset('fire.v.02'), 0.16460682824254036)] using strategy 3\n",
            "\n",
            "Finding path from artilleryman.n.01 to fire.v.02\n",
            "Most promising pairs for bidirectional exploration: [(Synset('serviceman.n.01'), Synset('fire.v.05'), 0.16554167866706848), (Synset('serviceman.n.01'), Synset('blast.v.07'), 0.14257041737437248), (Synset('serviceman.n.01'), Synset('loose_off.v.01'), 0.11548891291022301)] using strategy 3\n",
            "\n",
            "Finding path from blast.v.07 to gunman.n.02\n",
            "Found best matches for gunman.n.02: [(Synset('shot.n.05'), 0.3587132543325424), (Synset('shot.n.02'), 0.2527969740331173), (Synset('scene.n.04'), 0.23304107692092657)] using strategy 1\n",
            "Found best matches for blast.v.07: [(Synset('blast.v.07'), 0.5708323195576668), (Synset('shoot.v.08'), 0.39592608995735645), (Synset('shoot.v.17'), 0.39135297760367393)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('shoot.v.01'), Synset('shot.n.05'), 0.2762338671600446), (Synset('gun.v.01'), Synset('shot.n.05'), 0.2546169380657375), (Synset('open_fire.v.01'), Synset('shot.n.05'), 0.14190377108752728)] using strategy 3\n",
            "\n",
            "Finding path from gun.v.01 to shot.n.05\n",
            "Found best matches for shot.n.05: [(Synset('gunman.n.01'), 0.3787429127842188), (Synset('gunman.n.02'), 0.31602027267217636), (Synset('gun.n.01'), 0.25122627313248813)] using strategy 1\n",
            "Found best matches for gun.v.01: [(Synset('blast.v.07'), 0.23149843513965607), (Synset('shoot.v.08'), 0.21234652400016785), (Synset('tear.v.03'), 0.21102192625403404)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('machine_gun.v.01'), Synset('gunman.n.02'), 0.3460500240325928), (Synset('blast.v.07'), Synset('marksman.n.01'), 0.278120007365942), (Synset('blast.v.07'), Synset('gunman.n.02'), 0.24637753888964653)] using strategy 3\n",
            "\n",
            "Finding path from man.n.03 to film.v.01\n",
            "Found best matches for film.v.01: [(Synset('refer.v.02'), 0.2032759115099907), (Synset('mention.v.01'), 0.18359920755028725), (Synset('consult.v.02'), 0.12257418781518936)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('person.n.01'), Synset('record.v.01'), 0.1607471350580454), (Synset('person.n.01'), Synset('photograph.v.01'), 0.13748724572360516), (Synset('person.n.01'), Synset('reshoot.v.01'), 0.06465192325413227)] using strategy 3\n",
            "\n",
            "Finding path from person.n.01 to record.v.01\n",
            "Most promising pairs for bidirectional exploration: [(Synset('convert.n.01'), Synset('save.v.02'), 0.2155304653570056), (Synset('registrant.n.01'), Synset('register.v.01'), 0.212316969409585), (Synset('nude.n.03'), Synset('photograph.v.01'), 0.21170329209417105)] using strategy 3\n",
            "\n",
            "Finding path from convert.n.01 to save.v.02\n",
            "Found best matches for save.v.02: [(Synset('convert.v.09'), 0.26724155247211456), (Synset('convert.v.01'), 0.24795237183570862), (Synset('convert.v.02'), 0.22901592776179314)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('person.n.01'), Synset('record.v.01'), 0.1607471350580454), (Synset('person.n.01'), Synset('keep.v.03'), 0.1357090063393116), (Synset('proselyte.n.01'), Synset('conserve.v.03'), 0.12991733010858297)] using strategy 3\n",
            "\n",
            "Finding path from proselyte.n.01 to conserve.v.03\n",
            "Found best matches for conserve.v.03: [(Synset('change.v.06'), 0.23585936799645424), (Synset('commute.v.04'), 0.19039838667958975), (Synset('convert.v.09'), 0.16786613129079342)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('convert.n.01'), Synset('save.v.02'), 0.2155304653570056), (Synset('convert.n.01'), Synset('retrench.v.01'), 0.09671135246753693)] using strategy 3\n",
            "\n",
            "Finding path from registrant.n.01 to register.v.01\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'spacy.tokens.token.Token' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3332112220.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Find shortest connected paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m subject_path, object_path, connecting_predicate = find_connected_shortest_paths(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"man\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"shot\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4141195465.py\u001b[0m in \u001b[0;36mfind_connected_shortest_paths\u001b[0;34m(subject_word, predicate_word, object_word, max_depth, max_self_intersection)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0msubject_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msubj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubject_synsets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_subject_to_predicate_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m                 \u001b[0msubject_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4141195465.py\u001b[0m in \u001b[0;36mfind_subject_to_predicate_path\u001b[0;34m(subject_synset, predicate_synset, max_depth, visited, max_sample_size)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mvisited\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_synset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mvisited\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicate_synset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_subject_to_predicate_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubject_synset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpredicate_synset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4141195465.py\u001b[0m in \u001b[0;36mfind_subject_to_predicate_path\u001b[0;34m(subject_synset, predicate_synset, max_depth, visited, max_sample_size)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mvisited\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_synset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mvisited\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicate_synset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_subject_to_predicate_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubject_synset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpredicate_synset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4141195465.py\u001b[0m in \u001b[0;36mfind_subject_to_predicate_path\u001b[0;34m(subject_synset, predicate_synset, max_depth, visited, max_sample_size)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0msubjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubject_synset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactive_subjects\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;31m# of the remaining subjects, get the most similar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_most_similar_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_subjects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_sample_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_synset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Found best matches for {subject_synset.name()}: {top_k} using strategy 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4141195465.py\u001b[0m in \u001b[0;36mget_most_similar_synsets\u001b[0;34m(candidates, target_synset, k)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m#   where the subcategory != 'entity.n.01' or a similar top-level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         synsets = [\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_hypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowest_common_hypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_synset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         ]\n",
            "\u001b[0;31mTypeError\u001b[0m: 'spacy.tokens.token.Token' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJjxiYEy9smM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
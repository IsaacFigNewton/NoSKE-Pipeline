{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n7NZYyR-Pk8b",
        "dSlaJDUEPnfT",
        "H6Ahax9MciBc"
      ],
      "authorship_tag": "ABX9TyNbQQXT3zL/Z3Hu0/CLAQWx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/SMIED/blob/main/BFS_Semantic_Decomposition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "2J9B-9RI5lRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "n7NZYyR-Pk8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "GReqNuQt7wvb",
        "outputId": "2dcb7e97-beeb-4b2b-c04a-276a7c6b80a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "a8437b8478d848c39727a9bc193fdc21"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import, config stuff"
      ],
      "metadata": {
        "id": "dSlaJDUEPnfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Semantic decomposition of (\"cat\", \"eats\", \"mouse\") using WordNet + spaCy + depth-limited GBFS.\n",
        "- Uses spaCy to parse verb synset glosses and detect subject/object dependencies.\n",
        "- If both subject and object tokens are present, branches directly toward original triple synsets.\n",
        "- Otherwise falls back to WordNet relations.\n",
        "\"\"\"\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from heapq import heappush, heappop\n",
        "import numpy as np\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gensim.downloader as api\n",
        "from collections import deque\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "import spacy"
      ],
      "metadata": {
        "id": "Jb_gtRCvh-DF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "# Load spaCy English model for dependency parsing\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj4iZlAfh_t8",
        "outputId": "7c05f9b7-2206-4db0-8e1b-43bd1d7dc3c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5TGC7I88EvX",
        "outputId": "4a9a81d0-ea01-4cf2-edde-b20a110926fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpers"
      ],
      "metadata": {
        "id": "H6Ahax9MciBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_neighbors(synset: wn.synset):\n",
        "    \"\"\"Get all neighbors of a synset based on its POS.\"\"\"\n",
        "    neighbors = []\n",
        "\n",
        "    # Add hypernyms and hyponyms\n",
        "    neighbors.extend(synset.hypernyms())\n",
        "    neighbors.extend(synset.hyponyms())\n",
        "\n",
        "    # Add POS-specific neighbors\n",
        "    if synset.pos() == 'n':\n",
        "        neighbors.extend(get_noun_neighbors(synset))\n",
        "    else:\n",
        "        neighbors.extend(get_verb_neighbors(synset))\n",
        "\n",
        "    return neighbors\n",
        "\n",
        "\n",
        "def get_noun_neighbors(syn: wn.synset):\n",
        "    \"\"\"Get neighbors for a noun synset.\"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.part_meronyms())\n",
        "    nbrs.update(syn.substance_meronyms())\n",
        "    nbrs.update(syn.member_meronyms())\n",
        "    nbrs.update(syn.part_holonyms())\n",
        "    nbrs.update(syn.substance_holonyms())\n",
        "    nbrs.update(syn.member_holonyms())\n",
        "    return list(nbrs)\n",
        "\n",
        "\n",
        "def get_verb_neighbors(syn: wn.synset):\n",
        "    \"\"\"Get neighbors for a verb synset.\"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.entailments())\n",
        "    nbrs.update(syn.causes())\n",
        "    nbrs.update(syn.also_sees())\n",
        "    nbrs.update(syn.verb_groups())\n",
        "    return list(nbrs)"
      ],
      "metadata": {
        "id": "xoGoYrlVcjiG"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding similarities"
      ],
      "metadata": {
        "id": "zJVKWnjy5qNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synset_embedding_centroid(synset:wn.synset, model=word2vec_model):\n",
        "    \"\"\"\n",
        "    Get the centroid (mean) of Word2Vec embeddings for all lemmas in a synset.\n",
        "\n",
        "    Args:\n",
        "        synset: WordNet Synset object (e.g., 'dog.n.01')\n",
        "        model: Loaded Word2Vec model\n",
        "\n",
        "    Returns:\n",
        "        numpy array representing the centroid, or None if no lemmas found\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get all lemma names from the synset\n",
        "        lemmas = [lemma.name().lower().replace('_', ' ') for lemma in synset.lemmas()]\n",
        "        # Collect embeddings for lemmas that exist in the model\n",
        "        embeddings = []\n",
        "        found_lemmas = []\n",
        "\n",
        "        for lemma in lemmas:\n",
        "            # Try the lemma as-is first\n",
        "            if lemma in model:\n",
        "                embeddings.append(model[lemma])\n",
        "                found_lemmas.append(lemma)\n",
        "            # Try with underscores replaced by spaces (for multi-word terms)\n",
        "            elif lemma.replace(' ', '_') in model:\n",
        "                embeddings.append(model[lemma.replace(' ', '_')])\n",
        "                found_lemmas.append(lemma)\n",
        "            # Try individual words if it's a multi-word term\n",
        "            elif ' ' in lemma:\n",
        "                words = lemma.split()\n",
        "                word_embeddings = []\n",
        "                for word in words:\n",
        "                    if word in model:\n",
        "                        word_embeddings.append(model[word])\n",
        "                if word_embeddings:\n",
        "                    # Average the embeddings of individual words\n",
        "                    embeddings.append(np.mean(word_embeddings, axis=0))\n",
        "                    found_lemmas.append(lemma)\n",
        "\n",
        "        if not embeddings:\n",
        "            print(f\"Warning: No lemmas from {synset.name()} found in Word2Vec model\")\n",
        "            print(f\"  Attempted lemmas: {lemmas}\")\n",
        "            return None\n",
        "\n",
        "        # print(f\"Synset {synset.name()}: Found {len(found_lemmas)}/{len(lemmas)} lemmas in model\")\n",
        "        # print(f\"  Found: {found_lemmas}\")\n",
        "\n",
        "        # Return the mean of all embeddings\n",
        "        return np.mean(embeddings, axis=0)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing synset {synset.name()}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def emb_asymmetric_lex_rels(synset: wn.synset, model):\n",
        "    \"\"\"\n",
        "    match the asymmetric lexical relations between two synsets.\n",
        "    eg: meronyms-holonyms, hypernyms-hyponyms, etc.\n",
        "\n",
        "    Args:\n",
        "        synset: WordNet Synset object (e.g., 'dog.n.01')\n",
        "\n",
        "    Returns:\n",
        "        Dict of embeddings for asymmetric lexical relations of the synset.\n",
        "    \"\"\"\n",
        "    def _rel_centroid(get_attr):\n",
        "      try:\n",
        "        return np.mean([\n",
        "            get_synset_embedding_centroid(s)\n",
        "            for s in get_attr(synset)\n",
        "        ])\n",
        "      except Exception as e:\n",
        "        return np.ndarray([])\n",
        "\n",
        "    return {\n",
        "        \"meronyms\": {\n",
        "            \"part_meronyms\": _rel_centroid(lambda x: x.part_meronyms()),\n",
        "            \"substance_meronyms\": _rel_centroid(lambda x: x.substance_meronyms()),\n",
        "            \"member_meronyms\": _rel_centroid(lambda x: x.member_meronyms()),\n",
        "        },\n",
        "        \"holonyms\": {\n",
        "            \"part_holonyms\": _rel_centroid(lambda x: x.part_holonyms()),\n",
        "            \"substance_holonyms\": _rel_centroid(lambda x: x.substance_holonyms()),\n",
        "            \"member_holonyms\": _rel_centroid(lambda x: x.member_holonyms()),\n",
        "        },\n",
        "        \"hypernyms\": _rel_centroid(lambda x: x.hypernyms()),\n",
        "        \"hyponyms\": _rel_centroid(lambda x: x.hyponyms()),\n",
        "        \"entailments\": _rel_centroid(lambda x: x.entailments()),\n",
        "        \"causes\": _rel_centroid(lambda x: x.causes()),\n",
        "        \"also_sees\": _rel_centroid(lambda x: x.also_sees()),\n",
        "        \"verb_groups\": _rel_centroid(lambda x: x.verb_groups()),\n",
        "    }\n",
        "\n",
        "\n",
        "pairing_maps = [\n",
        "    {\n",
        "        \"meronyms\": {\n",
        "            \"part_meronyms\": \"part_holonym\",\n",
        "            \"substance_meronyms\": \"substance_holonym\",\n",
        "            \"member_meronyms\": \"member_holonym\",\n",
        "        },\n",
        "        \"hyponyms\": \"hypernym\",\n",
        "        # \"entailments\": \"entailments\",\n",
        "        # \"causes\": \"causes\",\n",
        "        # \"also_sees\": \"also_sees\",\n",
        "        # \"verb_groups\": \"verb_groups\",\n",
        "    },\n",
        "    {\n",
        "        \"holonyms\": {\n",
        "            \"part_holonyms\": \"part_meronym\",\n",
        "            \"substance_holonyms\": \"substance_meronym\",\n",
        "            \"member_holonyms\": \"member_meronym\",\n",
        "        },\n",
        "        \"hypernyms\": \"hyponym\",\n",
        "        # \"entailments\": \"entailments\",\n",
        "        # \"causes\": \"causes\",\n",
        "        # \"also_sees\": \"also_sees\",\n",
        "        # \"verb_groups\": \"verb_groups\",\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "def get_asymmetric_emb_similarity(emb1, emb2, model=word2vec_model):\n",
        "\n",
        "    def get_asym_sim(i, s1, s2, k, l=None):\n",
        "        if l is None:\n",
        "            return cosine_similarity(s1[k], s2[pairing_maps[i][k]])\n",
        "        else:\n",
        "            return cosine_similarity(s1[k][l], s2[pairing_maps[i][k][l]])\n",
        "\n",
        "    # check similarity of asymmetric relations\n",
        "    #   i.e. similarity of synset1's merynyms to synset2's holonyms\n",
        "    # need to aggregate similarities seperately to avoid destructive interference\n",
        "    #   if everything was aggregated together, similarity between hypernyms\n",
        "    #   might be cancelled by dissimilarity between hypernyms and hyponyms\n",
        "\n",
        "    asymm_rel_centroid_similarities = list()\n",
        "    # get centroid for if emb1 hyponyms/meronyms are ~ emb2 hypernyms/holonyms\n",
        "    for k, v in pairing_maps[0].items():\n",
        "        if isinstance(v, str):\n",
        "          asymm_rel_centroid_similarities.append(get_asym_sim(0, emb1, emb2, k))\n",
        "        else:\n",
        "          for l in emb1[k].keys():\n",
        "            asymm_rel_centroid_similarities.append(get_asym_sim(0, emb1, emb2, k, l))\n",
        "    asymm_sim_high_low = np.mean(asymm_rel_centroid_similarities)\n",
        "\n",
        "    asymm_centroid_sims = list()\n",
        "    # get centroid for if emb1 hypernyms/holonyms are ~ emb2 hyponyms/meronyms\n",
        "    for k, v in pairing_maps[1].items():\n",
        "        if isinstance(v, str):\n",
        "          asymm_rel_centroid_similarities.append(get_asym_sim(1, emb1, emb2, k))\n",
        "        else:\n",
        "          for l in emb1[k].keys():\n",
        "            asymm_rel_centroid_similarities.append(get_asym_sim(1, emb1, emb2, k, l))\n",
        "    asymm_sim_low_high = np.mean(asymm_rel_centroid_similarities)\n",
        "\n",
        "    # it doesn't matter which one is contains/includes/etc. the other,\n",
        "    #   as long as they're closer than antonyms or unrelated terms\n",
        "    # i.e. a good hyponym-hypernym pair is just as important\n",
        "    #   as a good hypernym-hyponym pair\n",
        "    return max(asymm_sim_high_low, asymm_sim_low_high)\n",
        "\n",
        "\n",
        "def get_synset_relatedness(\n",
        "      synset1:wn.synset,\n",
        "      synset2:wn.synset,\n",
        "      model=word2vec_model\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Compute cosine distance between centroids of two synsets.\n",
        "\n",
        "    Args:\n",
        "        synset1: WordNet Synset object (e.g., 'dog.n.01')\n",
        "        synset2: WordNet Synset object (e.g., 'cat.n.01')\n",
        "        model: Word2Vec model (if None, will load default)\n",
        "\n",
        "    Returns:\n",
        "        Float cosine distance between centroids (0 = identical, 1 = orthogonal, 2 = opposite)\n",
        "    \"\"\"\n",
        "    # Get centroids for both synsets\n",
        "    synset1_centroid = get_synset_embedding_centroid(synset1, model)\n",
        "    synset1_asym_rel_embs = emb_asymmetric_lex_rels(synset1, model)\n",
        "    synset2_centroid = get_synset_embedding_centroid(synset2, model)\n",
        "    synset2_asym_rel_embs = emb_asymmetric_lex_rels(synset2, model)\n",
        "\n",
        "    # Check if both centroids were successfully computed\n",
        "    if synset1_centroid is None or synset2_centroid is None:\n",
        "        raise ValueError(\"Could not compute centroids for one or both synsets\")\n",
        "\n",
        "    # Get cosine distance between asymmetric lexical relation pairings\n",
        "    asymm_lex_rel_sim = get_asymmetric_emb_similarity(synset1_asym_rel_embs, synset2_asym_rel_embs, model)\n",
        "\n",
        "    # Compute cosine distance between centroids\n",
        "    centroid_similarity = cosine_similarity([synset1_centroid], [synset2_centroid])\n",
        "\n",
        "    # idk maybe mean isn't best\n",
        "    return np.mean([centroid_similarity, asymm_lex_rel_sim])\n",
        "\n",
        "\n",
        "def get_k_closest_synset_pairs(\n",
        "    start_synsets: List[wn.synset],\n",
        "    end_synsets: List[wn.synset],\n",
        "    model=word2vec_model\n",
        "  ):\n",
        "  # pair opposing pairs\n",
        "  pairs = list()\n",
        "  for s1 in start_synsets:\n",
        "    for s2 in end_synsets:\n",
        "      pairs.append((s1, s2, get_synset_relatedness(s1, s2)))\n",
        "\n",
        "  # get the top k most semantically similar pairings\n",
        "  return sorted(pairs, key=lambda x: x[2], reverse=True)[:k]"
      ],
      "metadata": {
        "id": "25u1o6wq5req"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pathing"
      ],
      "metadata": {
        "id": "X1WdFnm55oTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "GZ7mH1XuhtBN"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Core Path Finding Functions\n",
        "# ============================================================================\n",
        "\n",
        "def path_syn_to_syn(start_synset, end_synset, max_depth=6):\n",
        "    \"\"\"\n",
        "    Find shortest path between synsets of the same POS using bidirectional BFS.\n",
        "    Returns a list of synsets forming the path, or None if no path found.\n",
        "    \"\"\"\n",
        "\n",
        "    if not (start_synset.pos() == end_synset.pos() and start_synset.pos() in {'n', 'v'}):\n",
        "      raise ValueError(f\"{start_synset.name()} POS tag != {end_synset.name()}. Synsets must be of the same POS (noun or verb).\")\n",
        "\n",
        "    # Handle the trivial case where start and end are the same\n",
        "    if start_synset.name() == end_synset.name():\n",
        "        return [start_synset]\n",
        "\n",
        "    # Initialize two search frontiers\n",
        "    forward_queue = deque([(start_synset, 0)])\n",
        "    forward_visited = {start_synset.name(): [start_synset]}\n",
        "\n",
        "    backward_queue = deque([(end_synset, 0)])\n",
        "    backward_visited = {end_synset.name(): [end_synset]}\n",
        "\n",
        "    def expand_frontier(queue, visited_from_this_side, visited_from_other_side, is_forward):\n",
        "        \"\"\"Expand one step of the search frontier.\"\"\"\n",
        "        if not queue:\n",
        "            return None\n",
        "\n",
        "        curr_synset, depth = queue.popleft()\n",
        "\n",
        "        if depth >= (max_depth + 1) // 2:\n",
        "            return None\n",
        "\n",
        "        path_to_current = visited_from_this_side[curr_synset.name()]\n",
        "\n",
        "        for neighbor in get_all_neighbors(curr_synset):\n",
        "            neighbor_name = neighbor.name()\n",
        "\n",
        "            if neighbor_name in visited_from_this_side:\n",
        "                continue\n",
        "\n",
        "            if is_forward:\n",
        "                new_path = path_to_current + [neighbor]\n",
        "            else:\n",
        "                new_path = [neighbor] + path_to_current\n",
        "\n",
        "            if neighbor_name in visited_from_other_side:\n",
        "                other_path = visited_from_other_side[neighbor_name]\n",
        "\n",
        "                if is_forward:\n",
        "                    full_path = path_to_current + other_path\n",
        "                else:\n",
        "                    full_path = other_path + path_to_current\n",
        "\n",
        "                return full_path\n",
        "\n",
        "            visited_from_this_side[neighbor_name] = new_path\n",
        "            queue.append((neighbor, depth + 1))\n",
        "\n",
        "        return None\n",
        "\n",
        "    # Alternate between forward and backward search\n",
        "    while forward_queue or backward_queue:\n",
        "        if forward_queue:\n",
        "            result = expand_frontier(forward_queue, forward_visited, backward_visited, True)\n",
        "            if result:\n",
        "                return result\n",
        "\n",
        "        if backward_queue:\n",
        "            result = expand_frontier(backward_queue, backward_visited, forward_visited, False)\n",
        "            if result:\n",
        "                return result\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Gloss Analysis Helper Functions\n",
        "# ============================================================================\n",
        "\n",
        "def extract_subjects_from_gloss(gloss_doc):\n",
        "    \"\"\"Extract subject tokens from a parsed gloss.\"\"\"\n",
        "    subjects = []\n",
        "\n",
        "    # Direct subjects\n",
        "    subjects.extend([tok for tok in gloss_doc if tok.dep_ == \"nsubj\"])\n",
        "\n",
        "    # Passive subjects (which are actually objects semantically)\n",
        "    # Skip these for actor identification\n",
        "    passive_subjects = [tok for tok in gloss_doc if tok.dep_ == \"nsubjpass\"]\n",
        "\n",
        "    # Filter out passive subjects from the main list\n",
        "    subjects = [s for s in subjects if s not in passive_subjects]\n",
        "\n",
        "    return subjects, passive_subjects\n",
        "\n",
        "\n",
        "def extract_objects_from_gloss(gloss_doc):\n",
        "    \"\"\"Extract various types of object tokens from a parsed gloss.\"\"\"\n",
        "    objs = []\n",
        "\n",
        "    # Indirect objects\n",
        "    iobjs = [tok for tok in gloss_doc if tok.dep_ == \"iobj\"]\n",
        "    objs.extend(iobjs)\n",
        "\n",
        "    # Direct objects\n",
        "    # Only include if there were no indirect objects,\n",
        "    #   crude, but good for MVP\n",
        "    if not iobjs:\n",
        "        objs.extend([tok for tok in gloss_doc if tok.dep_ == \"dobj\"])\n",
        "\n",
        "    # Prepositional objects\n",
        "    objs.extend([tok for tok in gloss_doc if tok.dep_ == \"pobj\"])\n",
        "\n",
        "    # General objects\n",
        "    objs.extend([tok for tok in gloss_doc if tok.dep_ == \"obj\"])\n",
        "\n",
        "    # Check for noun chunks related to root verb\n",
        "    root_verbs = [tok for tok in gloss_doc if tok.dep_ == \"ROOT\" and tok.pos_ == \"VERB\"]\n",
        "    if root_verbs and not objs:\n",
        "        for noun_chunk in gloss_doc.noun_chunks:\n",
        "            if any(token.head == root_verbs[0] for token in noun_chunk):\n",
        "                objs.append(noun_chunk.root)\n",
        "\n",
        "    return objs\n",
        "\n",
        "\n",
        "def extract_verbs_from_gloss(gloss_doc, include_passive=False):\n",
        "    \"\"\"Extract verb tokens from a parsed gloss.\"\"\"\n",
        "    verbs = [tok for tok in gloss_doc if tok.pos_ == \"VERB\"]\n",
        "\n",
        "    if include_passive:\n",
        "        # Past participles used as adjectives or in relative clauses\n",
        "        passive_verbs = [tok for tok in gloss_doc if\n",
        "                        tok.tag_ in [\"VBN\", \"VBD\"] and\n",
        "                        tok.dep_ in [\"acl\", \"relcl\", \"amod\"]]\n",
        "        verbs.extend(passive_verbs)\n",
        "\n",
        "    return verbs\n",
        "\n",
        "\n",
        "def find_instrumental_verbs(gloss_doc):\n",
        "    \"\"\"Find verbs associated with instrumental use (e.g., 'used for').\"\"\"\n",
        "    instrumental_verbs = []\n",
        "\n",
        "    if \"used\" in gloss_doc.text.lower():\n",
        "        for i, token in enumerate(gloss_doc):\n",
        "            if token.text.lower() == \"used\":\n",
        "                # Check tokens after \"used\"\n",
        "                for j in range(i+1, min(i+4, len(gloss_doc))):\n",
        "                    if gloss_doc[j].pos_ == \"VERB\":\n",
        "                        instrumental_verbs.append(gloss_doc[j])\n",
        "\n",
        "    return instrumental_verbs\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Cross-POS Path Finding Functions\n",
        "# ============================================================================\n",
        "def get_most_similar_synsets(\n",
        "      candidates: List[List[wn.synset]],\n",
        "      target_synset: wn.synset,\n",
        "      k=3\n",
        "    ) -> List[Tuple[str, float]]:\n",
        "    \"\"\"\n",
        "    Given a list of candidate tokens and a target synset,\n",
        "    return the synset (name, relatedness) most similar to the target.\n",
        "    \"\"\"\n",
        "    for synsets in candidates:\n",
        "        # filter to subjects based on whether they reside in the same sub-category\n",
        "        #   where the subcategory != 'entity.n.01' or a similar top-level term\n",
        "        synsets = [\n",
        "            s for s in synsets\n",
        "            if s.root_hypernyms() != s.lowest_common_hypernyms(target_synset)\n",
        "        ]\n",
        "        if synsets:\n",
        "            # if the target is a verb,\n",
        "            #   filter out any synsets with no lemma frames matching the target\n",
        "            #   frame patterns: (Somebody [v] something), (Somebody [v]), ...\n",
        "            if target_synset.pos() == 'v':\n",
        "                synsets = [\n",
        "                    s for s in synsets\n",
        "                    if any(\n",
        "                        frame in s.frame_ids()\n",
        "                        for frame in target_synset.frame_ids()\n",
        "                    )\n",
        "                ]\n",
        "            return sorted(\n",
        "                [\n",
        "                    (s, get_synset_relatedness(s, target_synset))\n",
        "                    for s in synsets\n",
        "                ],\n",
        "                key=lambda x: x[1],\n",
        "                reverse=True\n",
        "            )[:k]\n",
        "    return None\n",
        "\n",
        "\n",
        "def find_subject_to_predicate_path(\n",
        "      subject_synset: wn.synset,\n",
        "      predicate_synset: wn.synset,\n",
        "      max_depth=6\n",
        "    ):\n",
        "    \"\"\"Find path from subject (noun) to predicate (verb).\"\"\"\n",
        "    paths = []\n",
        "    print()\n",
        "    print(f\"Finding path from {subject_synset.name()} to {predicate_synset.name()}\")\n",
        "\n",
        "    # Strategy 1: Look for active subjects in verb's gloss\n",
        "    pred_gloss_doc = nlp(predicate_synset.definition())\n",
        "    # passive subjects are semantically equivalent to objects\n",
        "    active_subjects, _ = extract_subjects_from_gloss(pred_gloss_doc)\n",
        "    # convert spacy tokens to lists of synsets\n",
        "    subjects = [wn.synsets(s.text, pos=subject_synset.pos()) for s in active_subjects]\n",
        "    # of the remaining subjects, get the most similar\n",
        "    top_k = get_most_similar_synsets(active_subjects[:3], subject_synset)\n",
        "    if top_k:\n",
        "      print(f\"Found best matches for {subject_synset.name()}: {top_k} using strategy 1\")\n",
        "      for matched_synset, _ in top_k:\n",
        "        path = path_syn_to_syn(subject_synset, matched_synset, max_depth) + [predicate_synset]\n",
        "        if path:\n",
        "            paths.append(path)\n",
        "\n",
        "    # Strategy 2: Look for verbs in the noun's gloss\n",
        "    subj_gloss_doc = nlp(subject_synset.definition())\n",
        "    verbs = extract_verbs_from_gloss(subj_gloss_doc, include_passive=False)\n",
        "    # convert spacy tokens to lists of synsets\n",
        "    verbs = [wn.synsets(v.text, pos=predicate_synset.pos()) for v in verbs]\n",
        "    # of the remaining subjects, get the most similar\n",
        "    top_k = get_most_similar_synsets(verbs[:3], predicate_synset)\n",
        "    if top_k:\n",
        "      print(f\"Found best matches for {predicate_synset.name()}: {top_k} using strategy 2\")\n",
        "      for matched_synset, _ in top_k:\n",
        "        path = [subject_synset] + path_syn_to_syn(matched_synset, predicate_synset, max_depth)\n",
        "        if path:\n",
        "            paths.append(path)\n",
        "\n",
        "    # Strategy 3: Explore the 3 most promising pairs of neighbors\n",
        "    subject_neighbors = get_all_neighbors(subject_synset)\n",
        "    predicate_neighbors = get_all_neighbors(predicate_synset)\n",
        "    top_k = get_k_closest_synset_pairs(subject_neighbors, predicate_neighbors)\n",
        "    if top_k:\n",
        "      print(f\"Most promising pairs for bidirectional exploration: {top_k} using strategy 3\")\n",
        "      for s, p, _ in top_k:\n",
        "        path = [subject_synset] + path_syn_to_syn(s, p, max_depth) + [predicate_synset]\n",
        "        if path:\n",
        "            paths.append(path)\n",
        "\n",
        "\n",
        "    # Return shortest path if any found\n",
        "    return min(paths, key=len) if paths else None\n",
        "\n",
        "\n",
        "def find_predicate_to_object_path(\n",
        "      predicate_synset: wn.synset,\n",
        "      object_synset: wn.synset,\n",
        "      max_depth=6\n",
        "    ):\n",
        "    \"\"\"Find path from predicate (verb) to object (noun).\"\"\"\n",
        "    paths = []\n",
        "    print()\n",
        "    print(f\"Finding path from {predicate_synset.name()} to {object_synset.name()}\")\n",
        "\n",
        "    # === Strategy 1: Objects in predicate gloss (incl. passive subjects) ===\n",
        "    pred_gloss_doc = nlp(predicate_synset.definition())\n",
        "    objects = extract_objects_from_gloss(pred_gloss_doc)\n",
        "    _, passive_subjects = extract_subjects_from_gloss(pred_gloss_doc)\n",
        "    objects.extend(passive_subjects)\n",
        "    # convert spacy tokens to lists of synsets\n",
        "    objects = [wn.synsets(o.text, pos=object_synset.pos()) for o in objects]\n",
        "    top_k = get_most_similar_synsets(objects[:3], object_synset)\n",
        "    if top_k:\n",
        "      print(f\"Found best matches for {object_synset.name()}: {top_k} using strategy 1\")\n",
        "      for matched_synset, _ in top_k:\n",
        "        path = [predicate_synset] + path_syn_to_syn(matched_synset, object_synset, max_depth)\n",
        "        if path:\n",
        "            paths.append(path)\n",
        "\n",
        "    # === Strategy 2: Verbs in object's gloss ===\n",
        "    obj_gloss_doc = nlp(object_synset.definition())\n",
        "    verbs = extract_verbs_from_gloss(obj_gloss_doc, include_passive=True)\n",
        "    # Use instrumental verbs in object's gloss as backup\n",
        "    verbs.extend(find_instrumental_verbs(obj_gloss_doc))\n",
        "    # convert spacy tokens to lists of synsets\n",
        "    verbs = [wn.synsets(v.text, pos=predicate_synset.pos()) for v in verbs]\n",
        "    top_k = get_most_similar_synsets(verbs[:3], predicate_synset)\n",
        "    if top_k:\n",
        "      print(f\"Found best matches for {predicate_synset.name()}: {top_k} using strategy 2\")\n",
        "      for matched_synset, _ in top_k:\n",
        "        path = path_syn_to_syn(predicate_synset, matched_synset, max_depth) + [object_synset]\n",
        "        if path:\n",
        "            paths.append(path)\n",
        "\n",
        "    # Strategy 3: Explore the 3 most promising neighbors\n",
        "    predicate_neighbors = get_all_neighbors(predicate_synset)\n",
        "    object_neighbors = get_all_neighbors(object_synset)\n",
        "    top_k = get_k_closest_synset_pairs(predicate_neighbors, object_neighbors)\n",
        "    if top_k:\n",
        "      print(f\"Most promising pairs for bidirectional exploration: {top_k} using strategy 3\")\n",
        "      for p, o, _ in top_k:\n",
        "        path = [predicate_synset] + path_syn_to_syn(p, o, max_depth) + [object_synset]\n",
        "        if path:\n",
        "            paths.append(path)\n",
        "\n",
        "\n",
        "    # Return shortest path if any found\n",
        "    return min(paths, key=len) if paths else None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Main Connected Path Finding Function\n",
        "# ============================================================================\n",
        "\n",
        "def find_connected_shortest_paths(subject_word, predicate_word, object_word, max_depth=10):\n",
        "    \"\"\"\n",
        "    Find shortest connected paths from subject through predicate to object.\n",
        "    Ensures that the same predicate synset connects both paths.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get synsets for each word\n",
        "    subject_synsets = wn.synsets(subject_word, pos=wn.NOUN)\n",
        "    predicate_synsets = wn.synsets(predicate_word, pos=wn.VERB)\n",
        "    object_synsets = wn.synsets(object_word, pos=wn.NOUN)\n",
        "\n",
        "    best_combined_path_length = float('inf')\n",
        "    best_subject_path = None\n",
        "    best_object_path = None\n",
        "    best_predicate = None\n",
        "\n",
        "    # Try each predicate synset as the connector\n",
        "    for pred in predicate_synsets:\n",
        "        # Find paths from all subjects to this specific predicate\n",
        "        subject_paths = []\n",
        "        for subj in subject_synsets:\n",
        "            path = find_subject_to_predicate_path(subj, pred, max_depth)\n",
        "            if path:\n",
        "                subject_paths.append(path)\n",
        "\n",
        "        # Find paths from this specific predicate to all objects\n",
        "        object_paths = []\n",
        "        for obj in object_synsets:\n",
        "            path = find_predicate_to_object_path(pred, obj, max_depth)\n",
        "            if path:\n",
        "                object_paths.append(path)\n",
        "\n",
        "        # If we have both paths through this predicate, check if it's the best\n",
        "        if subject_paths and object_paths:\n",
        "            shortest_subj_path = min(subject_paths, key=len)\n",
        "            shortest_obj_path = min(object_paths, key=len)\n",
        "\n",
        "            # Calculate combined length (subtract 1 to avoid counting predicate twice)\n",
        "            combined_length = len(shortest_subj_path) + len(shortest_obj_path) - 1\n",
        "\n",
        "            if combined_length < best_combined_path_length:\n",
        "                best_combined_path_length = combined_length\n",
        "                best_subject_path = shortest_subj_path\n",
        "                best_object_path = shortest_obj_path\n",
        "                best_predicate = pred\n",
        "\n",
        "    return best_subject_path, best_object_path, best_predicate\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Display Functions\n",
        "# ============================================================================\n",
        "\n",
        "def show_path(label, path):\n",
        "    \"\"\"Pretty print a path of synsets.\"\"\"\n",
        "    if path:\n",
        "        print(f\"{label}:\")\n",
        "        print(\" -> \".join(f\"{s.name()} ({s.definition()})\" for s in path))\n",
        "        print(f\"Path length: {len(path)}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"{label}: No path found\")\n",
        "        print()\n",
        "\n",
        "\n",
        "def show_connected_paths(subject_path, object_path, predicate):\n",
        "    \"\"\"Display the connected paths with their shared predicate.\"\"\"\n",
        "    if subject_path and object_path and predicate:\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"CONNECTED PATH through predicate: {predicate.name()}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        show_path(\"Subject -> Predicate path\", subject_path)\n",
        "        show_path(\"Predicate -> Object path\", object_path)\n",
        "\n",
        "        # Show the complete connected path\n",
        "        complete_path = subject_path + object_path[1:]  # Avoid duplicating the predicate\n",
        "        print(\"Complete connected path:\")\n",
        "        print(\" -> \".join(f\"{s.name()}\" for s in complete_path))\n",
        "        print(f\"Total path length: {len(complete_path)}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(\"No connected path found through any predicate synset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "bwShzrxtIhOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Find shortest connected paths\n",
        "subject_path, object_path, connecting_predicate = find_connected_shortest_paths(\n",
        "    \"burglar\", \"shoot\", \"woman\", max_depth=10\n",
        ")\n",
        "\n",
        "# Display results\n",
        "show_connected_paths(subject_path, object_path, connecting_predicate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "valgVwQUh0SJ",
        "outputId": "de787100-2ea5-42b4-bf15-b516e4be0564"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finding path from burglar.n.01 to shoot.v.01\n",
            "Warning: No lemmas from re-enter.v.01 found in Word2Vec model\n",
            "  Attempted lemmas: ['re-enter']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'part_holonym'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3991999513.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Find shortest connected paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m subject_path, object_path, connecting_predicate = find_connected_shortest_paths(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"burglar\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shoot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"woman\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n",
            "\u001b[0;32m/tmp/ipython-input-2355607348.py\u001b[0m in \u001b[0;36mfind_connected_shortest_paths\u001b[0;34m(subject_word, predicate_word, object_word, max_depth)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0msubject_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msubj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubject_synsets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_subject_to_predicate_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0msubject_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2355607348.py\u001b[0m in \u001b[0;36mfind_subject_to_predicate_path\u001b[0;34m(subject_synset, predicate_synset, max_depth)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mverbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredicate_synset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mverbs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;31m# of the remaining subjects, get the most similar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_most_similar_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicate_synset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Found best matches for {predicate_synset.name()}: {top_k} using strategy 2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2355607348.py\u001b[0m in \u001b[0;36mget_most_similar_synsets\u001b[0;34m(candidates, target_synset, k)\u001b[0m\n\u001b[1;32m    189\u001b[0m             return sorted(\n\u001b[1;32m    190\u001b[0m                 [\n\u001b[0;32m--> 191\u001b[0;31m                     \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_synset_relatedness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_synset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 ],\n",
            "\u001b[0;32m/tmp/ipython-input-2049052510.py\u001b[0m in \u001b[0;36mget_synset_relatedness\u001b[0;34m(synset1, synset2, model)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# Get cosine distance between asymmetric lexical relation pairings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0masymm_lex_rel_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_asymmetric_emb_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset1_asym_rel_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset2_asym_rel_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;31m# Compute cosine distance between centroids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2049052510.py\u001b[0m in \u001b[0;36mget_asymmetric_emb_similarity\u001b[0;34m(emb1, emb2, model)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memb1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0masymm_rel_centroid_similarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_asym_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0masymm_sim_high_low\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masymm_rel_centroid_similarities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2049052510.py\u001b[0m in \u001b[0;36mget_asym_sim\u001b[0;34m(i, s1, s2, k, l)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpairing_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpairing_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# check similarity of asymmetric relations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'part_holonym'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJjxiYEy9smM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n7NZYyR-Pk8b",
        "dSlaJDUEPnfT",
        "H6Ahax9MciBc"
      ],
      "authorship_tag": "ABX9TyPNvRRSjT4kbx7WK2d4BXNW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/SMIED/blob/main/BFS_Semantic_Decomposition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "2J9B-9RI5lRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "n7NZYyR-Pk8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "GReqNuQt7wvb",
        "outputId": "2dcb7e97-beeb-4b2b-c04a-276a7c6b80a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "a8437b8478d848c39727a9bc193fdc21"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import, config stuff"
      ],
      "metadata": {
        "id": "dSlaJDUEPnfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Semantic decomposition of (\"cat\", \"eats\", \"mouse\") using WordNet + spaCy + depth-limited GBFS.\n",
        "- Uses spaCy to parse verb synset glosses and detect subject/object dependencies.\n",
        "- If both subject and object tokens are present, branches directly toward original triple synsets.\n",
        "- Otherwise falls back to WordNet relations.\n",
        "\"\"\"\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from heapq import heappush, heappop\n",
        "import numpy as np\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gensim.downloader as api\n",
        "from collections import deque\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "import spacy"
      ],
      "metadata": {
        "id": "Jb_gtRCvh-DF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "# Load spaCy English model for dependency parsing\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj4iZlAfh_t8",
        "outputId": "7c05f9b7-2206-4db0-8e1b-43bd1d7dc3c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5TGC7I88EvX",
        "outputId": "4a9a81d0-ea01-4cf2-edde-b20a110926fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpers"
      ],
      "metadata": {
        "id": "H6Ahax9MciBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_neighbors(synset: wn.synset):\n",
        "    \"\"\"Get all neighbors of a synset based on its POS.\"\"\"\n",
        "    neighbors = []\n",
        "\n",
        "    # Add hypernyms and hyponyms\n",
        "    neighbors.extend(synset.hypernyms())\n",
        "    neighbors.extend(synset.hyponyms())\n",
        "\n",
        "    # Add POS-specific neighbors\n",
        "    if synset.pos() == 'n':\n",
        "        neighbors.extend(get_noun_neighbors(synset))\n",
        "    else:\n",
        "        neighbors.extend(get_verb_neighbors(synset))\n",
        "\n",
        "    return neighbors\n",
        "\n",
        "\n",
        "def get_noun_neighbors(syn: wn.synset):\n",
        "    \"\"\"Get neighbors for a noun synset.\"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.part_meronyms())\n",
        "    nbrs.update(syn.substance_meronyms())\n",
        "    nbrs.update(syn.member_meronyms())\n",
        "    nbrs.update(syn.part_holonyms())\n",
        "    nbrs.update(syn.substance_holonyms())\n",
        "    nbrs.update(syn.member_holonyms())\n",
        "    return list(nbrs)\n",
        "\n",
        "\n",
        "def get_verb_neighbors(syn: wn.synset):\n",
        "    \"\"\"Get neighbors for a verb synset.\"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.entailments())\n",
        "    nbrs.update(syn.causes())\n",
        "    nbrs.update(syn.also_sees())\n",
        "    nbrs.update(syn.verb_groups())\n",
        "    return list(nbrs)"
      ],
      "metadata": {
        "id": "xoGoYrlVcjiG"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding similarities"
      ],
      "metadata": {
        "id": "zJVKWnjy5qNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synset_embedding_centroid(synset:wn.synset, model=word2vec_model):\n",
        "    \"\"\"\n",
        "    Get the centroid (mean) of Word2Vec embeddings for all lemmas in a synset.\n",
        "\n",
        "    Args:\n",
        "        synset: WordNet Synset object (e.g., 'dog.n.01')\n",
        "        model: Loaded Word2Vec model\n",
        "\n",
        "    Returns:\n",
        "        numpy array representing the centroid, or an empty numpy array if no lemmas found\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get all lemma names from the synset\n",
        "        lemmas = [lemma.name().lower().replace('_', ' ') for lemma in synset.lemmas()]\n",
        "        # Collect embeddings for lemmas that exist in the model\n",
        "        embeddings = []\n",
        "        found_lemmas = []\n",
        "\n",
        "        for lemma in lemmas:\n",
        "            # Try the lemma as-is first\n",
        "            if lemma in model:\n",
        "                embeddings.append(model[lemma])\n",
        "                found_lemmas.append(lemma)\n",
        "            # Try with underscores replaced by spaces (for multi-word terms)\n",
        "            elif lemma.replace(' ', '_') in model:\n",
        "                embeddings.append(model[lemma.replace(' ', '_')])\n",
        "                found_lemmas.append(lemma)\n",
        "            # Try individual words if it's a multi-word term\n",
        "            elif ' ' in lemma:\n",
        "                words = lemma.split()\n",
        "                word_embeddings = []\n",
        "                for word in words:\n",
        "                    if word in model:\n",
        "                        word_embeddings.append(model[word])\n",
        "                if word_embeddings:\n",
        "                    # Average the embeddings of individual words\n",
        "                    embeddings.append(np.mean(word_embeddings, axis=0))\n",
        "                    found_lemmas.append(lemma)\n",
        "\n",
        "        if not embeddings:\n",
        "            # print(f\"Warning: No lemmas from {synset.name()} found in Word2Vec model\")\n",
        "            # print(f\"  Attempted lemmas: {lemmas}\")\n",
        "            return np.array([])\n",
        "\n",
        "        # print(f\"Synset {synset.name()}: Found {len(found_lemmas)}/{len(lemmas)} lemmas in model\")\n",
        "        # print(f\"  Found: {found_lemmas}\")\n",
        "\n",
        "        # Return the mean of all embeddings\n",
        "        return np.mean(embeddings, axis=0)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing synset {synset.name()}: {e}\")\n",
        "        return np.array([]) # Return empty array on error\n",
        "\n",
        "\n",
        "def emb_asymmetric_lex_rels(synset: wn.synset, model):\n",
        "    \"\"\"\n",
        "    match the asymmetric lexical relations between two synsets.\n",
        "    eg: meronyms-holonyms, hypernyms-hyponyms, etc.\n",
        "\n",
        "    Args:\n",
        "        synset: WordNet Synset object (e.g., 'dog.n.01')\n",
        "\n",
        "    Returns:\n",
        "        Dict of embeddings for asymmetric lexical relations of the synset.\n",
        "    \"\"\"\n",
        "    def _rel_centroid(get_attr):\n",
        "      try:\n",
        "        centroids = [\n",
        "            get_synset_embedding_centroid(s)\n",
        "            for s in get_attr(synset)\n",
        "        ]\n",
        "        # Filter out empty arrays and return the mean\n",
        "        valid_centroids = [c for c in centroids if c.size > 0]\n",
        "        return np.mean(valid_centroids, axis=0) if valid_centroids else np.array([])\n",
        "      except Exception as e:\n",
        "        print(f\"Error processing relation for synset {synset.name()}: {e}\")\n",
        "        return np.array([])\n",
        "\n",
        "    return {\n",
        "        \"meronyms\": {\n",
        "            \"part_meronyms\": _rel_centroid(lambda x: x.part_meronyms()),\n",
        "            \"substance_meronyms\": _rel_centroid(lambda x: x.substance_meronyms()),\n",
        "            \"member_meronyms\": _rel_centroid(lambda x: x.member_meronyms()),\n",
        "        },\n",
        "        \"holonyms\": {\n",
        "            \"part_holonyms\": _rel_centroid(lambda x: x.part_holonyms()),\n",
        "            \"substance_holonyms\": _rel_centroid(lambda x: x.substance_holonyms()),\n",
        "            \"member_holonyms\": _rel_centroid(lambda x: x.member_holonyms()),\n",
        "        },\n",
        "        \"hypernyms\": _rel_centroid(lambda x: x.hypernyms()),\n",
        "        \"hyponyms\": _rel_centroid(lambda x: x.hyponyms()),\n",
        "        \"entailments\": _rel_centroid(lambda x: x.entailments()),\n",
        "        \"causes\": _rel_centroid(lambda x: x.causes()),\n",
        "        \"also_sees\": _rel_centroid(lambda x: x.also_sees()),\n",
        "        \"verb_groups\": _rel_centroid(lambda x: x.verb_groups()),\n",
        "    }\n",
        "\n",
        "meronymy_map = {\n",
        "    \"meronyms\": \"holonyms\",\n",
        "    \"holonyms\": \"meronyms\",\n",
        "}\n",
        "pairing_maps = [\n",
        "    {\n",
        "        \"meronyms\": {\n",
        "            \"part_meronyms\": \"part_holonyms\",\n",
        "            \"substance_meronyms\": \"substance_holonyms\",\n",
        "            \"member_meronyms\": \"member_holonyms\",\n",
        "        },\n",
        "        \"hyponyms\": \"hypernyms\",\n",
        "        # \"entailments\": \"entailments\",\n",
        "        # \"causes\": \"causes\",\n",
        "        # \"also_sees\": \"also_sees\",\n",
        "        # \"verb_groups\": \"verb_groups\",\n",
        "    },\n",
        "    {\n",
        "        \"holonyms\": {\n",
        "            \"part_holonyms\": \"part_meronyms\",\n",
        "            \"substance_holonyms\": \"substance_meronyms\",\n",
        "            \"member_holonyms\": \"member_meronyms\",\n",
        "        },\n",
        "        \"hypernyms\": \"hyponyms\",\n",
        "        # \"entailments\": \"entailments\",\n",
        "        # \"causes\": \"causes\",\n",
        "        # \"also_sees\": \"also_sees\",\n",
        "        # \"verb_groups\": \"verb_groups\",\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "def get_asymmetric_emb_similarity(emb1, emb2, model=word2vec_model):\n",
        "\n",
        "    def get_asym_sim(i, k, l=None):\n",
        "        e1 = None\n",
        "        e2 = None\n",
        "        if l is None:\n",
        "            e1 = emb1.get(k)\n",
        "            e2 = emb2.get(pairing_maps[i].get(k))\n",
        "        else:\n",
        "            meronymy_key = meronymy_map.get(k)\n",
        "            if meronymy_key:\n",
        "                e1 = emb1.get(k, {}).get(l)\n",
        "                e2 = emb2.get(meronymy_key, {}).get(pairing_maps[i].get(k, {}).get(l))\n",
        "\n",
        "        # Check if both embeddings are valid (not None and not empty)\n",
        "        if e1 is not None and e2 is not None and e1.size > 0 and e2.size > 0:\n",
        "            # print(f\"Comparing embeddings for {k} and {pairing_maps[i].get(k) or pairing_maps[i].get(k, {}).get(l)}: {e1[:5]}... vs {e2[:5]}...\") # Print first 5 elements for debugging\n",
        "            return cosine_similarity([e1], [e2])[0][0] # Return single similarity value\n",
        "        return 0.0 # Return 0.0 similarity for invalid pairs\n",
        "\n",
        "\n",
        "    # check similarity of asymmetric relations\n",
        "    #   i.e. similarity of synset1's merynyms to synset2's holonyms\n",
        "    # need to aggregate similarities seperately to avoid destructive interference\n",
        "    #   if everything was aggregated together, similarity between hypernyms\n",
        "    #   might be cancelled by dissimilarity between hypernyms and hyponyms\n",
        "\n",
        "    asymm_rel_centroid_similarities = list()\n",
        "    # get centroid for if emb1 hyponyms/meronyms are ~ emb2 hypernyms/holonyms\n",
        "    for k, v in pairing_maps[0].items():\n",
        "        if isinstance(v, str):\n",
        "          sim = get_asym_sim(0, k)\n",
        "          if sim is not None:\n",
        "              asymm_rel_centroid_similarities.append(sim)\n",
        "        else:\n",
        "          for l in v.keys():\n",
        "            sim = get_asym_sim(0, k, l)\n",
        "            if sim is not None:\n",
        "                asymm_rel_centroid_similarities.append(sim)\n",
        "\n",
        "    asymm_sim_high_low = np.mean(asymm_rel_centroid_similarities) if asymm_rel_centroid_similarities else 0.0\n",
        "\n",
        "    asymm_centroid_sims = list()\n",
        "    # get centroid for if emb1 hypernyms/holonyms are ~ emb2 hyponyms/meronyms\n",
        "    for k, v in pairing_maps[1].items():\n",
        "        if isinstance(v, str):\n",
        "          sim = get_asym_sim(1, k)\n",
        "          if sim is not None:\n",
        "              asymm_rel_centroid_similarities.append(sim)\n",
        "        else:\n",
        "          for l in v.keys():\n",
        "            sim = get_asym_sim(1, k, l)\n",
        "            if sim is not None:\n",
        "                asymm_rel_centroid_similarities.append(sim)\n",
        "\n",
        "    asymm_sim_low_high = np.mean(asymm_rel_centroid_similarities) if asymm_rel_centroid_similarities else 0.0\n",
        "\n",
        "\n",
        "    # it doesn't matter which one is contains/includes/etc. the other,\n",
        "    #   as long as they're closer than antonyms or unrelated terms\n",
        "    # i.e. a good hyponym-hypernym pair is just as important\n",
        "    #   as a good hypernym-hyponym pair\n",
        "    return max(asymm_sim_high_low, asymm_sim_low_high)\n",
        "\n",
        "\n",
        "def get_synset_relatedness(\n",
        "      synset1:wn.synset,\n",
        "      synset2:wn.synset,\n",
        "      model=word2vec_model\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Compute cosine distance between centroids of two synsets.\n",
        "\n",
        "    Args:\n",
        "        synset1: WordNet Synset object (e.g., 'dog.n.01')\n",
        "        synset2: WordNet Synset object (e.g., 'cat.n.01')\n",
        "        model: Word2Vec model (if None, will load default)\n",
        "\n",
        "    Returns:\n",
        "        Float cosine distance between centroids (0 = identical, 1 = orthogonal, 2 = opposite)\n",
        "    \"\"\"\n",
        "    # Get centroids for both synsets\n",
        "    synset1_centroid = get_synset_embedding_centroid(synset1, model)\n",
        "    synset1_asym_rel_embs = emb_asymmetric_lex_rels(synset1, model)\n",
        "    synset2_centroid = get_synset_embedding_centroid(synset2, model)\n",
        "    synset2_asym_rel_embs = emb_asymmetric_lex_rels(synset2, model)\n",
        "\n",
        "    # Check if both main centroids were successfully computed and are not empty\n",
        "    if synset1_centroid.size == 0 or synset2_centroid.size == 0:\n",
        "        # print(f\"Warning: Could not compute main centroids for {synset1.name()} or {synset2.name()}\")\n",
        "        return 0.0  # Return 0.0 relatedness if main centroids are missing\n",
        "\n",
        "    # Get cosine distance between asymmetric lexical relation pairings\n",
        "    asymm_lex_rel_sim = get_asymmetric_emb_similarity(synset1_asym_rel_embs, synset2_asym_rel_embs, model)\n",
        "\n",
        "    # Compute cosine distance between centroids\n",
        "    centroid_similarity = cosine_similarity([synset1_centroid], [synset2_centroid])[0][0]\n",
        "\n",
        "    # idk maybe mean isn't best\n",
        "    return np.mean([centroid_similarity, asymm_lex_rel_sim])\n",
        "\n",
        "\n",
        "def get_k_closest_synset_pairs(\n",
        "    start_synsets: List[wn.synset],\n",
        "    end_synsets: List[wn.synset],\n",
        "    k=3, # Added k as a parameter with a default value\n",
        "    model=word2vec_model\n",
        "  ):\n",
        "  # pair opposing pairs\n",
        "  pairs = list()\n",
        "  for s1 in start_synsets:\n",
        "    for s2 in end_synsets:\n",
        "      relatedness = get_synset_relatedness(s1, s2)\n",
        "      if relatedness is not None: # Check for None relatedness\n",
        "          pairs.append((s1, s2, relatedness))\n",
        "\n",
        "  # get the top k most semantically similar pairings\n",
        "  return sorted(pairs, key=lambda x: x[2], reverse=True)[:k]"
      ],
      "metadata": {
        "id": "25u1o6wq5req"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pathing"
      ],
      "metadata": {
        "id": "X1WdFnm55oTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "GZ7mH1XuhtBN"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Core Path Finding Functions\n",
        "# ============================================================================\n",
        "\n",
        "def path_syn_to_syn(start_synset, end_synset, max_depth=6):\n",
        "    \"\"\"\n",
        "    Find shortest path between synsets of the same POS using bidirectional BFS.\n",
        "    Returns a list of synsets forming the path, or None if no path found.\n",
        "    \"\"\"\n",
        "\n",
        "    if not (start_synset.pos() == end_synset.pos() and start_synset.pos() in {'n', 'v'}):\n",
        "      raise ValueError(f\"{start_synset.name()} POS tag != {end_synset.name()}. Synsets must be of the same POS (noun or verb).\")\n",
        "\n",
        "    # Handle the trivial case where start and end are the same\n",
        "    if start_synset.name() == end_synset.name():\n",
        "        return [start_synset]\n",
        "\n",
        "    # Initialize two search frontiers\n",
        "    forward_queue = deque([(start_synset, 0)])\n",
        "    forward_visited = {start_synset.name(): [start_synset]}\n",
        "\n",
        "    backward_queue = deque([(end_synset, 0)])\n",
        "    backward_visited = {end_synset.name(): [end_synset]}\n",
        "\n",
        "    def expand_frontier(queue, visited_from_this_side, visited_from_other_side, is_forward):\n",
        "        \"\"\"Expand one step of the search frontier.\"\"\"\n",
        "        if not queue:\n",
        "            return None\n",
        "\n",
        "        curr_synset, depth = queue.popleft()\n",
        "\n",
        "        if depth >= (max_depth + 1) // 2:\n",
        "            return None\n",
        "\n",
        "        path_to_current = visited_from_this_side[curr_synset.name()]\n",
        "\n",
        "        for neighbor in get_all_neighbors(curr_synset):\n",
        "            neighbor_name = neighbor.name()\n",
        "\n",
        "            if neighbor_name in visited_from_this_side:\n",
        "                continue\n",
        "\n",
        "            if is_forward:\n",
        "                new_path = path_to_current + [neighbor]\n",
        "            else:\n",
        "                new_path = [neighbor] + path_to_current\n",
        "\n",
        "            if neighbor_name in visited_from_other_side:\n",
        "                other_path = visited_from_other_side[neighbor_name]\n",
        "\n",
        "                if is_forward:\n",
        "                    full_path = path_to_current + other_path\n",
        "                else:\n",
        "                    full_path = other_path + path_to_current\n",
        "\n",
        "                return full_path\n",
        "\n",
        "            visited_from_this_side[neighbor_name] = new_path\n",
        "            queue.append((neighbor, depth + 1))\n",
        "\n",
        "        return None\n",
        "\n",
        "    # Alternate between forward and backward search\n",
        "    while forward_queue or backward_queue:\n",
        "        if forward_queue:\n",
        "            result = expand_frontier(forward_queue, forward_visited, backward_visited, True)\n",
        "            if result:\n",
        "                return result\n",
        "\n",
        "        if backward_queue:\n",
        "            result = expand_frontier(backward_queue, backward_visited, forward_visited, False)\n",
        "            if result:\n",
        "                return result\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Gloss Analysis Helper Functions\n",
        "# ============================================================================\n",
        "\n",
        "def extract_subjects_from_gloss(gloss_doc):\n",
        "    \"\"\"Extract subject tokens from a parsed gloss.\"\"\"\n",
        "    subjects = []\n",
        "\n",
        "    # Direct subjects\n",
        "    subjects.extend([tok for tok in gloss_doc if tok.dep_ == \"nsubj\"])\n",
        "\n",
        "    # Passive subjects (which are actually objects semantically)\n",
        "    # Skip these for actor identification\n",
        "    passive_subjects = [tok for tok in gloss_doc if tok.dep_ == \"nsubjpass\"]\n",
        "\n",
        "    # Filter out passive subjects from the main list\n",
        "    subjects = [s for s in subjects if s not in passive_subjects]\n",
        "\n",
        "    return subjects, passive_subjects\n",
        "\n",
        "\n",
        "def extract_objects_from_gloss(gloss_doc):\n",
        "    \"\"\"Extract various types of object tokens from a parsed gloss.\"\"\"\n",
        "    objs = []\n",
        "\n",
        "    # Indirect objects\n",
        "    iobjs = [tok for tok in gloss_doc if tok.dep_ == \"iobj\"]\n",
        "    objs.extend(iobjs)\n",
        "\n",
        "    # Direct objects\n",
        "    # Only include if there were no indirect objects,\n",
        "    #   crude, but good for MVP\n",
        "    if not iobjs:\n",
        "        objs.extend([tok for tok in gloss_doc if tok.dep_ == \"dobj\"])\n",
        "\n",
        "    # Prepositional objects\n",
        "    objs.extend([tok for tok in gloss_doc if tok.dep_ == \"pobj\"])\n",
        "\n",
        "    # General objects\n",
        "    objs.extend([tok for tok in gloss_doc if tok.dep_ == \"obj\"])\n",
        "\n",
        "    # Check for noun chunks related to root verb\n",
        "    root_verbs = [tok for tok in gloss_doc if tok.dep_ == \"ROOT\" and tok.pos_ == \"VERB\"]\n",
        "    if root_verbs and not objs:\n",
        "        for noun_chunk in gloss_doc.noun_chunks:\n",
        "            if any(token.head == root_verbs[0] for token in noun_chunk):\n",
        "                objs.append(noun_chunk.root)\n",
        "\n",
        "    return objs\n",
        "\n",
        "\n",
        "def extract_verbs_from_gloss(gloss_doc, include_passive=False):\n",
        "    \"\"\"Extract verb tokens from a parsed gloss.\"\"\"\n",
        "    verbs = [tok for tok in gloss_doc if tok.pos_ == \"VERB\"]\n",
        "\n",
        "    if include_passive:\n",
        "        # Past participles used as adjectives or in relative clauses\n",
        "        passive_verbs = [tok for tok in gloss_doc if\n",
        "                        tok.tag_ in [\"VBN\", \"VBD\"] and\n",
        "                        tok.dep_ in [\"acl\", \"relcl\", \"amod\"]]\n",
        "        verbs.extend(passive_verbs)\n",
        "\n",
        "    return verbs\n",
        "\n",
        "\n",
        "def find_instrumental_verbs(gloss_doc):\n",
        "    \"\"\"Find verbs associated with instrumental use (e.g., 'used for').\"\"\"\n",
        "    instrumental_verbs = []\n",
        "\n",
        "    if \"used\" in gloss_doc.text.lower():\n",
        "        for i, token in enumerate(gloss_doc):\n",
        "            if token.text.lower() == \"used\":\n",
        "                # Check tokens after \"used\"\n",
        "                for j in range(i+1, min(i+4, len(gloss_doc))):\n",
        "                    if gloss_doc[j].pos_ == \"VERB\":\n",
        "                        instrumental_verbs.append(gloss_doc[j])\n",
        "\n",
        "    return instrumental_verbs\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Cross-POS Path Finding Functions\n",
        "# ============================================================================\n",
        "def get_most_similar_synsets(\n",
        "      candidates: List[List[wn.synset]],\n",
        "      target_synset: wn.synset,\n",
        "      k=3\n",
        "    ) -> List[Tuple[str, float]]:\n",
        "    \"\"\"\n",
        "    Given a list of candidate tokens and a target synset,\n",
        "    return the synset (name, relatedness) most similar to the target.\n",
        "    \"\"\"\n",
        "    for synsets in candidates:\n",
        "        # filter to subjects based on whether they reside in the same sub-category\n",
        "        #   where the subcategory != 'entity.n.01' or a similar top-level\n",
        "        synsets = [\n",
        "            s for s in synsets\n",
        "            if s.root_hypernyms() != s.lowest_common_hypernyms(target_synset)\n",
        "        ]\n",
        "        if synsets:\n",
        "            # if the target is a verb,\n",
        "            #   filter out any synsets with no lemma frames matching the target\n",
        "            #   frame patterns: (Somebody [v] something), (Somebody [v]), ...\n",
        "            if target_synset.pos() == 'v':\n",
        "                synsets = [\n",
        "                    s for s in synsets\n",
        "                    if any(\n",
        "                        frame in s.frame_ids()\n",
        "                        for frame in target_synset.frame_ids()\n",
        "                    )\n",
        "                ]\n",
        "            return sorted(\n",
        "                [\n",
        "                    (s, get_synset_relatedness(s, target_synset))\n",
        "                    for s in synsets\n",
        "                ],\n",
        "                key=lambda x: x[1],\n",
        "                reverse=True\n",
        "            )[:k]\n",
        "    return None\n",
        "\n",
        "\n",
        "def find_subject_to_predicate_path(\n",
        "      subject_synset: wn.synset,\n",
        "      predicate_synset: wn.synset,\n",
        "      max_depth=6,\n",
        "      visited=set(),\n",
        "    ):\n",
        "    \"\"\"Find path from subject (noun) to predicate (verb).\"\"\"\n",
        "    if subject_synset.name() in visited or predicate_synset.name() in visited:\n",
        "      return None\n",
        "\n",
        "    paths = []\n",
        "    print()\n",
        "    print(f\"Finding path from {subject_synset.name()} to {predicate_synset.name()}\")\n",
        "\n",
        "    # Strategy 1: Look for active subjects in verb's gloss\n",
        "    pred_gloss_doc = nlp(predicate_synset.definition())\n",
        "    # passive subjects are semantically equivalent to objects\n",
        "    active_subjects, _ = extract_subjects_from_gloss(pred_gloss_doc)\n",
        "    # convert spacy tokens to lists of synsets\n",
        "    subjects = [wn.synsets(s.text, pos=subject_synset.pos()) for s in active_subjects]\n",
        "    # of the remaining subjects, get the most similar\n",
        "    top_k = get_most_similar_synsets(active_subjects[:3], subject_synset)\n",
        "    if top_k:\n",
        "      print(f\"Found best matches for {subject_synset.name()}: {top_k} using strategy 1\")\n",
        "      for matched_synset, _ in top_k:\n",
        "        path = path_syn_to_syn(subject_synset, matched_synset, max_depth-1)\n",
        "        if path:\n",
        "            paths.append(path + [predicate_synset])\n",
        "\n",
        "    # Strategy 2: Look for verbs in the noun's gloss\n",
        "    subj_gloss_doc = nlp(subject_synset.definition())\n",
        "    verbs = extract_verbs_from_gloss(subj_gloss_doc, include_passive=False)\n",
        "    # convert spacy tokens to lists of synsets\n",
        "    verbs = [wn.synsets(v.text, pos=predicate_synset.pos()) for v in verbs]\n",
        "    # of the remaining subjects, get the most similar\n",
        "    top_k = get_most_similar_synsets(verbs[:3], predicate_synset)\n",
        "    if top_k:\n",
        "      print(f\"Found best matches for {predicate_synset.name()}: {top_k} using strategy 2\")\n",
        "      for matched_synset, _ in top_k:\n",
        "        path = path_syn_to_syn(matched_synset, predicate_synset, max_depth-1)\n",
        "        if path:\n",
        "            paths.append([subject_synset] + path)\n",
        "\n",
        "    # Strategy 3: Explore the 3 most promising pairs of neighbors\n",
        "    subject_neighbors = get_all_neighbors(subject_synset)\n",
        "    predicate_neighbors = get_all_neighbors(predicate_synset)\n",
        "    top_k = get_k_closest_synset_pairs(subject_neighbors, predicate_neighbors)\n",
        "    if top_k:\n",
        "      print(f\"Most promising pairs for bidirectional exploration: {top_k} using strategy 3\")\n",
        "      for s, p, _ in top_k:\n",
        "        visited.add(subject_synset.name())\n",
        "        visited.add(predicate_synset.name())\n",
        "        path = find_subject_to_predicate_path(s, p, max_depth-1, visited)\n",
        "        if path:\n",
        "            paths.append([subject_synset] + path + [predicate_synset])\n",
        "\n",
        "\n",
        "    # Return shortest path if any found\n",
        "    return min(paths, key=len) if paths else None\n",
        "\n",
        "\n",
        "def find_predicate_to_object_path(\n",
        "      predicate_synset: wn.synset,\n",
        "      object_synset: wn.synset,\n",
        "      max_depth=6,\n",
        "      visited=set(),\n",
        "    ):\n",
        "    \"\"\"Find path from predicate (verb) to object (noun).\"\"\"\n",
        "\n",
        "    if predicate_synset.name() in visited or object_synset.name() in visited:\n",
        "      return None\n",
        "\n",
        "    paths = []\n",
        "    print()\n",
        "    print(f\"Finding path from {predicate_synset.name()} to {object_synset.name()}\")\n",
        "\n",
        "    # === Strategy 1: Objects in predicate gloss (incl. passive subjects) ===\n",
        "    pred_gloss_doc = nlp(predicate_synset.definition())\n",
        "    objects = extract_objects_from_gloss(pred_gloss_doc)\n",
        "    _, passive_subjects = extract_subjects_from_gloss(pred_gloss_doc)\n",
        "    objects.extend(passive_subjects)\n",
        "    # convert spacy tokens to lists of synsets\n",
        "    objects = [wn.synsets(o.text, pos=object_synset.pos()) for o in objects]\n",
        "    top_k = get_most_similar_synsets(objects[:3], object_synset)\n",
        "    if top_k:\n",
        "      print(f\"Found best matches for {object_synset.name()}: {top_k} using strategy 1\")\n",
        "      for matched_synset, _ in top_k:\n",
        "        path = path_syn_to_syn(matched_synset, object_synset, max_depth-1)\n",
        "        if path:\n",
        "            paths.append([predicate_synset] + path)\n",
        "\n",
        "    # === Strategy 2: Verbs in object's gloss ===\n",
        "    obj_gloss_doc = nlp(object_synset.definition())\n",
        "    verbs = extract_verbs_from_gloss(obj_gloss_doc, include_passive=True)\n",
        "    # Use instrumental verbs in object's gloss as backup\n",
        "    verbs.extend(find_instrumental_verbs(obj_gloss_doc))\n",
        "    # convert spacy tokens to lists of synsets\n",
        "    verbs = [wn.synsets(v.text, pos=predicate_synset.pos()) for v in verbs]\n",
        "    top_k = get_most_similar_synsets(verbs[:3], predicate_synset)\n",
        "    if top_k:\n",
        "      print(f\"Found best matches for {predicate_synset.name()}: {top_k} using strategy 2\")\n",
        "      for matched_synset, _ in top_k:\n",
        "        path = path_syn_to_syn(predicate_synset, matched_synset, max_depth-1)\n",
        "        if path:\n",
        "            paths.append(path + [object_synset])\n",
        "\n",
        "    # Strategy 3: Explore the 3 most promising neighbors\n",
        "    predicate_neighbors = get_all_neighbors(predicate_synset)\n",
        "    object_neighbors = get_all_neighbors(object_synset)\n",
        "    top_k = get_k_closest_synset_pairs(predicate_neighbors, object_neighbors)\n",
        "    if top_k:\n",
        "      print(f\"Most promising pairs for bidirectional exploration: {top_k} using strategy 3\")\n",
        "      for p, o, _ in top_k:\n",
        "        visited.add(predicate_synset.name())\n",
        "        visited.add(object_synset.name())\n",
        "        path = find_predicate_to_object_path(p, o, max_depth-1, visited)\n",
        "        if path:\n",
        "            paths.append([predicate_synset] + path + [object_synset])\n",
        "\n",
        "\n",
        "    # Return shortest path if any found\n",
        "    return min(paths, key=len) if paths else None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Main Connected Path Finding Function\n",
        "# ============================================================================\n",
        "\n",
        "def find_connected_shortest_paths(subject_word, predicate_word, object_word, max_depth=10):\n",
        "    \"\"\"\n",
        "    Find shortest connected paths from subject through predicate to object.\n",
        "    Ensures that the same predicate synset connects both paths.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get synsets for each word\n",
        "    subject_synsets = wn.synsets(subject_word, pos=wn.NOUN)\n",
        "    predicate_synsets = wn.synsets(predicate_word, pos=wn.VERB)\n",
        "    object_synsets = wn.synsets(object_word, pos=wn.NOUN)\n",
        "\n",
        "    best_combined_path_length = float('inf')\n",
        "    best_subject_path = None\n",
        "    best_object_path = None\n",
        "    best_predicate = None\n",
        "\n",
        "    # Try each predicate synset as the connector\n",
        "    for pred in predicate_synsets:\n",
        "        # Find paths from all subjects to this specific predicate\n",
        "        subject_paths = []\n",
        "        for subj in subject_synsets:\n",
        "            path = find_subject_to_predicate_path(subj, pred, max_depth)\n",
        "            if path:\n",
        "                subject_paths.append(path)\n",
        "\n",
        "        # Find paths from this specific predicate to all objects\n",
        "        object_paths = []\n",
        "        for obj in object_synsets:\n",
        "            path = find_predicate_to_object_path(pred, obj, max_depth)\n",
        "            if path:\n",
        "                object_paths.append(path)\n",
        "\n",
        "        # If we have both paths through this predicate, check if it's the best\n",
        "        if subject_paths and object_paths:\n",
        "            shortest_subj_path = min(subject_paths, key=len)\n",
        "            shortest_obj_path = min(object_paths, key=len)\n",
        "\n",
        "            # Calculate combined length (subtract 1 to avoid counting predicate twice)\n",
        "            combined_length = len(shortest_subj_path) + len(shortest_obj_path) - 1\n",
        "\n",
        "            if combined_length < best_combined_path_length:\n",
        "                best_combined_path_length = combined_length\n",
        "                best_subject_path = shortest_subj_path\n",
        "                best_object_path = shortest_obj_path\n",
        "                best_predicate = pred\n",
        "\n",
        "    return best_subject_path, best_object_path, best_predicate\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Display Functions\n",
        "# ============================================================================\n",
        "\n",
        "def show_path(label, path):\n",
        "    \"\"\"Pretty print a path of synsets.\"\"\"\n",
        "    if path:\n",
        "        print(f\"{label}:\")\n",
        "        print(\" -> \".join(f\"{s.name()} ({s.definition()})\" for s in path))\n",
        "        print(f\"Path length: {len(path)}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"{label}: No path found\")\n",
        "        print()\n",
        "\n",
        "\n",
        "def show_connected_paths(subject_path, object_path, predicate):\n",
        "    \"\"\"Display the connected paths with their shared predicate.\"\"\"\n",
        "    if subject_path and object_path and predicate:\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"CONNECTED PATH through predicate: {predicate.name()}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        show_path(\"Subject -> Predicate path\", subject_path)\n",
        "        show_path(\"Predicate -> Object path\", object_path)\n",
        "\n",
        "        # Show the complete connected path\n",
        "        complete_path = subject_path + object_path[1:]  # Avoid duplicating the predicate\n",
        "        print(\"Complete connected path:\")\n",
        "        print(\" -> \".join(f\"{s.name()}\" for s in complete_path))\n",
        "        print(f\"Total path length: {len(complete_path)}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(\"No connected path found through any predicate synset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "bwShzrxtIhOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Find shortest connected paths\n",
        "subject_path, object_path, connecting_predicate = find_connected_shortest_paths(\n",
        "    \"burglar\", \"shoot\", \"woman\", max_depth=10\n",
        ")\n",
        "\n",
        "# Display results\n",
        "show_connected_paths(subject_path, object_path, connecting_predicate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "valgVwQUh0SJ",
        "outputId": "71f2218d-1a86-47bf-af15-79250b695334"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finding path from burglar.n.01 to shoot.v.01\n",
            "Found best matches for shoot.v.01: [(Synset('enter.v.01'), 0.21358663216233253), (Synset('record.v.01'), 0.207358255982399), (Synset('insert.v.01'), 0.15856357663869858)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('thief.n.01'), Synset('gun_down.v.01'), 0.118843337520957), (Synset('thief.n.01'), Synset('shoot.v.02'), 0.10573970898985863), (Synset('thief.n.01'), Synset('strike.v.04'), 0.09913444984704256)] using strategy 3\n",
            "\n",
            "Finding path from thief.n.01 to gun_down.v.01\n",
            "Found best matches for gun_down.v.01: [(Synset('film.v.01'), 0.18922990933060646), (Synset('choose.v.01'), 0.14402316510677338), (Synset('take.v.04'), 0.12459021806716919)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('pickpocket.n.01'), Synset('shoot.v.01'), 0.15776156028732657), (Synset('sneak_thief.n.01'), Synset('shoot.v.01'), 0.1319025754928589), (Synset('plunderer.n.01'), Synset('shoot.v.01'), 0.12187455524690449)] using strategy 3\n",
            "\n",
            "Finding path from shoot.v.01 to woman.n.01\n",
            "Found best matches for woman.n.01: [(Synset('projectile.n.01'), 0.08334837853908539), (Synset('missile.n.01'), 0.051999781746417284)] using strategy 1\n",
            "Found best matches for shoot.v.01: [(Synset('pit.v.01'), 0.22008430119603872), (Synset('fight.v.02'), 0.21860437840223312), (Synset('oppose.v.06'), 0.12022987008094788)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('shoot.v.02'), Synset('smasher.n.02'), 0.18848449550569057), (Synset('grass.v.01'), Synset('divorcee.n.01'), 0.18262991309165955), (Synset('shoot.v.02'), Synset('girl.n.01'), 0.16449726186692715)] using strategy 3\n",
            "\n",
            "Finding path from shoot.v.02 to smasher.n.02\n",
            "Found best matches for smasher.n.02: [(Synset('projectile.n.01'), 0.12665101885795593), (Synset('missile.n.01'), 0.0869186008349061)] using strategy 1\n",
            "Found best matches for shoot.v.02: [(Synset('count.v.08'), 0.15606682654470205), (Synset('expect.v.03'), 0.11550608556717634), (Synset('search.v.02'), 0.09198372066020966)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('kill.v.01'), Synset('woman.n.01'), 0.11191766150295734), (Synset('flight.v.01'), Synset('woman.n.01'), 0.09251023642718792), (Synset('shoot.v.01'), Synset('woman.n.01'), 0.07096765004098415)] using strategy 3\n",
            "\n",
            "Finding path from grass.v.01 to divorcee.n.01\n",
            "Found best matches for divorcee.n.01: [(Synset('dame.n.01'), 0.21414978802204132), (Synset('bird.n.01'), 0.10171869397163391), (Synset('shuttlecock.n.01'), 0.10148217529058456)] using strategy 1\n",
            "Found best matches for grass.v.01: [(Synset('separate.v.02'), 0.061330899596214294), (Synset('separate.v.07'), 0.06114692892879248), (Synset('classify.v.01'), 0.05450662970542908)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('shoot.v.01'), Synset('woman.n.01'), 0.07096765004098415)] using strategy 3\n",
            "\n",
            "Finding path from blast.v.07 to woman.n.02\n",
            "Found best matches for woman.n.02: [(Synset('scene.n.04'), 0.15885766621795483), (Synset('shot.n.05'), 0.14221943728625774), (Synset('shot.n.12'), 0.10786879062652588)] using strategy 1\n",
            "Found best matches for blast.v.07: [(Synset('play.v.01'), 0.2021655235439539), (Synset('play.v.15'), 0.19712108746170998), (Synset('play.v.31'), 0.19652116298675537)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('shoot.v.01'), Synset('female.n.02'), 0.11228062957525253), (Synset('gun.v.01'), Synset('female.n.02'), 0.1083666980266571), (Synset('fire.v.02'), Synset('female.n.02'), 0.06762178614735603)] using strategy 3\n",
            "\n",
            "Finding path from gun.v.01 to female.n.02\n",
            "Found best matches for female.n.02: [(Synset('gunman.n.01'), 0.1259566592052579), (Synset('gun.n.01'), 0.1033579520881176), (Synset('gunman.n.02'), 0.09771857876330614)] using strategy 1\n",
            "Found best matches for gun.v.01: [(Synset('belong.v.02'), 0.05266899894922972), (Synset('belong.v.05'), 0.05266899894922972)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('machine_gun.v.01'), Synset('person.n.01'), 0.18206517584621906), (Synset('machine_gun.v.01'), Synset('woman.n.01'), 0.11714725568890572), (Synset('blast.v.07'), Synset('girl_wonder.n.01'), 0.11629236862063408)] using strategy 3\n",
            "\n",
            "Finding path from machine_gun.v.01 to person.n.01\n",
            "Found best matches for person.n.01: [(Synset('gunman.n.01'), 0.21503830887377262), (Synset('gunman.n.02'), 0.15399414487183094), (Synset('gun.n.01'), 0.15276937698945403)] using strategy 1\n",
            "Most promising pairs for bidirectional exploration: [(Synset('gun.v.01'), Synset('sex_object.n.01'), 0.19522935897111893), (Synset('gun.v.01'), Synset('suspect.n.01'), 0.18982163816690445), (Synset('shoot.v.01'), Synset('toucher.n.01'), 0.18302162736654282)] using strategy 3\n",
            "\n",
            "Finding path from film.v.01 to charwoman.n.01\n",
            "Found best matches for charwoman.n.01: [(Synset('movie.n.01'), 0.1332193836569786), (Synset('film.n.02'), 0.12289061211049557), (Synset('film.n.04'), 0.1084623672068119)] using strategy 1\n",
            "Found best matches for film.v.01: [(Synset('hire.v.01'), 0.22168723121285439), (Synset('use.v.01'), 0.21810607612133026)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('record.v.01'), Synset('cleaner.n.03'), 0.04929240047931671), (Synset('photograph.v.01'), Synset('cleaner.n.03'), 0.029541961383074522), (Synset('reshoot.v.01'), Synset('cleaner.n.03'), 0.021005313843488693)] using strategy 3\n",
            "\n",
            "Finding path from record.v.01 to cleaner.n.03\n",
            "Found best matches for cleaner.n.03: [(Synset('phonograph_record.n.01'), 0.050988875329494476)] using strategy 1\n",
            "Found best matches for record.v.01: [(Synset('clean.v.01'), 0.16458315774798393), (Synset('clean.v.02'), 0.1630630623549223), (Synset('clean.v.07'), 0.13268176279962063)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('save.v.02'), Synset('window_cleaner.n.01'), 0.13647998683154583), (Synset('tally.v.03'), Synset('chimneysweeper.n.01'), 0.12457721680402756), (Synset('clock_in.v.01'), Synset('window_cleaner.n.01'), 0.11516962945461273)] using strategy 3\n",
            "\n",
            "Finding path from save.v.02 to window_cleaner.n.01\n",
            "Found best matches for window_cleaner.n.01: [(Synset('consumption.n.03'), 0.11883455142378807)] using strategy 1\n",
            "Found best matches for save.v.02: [(Synset('clean.v.01'), 0.22162983939051628), (Synset('cleanse.v.01'), 0.20707804802805185), (Synset('scavenge.v.04'), 0.2044342365115881)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('conserve.v.03'), Synset('cleaner.n.03'), 0.1606996152549982), (Synset('keep.v.03'), Synset('cleaner.n.03'), 0.09383532591164112), (Synset('record.v.01'), Synset('cleaner.n.03'), 0.04929240047931671)] using strategy 3\n",
            "\n",
            "Finding path from tally.v.03 to chimneysweeper.n.01\n",
            "Found best matches for chimneysweeper.n.01: [(Synset('game.n.07'), 0.10237508360296488), (Synset('game.n.04'), 0.09924041572958231), (Synset('game.n.09'), 0.0899518842343241)] using strategy 1\n",
            "Found best matches for tally.v.03: [(Synset('cleanse.v.01'), 0.11232186295092106), (Synset('houseclean.v.01'), 0.10783211648231372)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('record.v.01'), Synset('cleaner.n.03'), 0.04929240047931671), (Synset('count.v.01'), Synset('cleaner.n.03'), 0.03422058233991265)] using strategy 3\n",
            "\n",
            "Finding path from shoot.v.05 to womanhood.n.02\n",
            "Most promising pairs for bidirectional exploration: [(Synset('project.v.10'), Synset('womankind.n.01'), 0.08301948010921478), (Synset('project.v.10'), Synset('class.n.03'), 0.06382068432867527)] using strategy 3\n",
            "\n",
            "Finding path from project.v.10 to womankind.n.01\n",
            "Found best matches for project.v.10: [(Synset('spot.v.02'), 0.17868830263614655), (Synset('distinguish.v.01'), 0.148460540920496), (Synset('identify.v.05'), 0.1294664442539215)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('send.v.01'), Synset('people.n.01'), 0.06482312362641096), (Synset('shoot.v.05'), Synset('people.n.01'), 0.06077452190220356), (Synset('shoot.v.05'), Synset('womanhood.n.02'), 0.057982489466667175)] using strategy 3\n",
            "\n",
            "Finding path from send.v.01 to people.n.01\n",
            "Most promising pairs for bidirectional exploration: [(Synset('move.v.02'), Synset('initiate.n.03'), 0.16020063310861588), (Synset('refer.v.04'), Synset('initiate.n.03'), 0.15761913172900677), (Synset('turn.v.20'), Synset('initiate.n.03'), 0.15550506487488747)] using strategy 3\n",
            "\n",
            "Finding path from move.v.02 to initiate.n.03\n",
            "Found best matches for initiate.n.03: [(Synset('position.n.04'), 0.1568557173013687), (Synset('position.n.03'), 0.14228534884750843), (Synset('position.n.12'), 0.13744705729186535)] using strategy 1\n",
            "Found best matches for move.v.02: [(Synset('insert.v.01'), 0.23582375049591064), (Synset('introduce.v.07'), 0.23301414027810097), (Synset('bring_in.v.01'), 0.21472591161727905)] using strategy 2\n",
            "Most promising pairs for bidirectional exploration: [(Synset('change_hands.v.01'), Synset('people.n.01'), 0.16335549112409353), (Synset('herd.v.01'), Synset('people.n.01'), 0.15014286153018475), (Synset('separate.v.02'), Synset('people.n.01'), 0.1358993323519826)] using strategy 3\n",
            "======================================================================\n",
            "CONNECTED PATH through predicate: shoot.v.01\n",
            "======================================================================\n",
            "Subject -> Predicate path:\n",
            "burglar.n.01 (a thief who enters a building with intent to steal) -> enter.v.01 (to come or go into) -> board.v.01 (get on board of (trains, buses, ships, aircraft, etc.)) -> catch.v.09 (reach in time) -> overtake.v.01 (catch up with and possibly overtake) -> compete.v.01 (compete for something; engage in a contest; measure oneself against others) -> contend.v.06 (be engaged in a fight; carry on a fight) -> attack.v.01 (launch an attack or assault on; begin hostilities or start warfare with) -> strike.v.04 (make a strategic, offensive, assault against an enemy, opponent, or a target) -> shoot.v.01 (hit with a missile from a weapon)\n",
            "Path length: 10\n",
            "\n",
            "Predicate -> Object path:\n",
            "shoot.v.01 (hit with a missile from a weapon) -> strike.v.04 (make a strategic, offensive, assault against an enemy, opponent, or a target) -> attack.v.01 (launch an attack or assault on; begin hostilities or start warfare with) -> contend.v.06 (be engaged in a fight; carry on a fight) -> fight.v.02 (fight against or resist strongly) -> woman.n.01 (an adult female person (as opposed to a man))\n",
            "Path length: 6\n",
            "\n",
            "Complete connected path:\n",
            "burglar.n.01 -> enter.v.01 -> board.v.01 -> catch.v.09 -> overtake.v.01 -> compete.v.01 -> contend.v.06 -> attack.v.01 -> strike.v.04 -> shoot.v.01 -> strike.v.04 -> attack.v.01 -> contend.v.06 -> fight.v.02 -> woman.n.01\n",
            "Total path length: 15\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJjxiYEy9smM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPf51Rq6Gk2CG38ivUiLtzh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/SMIED/blob/main/BFS_Semantic_Decomposition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Semantic decomposition of (\"cat\", \"eats\", \"mouse\") using WordNet + spaCy + depth-limited GBFS.\n",
        "- Uses spaCy to parse verb synset glosses and detect subject/object dependencies.\n",
        "- If both subject and object tokens are present, branches directly toward original triple synsets.\n",
        "- Otherwise falls back to WordNet relations.\n",
        "\"\"\"\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.wsd import lesk\n",
        "from heapq import heappush, heappop"
      ],
      "metadata": {
        "id": "Jb_gtRCvh-DF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "# Load spaCy English model for dependency parsing\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj4iZlAfh_t8",
        "outputId": "93488893-eda4-41f3-9b0f-9e4c3eecc62d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GZ7mH1XuhtBN"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.wsd import lesk\n",
        "import spacy\n",
        "\n",
        "# Initialize spaCy (assuming you have it loaded)\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def path_syn_to_syn(start_synset, end_synset, max_depth=6):\n",
        "    \"\"\"\n",
        "    Find shortest path between synsets of the same POS using BFS.\n",
        "    Returns a list of synsets forming the path, or None if no path found.\n",
        "    \"\"\"\n",
        "\n",
        "    assert start_synset.pos() == end_synset.pos() and start_synset.pos() in {'n', 'v'}\n",
        "\n",
        "    # Use deque for efficient FIFO operations (true BFS)\n",
        "    queue = deque([(start_synset, [start_synset])])  # Store (synset, path_to_synset)\n",
        "    visited = {start_synset.name()}  # Track visited synsets to avoid cycles\n",
        "\n",
        "    while queue:\n",
        "        curr_synset, path = queue.popleft()  # FIFO for BFS (shortest path)\n",
        "\n",
        "        # Check if we've reached the target\n",
        "        if curr_synset.name() == end_synset.name():\n",
        "            return path  # Return the complete path of synsets\n",
        "\n",
        "        # Check if we've exceeded max depth\n",
        "        if len(path) > max_depth:\n",
        "            continue\n",
        "\n",
        "        # Get all neighbors based on POS\n",
        "        neighbors = []\n",
        "\n",
        "        # Add hypernyms and hyponyms\n",
        "        neighbors.extend(curr_synset.hypernyms())\n",
        "        neighbors.extend(curr_synset.hyponyms())\n",
        "\n",
        "        # Add POS-specific neighbors\n",
        "        if start_synset.pos() == 'n':\n",
        "            neighbors.extend(get_noun_neighbors(curr_synset))\n",
        "        else:\n",
        "            neighbors.extend(get_verb_neighbors(curr_synset))\n",
        "\n",
        "        # Process unvisited neighbors\n",
        "        for neighbor in neighbors:\n",
        "            if neighbor.name() not in visited:\n",
        "                visited.add(neighbor.name())\n",
        "                new_path = path + [neighbor]\n",
        "                queue.append((neighbor, new_path))\n",
        "\n",
        "    return None  # No path found\n",
        "\n",
        "\n",
        "def get_noun_neighbors(syn):\n",
        "    \"\"\"\n",
        "    Get neighbors for a noun synset.\n",
        "    \"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.part_meronyms())\n",
        "    nbrs.update(syn.substance_meronyms())\n",
        "    nbrs.update(syn.member_meronyms())\n",
        "    nbrs.update(syn.part_holonyms())\n",
        "    nbrs.update(syn.substance_holonyms())\n",
        "    nbrs.update(syn.member_holonyms())\n",
        "    return list(nbrs)\n",
        "\n",
        "\n",
        "def get_verb_neighbors(syn):\n",
        "    \"\"\"\n",
        "    Get neighbors for a verb synset.\n",
        "    \"\"\"\n",
        "    nbrs = set()\n",
        "    nbrs.update(syn.entailments())\n",
        "    nbrs.update(syn.causes())\n",
        "    nbrs.update(syn.also_sees())\n",
        "    nbrs.update(syn.verb_groups())\n",
        "    return list(nbrs)\n",
        "\n",
        "def cross_pos_path(start_synset, end_synset, max_depth=6):\n",
        "    \"\"\"\n",
        "    Find path between synsets of different POS using gloss analysis.\n",
        "    Returns a list of synsets forming the path, or None if no path found.\n",
        "    \"\"\"\n",
        "\n",
        "    assert start_synset.pos() != end_synset.pos() and start_synset.pos() in {'n', 'v'}\n",
        "\n",
        "    # If start is a noun and end is a verb (subject -> predicate)\n",
        "    if start_synset.pos() == 'n':\n",
        "        # Strategy 1: Look for subject references in the verb's gloss\n",
        "        pred_gloss_doc = nlp(end_synset.definition())\n",
        "        subjs = [tok for tok in pred_gloss_doc if tok.dep_ == \"nsubj\"]\n",
        "\n",
        "        if subjs:\n",
        "            try:\n",
        "                subject_synset = lesk(pred_gloss_doc.text, subjs[0].text, pos='n')\n",
        "                if subject_synset:\n",
        "                    # Find path from start to the subject mentioned in verb's gloss\n",
        "                    path = path_syn_to_syn(start_synset, subject_synset, max_depth=max_depth)\n",
        "                    if path:\n",
        "                        # Add the verb at the end to complete the cross-POS path\n",
        "                        return path + [end_synset]\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Strategy 2: Look for verb references in the noun's gloss\n",
        "        subj_gloss_doc = nlp(start_synset.definition())\n",
        "        preds = [tok for tok in subj_gloss_doc if tok.pos_ == \"VERB\"]\n",
        "\n",
        "        if preds:\n",
        "            try:\n",
        "                subj_pred_synset = lesk(subj_gloss_doc.text, preds[0].text, pos='v')\n",
        "                if subj_pred_synset:\n",
        "                    # Find path from verb found in subj gloss to the pred synset\n",
        "                    path = path_syn_to_syn(subj_pred_synset, end_synset, max_depth=max_depth)\n",
        "                    if path:\n",
        "                        # Prepend the noun to complete the cross-POS path (bug fix)\n",
        "                        return [start_synset] + path\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    # If start is a verb and end is a noun (predicate -> object)\n",
        "    elif start_synset.pos() == 'v':\n",
        "        # Strategy 1: Look for direct objects in the verb's gloss\n",
        "        pred_gloss_doc = nlp(start_synset.definition())\n",
        "\n",
        "        # Expand to look for multiple types of objects\n",
        "        objs = []\n",
        "        # Direct objects\n",
        "        objs.extend([tok for tok in pred_gloss_doc if tok.dep_ == \"dobj\"])\n",
        "        # Prepositional objects (e.g., \"eat at a table\")\n",
        "        objs.extend([tok for tok in pred_gloss_doc if tok.dep_ == \"pobj\"])\n",
        "        # Indirect objects (e.g., \"give someone something\")\n",
        "        objs.extend([tok for tok in pred_gloss_doc if tok.dep_ == \"iobj\"])\n",
        "        # Objects of prepositions that relate to the main verb\n",
        "        objs.extend([tok for tok in pred_gloss_doc if tok.dep_ == \"obj\"])\n",
        "\n",
        "        # Also look for noun chunks that might be objects\n",
        "        if not objs:\n",
        "            # Look for nouns that are children of the root verb\n",
        "            root_verbs = [tok for tok in pred_gloss_doc if tok.dep_ == \"ROOT\" and tok.pos_ == \"VERB\"]\n",
        "            if root_verbs:\n",
        "                for noun_chunk in pred_gloss_doc.noun_chunks:\n",
        "                    # Check if this noun chunk is related to the main verb\n",
        "                    if any(token.head == root_verbs[0] for token in noun_chunk):\n",
        "                        objs.append(noun_chunk.root)\n",
        "\n",
        "        if objs:\n",
        "            # Try multiple object candidates\n",
        "            for obj in objs[:3]:  # Limit to first 3 to avoid excessive computation\n",
        "                try:\n",
        "                    object_synset = lesk(pred_gloss_doc.text, obj.text, pos='n')\n",
        "                    if object_synset:\n",
        "                        # Find path from object mentioned in verb's gloss to end\n",
        "                        path = path_syn_to_syn(object_synset, end_synset, max_depth=max_depth)\n",
        "                        if path:\n",
        "                            # Add the verb at the beginning to complete the cross-POS path\n",
        "                            return [start_synset] + path\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # Strategy 2: Look for verb references in the object's gloss\n",
        "        obj_gloss_doc = nlp(end_synset.definition())\n",
        "\n",
        "        # Look for verbs that might describe actions done to/with this object\n",
        "        verbs = [tok for tok in obj_gloss_doc if tok.pos_ == \"VERB\"]\n",
        "\n",
        "        # Also check if the object is described as something that gets verbed\n",
        "        # (e.g., \"food: something that is eaten\")\n",
        "        passive_verbs = [tok for tok in obj_gloss_doc if tok.tag_ in [\"VBN\", \"VBD\"] and tok.dep_ in [\"acl\", \"relcl\"]]\n",
        "        verbs.extend(passive_verbs)\n",
        "\n",
        "        if verbs:\n",
        "            # Try multiple verb candidates\n",
        "            for verb in verbs[:3]:  # Limit to first 3\n",
        "                try:\n",
        "                    obj_verb_synset = lesk(obj_gloss_doc.text, verb.text, pos='v')\n",
        "                    if obj_verb_synset:\n",
        "                        # Find path from start verb to verb found in object's gloss\n",
        "                        path = path_syn_to_syn(start_synset, obj_verb_synset, max_depth=max_depth)\n",
        "                        if path:\n",
        "                            # Append the noun to complete the cross-POS path\n",
        "                            return path + [end_synset]\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "    # If gloss-based approach fails, return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def find_shortest_paths(subject_word, predicate_word, object_word, max_depth=10):\n",
        "    \"\"\"\n",
        "    Find shortest paths from subject to predicate and predicate to object.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get synsets for each word\n",
        "    subject_synsets = wn.synsets(subject_word, pos=wn.NOUN)\n",
        "    predicate_synsets = wn.synsets(predicate_word, pos=wn.VERB)\n",
        "    object_synsets = wn.synsets(object_word, pos=wn.NOUN)\n",
        "\n",
        "    # Find all possible paths from subject to predicate\n",
        "    subject_paths = []\n",
        "    for subj in subject_synsets:\n",
        "        for pred in predicate_synsets:\n",
        "            path = cross_pos_path(subj, pred, max_depth=max_depth)\n",
        "            if path:\n",
        "                subject_paths.append(path)\n",
        "\n",
        "    # Find all possible paths from predicate to object\n",
        "    object_paths = []\n",
        "    for pred in predicate_synsets:\n",
        "        for obj in object_synsets:\n",
        "            path = cross_pos_path(pred, obj, max_depth=max_depth)\n",
        "            if path:\n",
        "                object_paths.append(path)\n",
        "\n",
        "    # Get the shortest paths (if any found)\n",
        "    shortest_subject_path = min(subject_paths, key=len) if subject_paths else None\n",
        "    shortest_object_path = min(object_paths, key=len) if object_paths else None\n",
        "\n",
        "    return shortest_subject_path, shortest_object_path\n",
        "\n",
        "\n",
        "def show_path(label, path):\n",
        "    \"\"\"\n",
        "    Pretty print a path of synsets.\n",
        "    \"\"\"\n",
        "    if path:\n",
        "        print(f\"{label}:\")\n",
        "        print(\" -> \".join(f\"{s.name()} ({s.definition()})\" for s in path))\n",
        "        print(f\"Path length: {len(path)}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"{label}: No path found\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Find shortest paths\n",
        "cat_path, mouse_path = find_shortest_paths(\"cat\", \"eat\", \"mouse\", max_depth=15)\n",
        "\n",
        "# Display results\n",
        "show_path(\"Path from 'cat' to 'eat' (subject -> predicate)\", cat_path)\n",
        "show_path(\"Path from 'eat' to 'mouse' (predicate -> object)\", mouse_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "valgVwQUh0SJ",
        "outputId": "f1568e03-7f15-4ec1-fb6a-3796c21a11eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path from 'cat' to 'eat' (subject -> predicate):\n",
            "kat.n.01 (the leaves of the shrub Catha edulis which are chewed like tobacco or used to make tea; has the effect of a euphoric stimulant) -> chew.v.01 (chew (food); to bite and grind with the teeth) -> grate.v.04 (make a grating or grinding sound by rubbing together) -> break_up.v.16 (break or cause to break into pieces) -> break.v.02 (become separated into pieces or fragments) -> change_integrity.v.01 (change in physical make-up) -> change.v.02 (undergo a change; become different in essence; losing one's or its original nature) -> play_out.v.04 (become spent or exhausted) -> run_down.v.06 (deplete) -> consume.v.05 (use up (resources or materials))\n",
            "Path length: 10\n",
            "\n",
            "Path from 'eat' to 'mouse' (predicate -> object):\n",
            "corrode.v.01 (cause to deteriorate due to the action of water, air, or an acid) -> natural_process.n.01 (a process existing in or produced by nature (rather than by the intent of human beings)) -> process.n.06 (a sustained phenomenon or one marked by gradual changes through a series of states) -> physical_entity.n.01 (an entity that has physical existence) -> causal_agent.n.01 (any entity that produces an effect or is responsible for events or results) -> person.n.01 (a human being) -> mouse.n.03 (person who is quiet or timid)\n",
            "Path length: 7\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EMRfljk7h1no"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}
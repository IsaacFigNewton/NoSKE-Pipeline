{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM40BwekfUEwvGqHE2gAljy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/SMIED/blob/main/FrameNetSpaCySRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB1kvDkNJ-tJ",
        "outputId": "5d03d228-826b-463a-a956-aad7731ff775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package framenet_v17 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "import nltk\n",
        "nltk.download('framenet_v17')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class Definitions"
      ],
      "metadata": {
        "id": "C7T7BvgkN7pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "FrameNet-SpaCy Semantic Role Labeling Pipeline\n",
        "A complete span-labeling based SRL system integrating NLTK's FrameNet with SpaCy\n",
        "\"\"\"\n",
        "\n",
        "import spacy\n",
        "from spacy.tokens import Doc, Span, Token\n",
        "from spacy.language import Language\n",
        "from nltk.corpus import framenet as fn\n",
        "from nltk.corpus import wordnet as wn\n",
        "from typing import List, Dict, Tuple, Optional, Set\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class FrameElement:\n",
        "    \"\"\"Represents a frame element with its span and metadata\"\"\"\n",
        "    name: str\n",
        "    span: Span\n",
        "    frame_name: str\n",
        "    confidence: float\n",
        "    fe_type: str  # Core, Non-Core, Extra-Thematic\n",
        "    definition: str = \"\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class FrameInstance:\n",
        "    \"\"\"Represents an evoked frame with its target and elements\"\"\"\n",
        "    name: str\n",
        "    target: Span  # The frame-evoking element (predicate)\n",
        "    elements: List[FrameElement]\n",
        "    confidence: float\n",
        "    definition: str = \"\"\n",
        "    lexical_unit: str = \"\"\n",
        "\n",
        "\n",
        "# Register the factory before the class definition\n",
        "@Language.factory(\n",
        "    \"framenet_srl\",\n",
        "    default_config={\n",
        "        \"min_confidence\": 0.5,\n",
        "        \"use_wordnet_expansion\": True\n",
        "    }\n",
        ")\n",
        "def create_framenet_srl_component(nlp, name, min_confidence, use_wordnet_expansion):\n",
        "    \"\"\"Factory to create FrameNetSpaCySRL component\"\"\"\n",
        "    return FrameNetSpaCySRL(\n",
        "        nlp=nlp,\n",
        "        min_confidence=min_confidence,\n",
        "        use_wordnet_expansion=use_wordnet_expansion\n",
        "    )\n",
        "\n",
        "\n",
        "class FrameNetSpaCySRL:\n",
        "    \"\"\"\n",
        "    Complete FrameNet-based Semantic Role Labeling pipeline for SpaCy.\n",
        "\n",
        "    This class identifies frames evoked by predicates in text and maps\n",
        "    frame elements to syntactic spans (noun chunks, prepositional phrases, etc.)\n",
        "\n",
        "    Usage:\n",
        "        # Method 1: Use as a SpaCy pipeline component\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        nlp.add_pipe(\"framenet_srl\", config={\"min_confidence\": 0.5})\n",
        "        doc = nlp(\"John gave Mary a book.\")\n",
        "\n",
        "        # Method 2: Use standalone\n",
        "        srl = FrameNetSpaCySRL()\n",
        "        doc = srl.process(\"John gave Mary a book.\")\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 nlp: Optional[Language] = None,\n",
        "                 spacy_model: str = \"en_core_web_sm\",\n",
        "                 min_confidence: float = 0.5,\n",
        "                 use_wordnet_expansion: bool = True):\n",
        "        \"\"\"\n",
        "        Initialize the FrameNet-SpaCy SRL pipeline.\n",
        "\n",
        "        Args:\n",
        "            nlp: Optional pre-loaded SpaCy Language object\n",
        "            spacy_model: Name of SpaCy model to load (ignored if nlp is provided)\n",
        "            min_confidence: Minimum confidence threshold for frame/element assignment\n",
        "            use_wordnet_expansion: Whether to use WordNet for frame expansion\n",
        "        \"\"\"\n",
        "        # Load or use provided SpaCy model\n",
        "        if nlp is not None:\n",
        "            self.nlp = nlp\n",
        "        else:\n",
        "            self.nlp = spacy.load(spacy_model)\n",
        "\n",
        "        self.min_confidence = min_confidence\n",
        "        self.use_wordnet_expansion = use_wordnet_expansion\n",
        "\n",
        "        # Setup SpaCy extensions\n",
        "        self._setup_extensions()\n",
        "\n",
        "        # Build caches for efficiency\n",
        "        self.frame_cache = self._build_frame_cache()\n",
        "        self.lexical_unit_cache = self._build_lexical_unit_cache()\n",
        "        self.fe_coreness_cache = self._build_fe_coreness_cache()\n",
        "\n",
        "    def _setup_extensions(self):\n",
        "        \"\"\"Setup custom SpaCy extensions for storing frame information\"\"\"\n",
        "        # Document level\n",
        "        if not Doc.has_extension(\"frames\"):\n",
        "            Doc.set_extension(\"frames\", default=[])\n",
        "        if not Doc.has_extension(\"frame_elements\"):\n",
        "            Doc.set_extension(\"frame_elements\", default=[])\n",
        "\n",
        "        # Token level\n",
        "        if not Token.has_extension(\"frames\"):\n",
        "            Token.set_extension(\"frames\", default=[])\n",
        "        if not Token.has_extension(\"is_predicate\"):\n",
        "            Token.set_extension(\"is_predicate\", default=False)\n",
        "\n",
        "        # Span level\n",
        "        if not Span.has_extension(\"frame\"):\n",
        "            Span.set_extension(\"frame\", default=None)\n",
        "        if not Span.has_extension(\"frame_element\"):\n",
        "            Span.set_extension(\"frame_element\", default=None)\n",
        "        if not Span.has_extension(\"semantic_role\"):\n",
        "            Span.set_extension(\"semantic_role\", default=None)\n",
        "\n",
        "    def _build_frame_cache(self) -> Dict:\n",
        "        \"\"\"Build cache of all frames for quick lookup\"\"\"\n",
        "        cache = {}\n",
        "        for frame in fn.frames():\n",
        "            cache[frame.name] = frame\n",
        "        return cache\n",
        "\n",
        "    def _build_lexical_unit_cache(self) -> Dict[Tuple[str, str], List[str]]:\n",
        "        \"\"\"Build cache mapping (lemma, pos) -> [frame_names]\"\"\"\n",
        "        cache = defaultdict(list)\n",
        "        for frame in fn.frames():\n",
        "            for lu_name, lu_data in frame.lexUnit.items():\n",
        "                # Parse lexical unit name (e.g., \"run.v\" -> (\"run\", \"v\"))\n",
        "                parts = lu_name.split('.')\n",
        "                if len(parts) >= 2:\n",
        "                    lemma = '.'.join(parts[:-1])\n",
        "                    pos = parts[-1]\n",
        "                    cache[(lemma.lower(), pos)].append(frame.name)\n",
        "        return dict(cache)\n",
        "\n",
        "    def _build_fe_coreness_cache(self) -> Dict[Tuple[str, str], str]:\n",
        "        \"\"\"Build cache of frame element coreness types\"\"\"\n",
        "        cache = {}\n",
        "        for frame in fn.frames():\n",
        "            for fe_name, fe_data in frame.FE.items():\n",
        "                cache[(frame.name, fe_name)] = fe_data.coreType\n",
        "        return cache\n",
        "\n",
        "    def __call__(self, doc: Doc) -> Doc:\n",
        "        \"\"\"Make the class callable as a SpaCy pipeline component\"\"\"\n",
        "        return self.process(doc)\n",
        "\n",
        "    def process_text(self, text: str) -> Doc:\n",
        "        \"\"\"\n",
        "        Convenience method to process raw text.\n",
        "\n",
        "        Args:\n",
        "            text: Input text string\n",
        "\n",
        "        Returns:\n",
        "            Processed SpaCy Doc with frame annotations\n",
        "        \"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        return self.process(doc)\n",
        "\n",
        "    def process(self, doc: Doc) -> Doc:\n",
        "        \"\"\"\n",
        "        Process a SpaCy Doc to identify frames and label semantic roles.\n",
        "\n",
        "        Args:\n",
        "            doc: SpaCy Doc object\n",
        "\n",
        "        Returns:\n",
        "            Doc with frame annotations added\n",
        "        \"\"\"\n",
        "        # Clear previous annotations\n",
        "        doc._.frames = []\n",
        "        doc._.frame_elements = []\n",
        "\n",
        "        # Step 1: Identify predicates (frame-evoking elements)\n",
        "        predicates = self._identify_predicates(doc)\n",
        "\n",
        "        # Step 2: For each predicate, identify possible frames\n",
        "        for pred_span in predicates:\n",
        "            frames = self._get_frames_for_predicate(pred_span)\n",
        "\n",
        "            if frames:\n",
        "                # Step 3: For best frame, identify and align frame elements\n",
        "                best_frame = self._select_best_frame(pred_span, frames, doc)\n",
        "                if best_frame:\n",
        "                    frame_instance = self._extract_frame_elements(\n",
        "                        doc, pred_span, best_frame\n",
        "                    )\n",
        "                    if frame_instance:\n",
        "                        doc._.frames.append(frame_instance)\n",
        "                        # Mark predicate token\n",
        "                        for token in pred_span:\n",
        "                            token._.is_predicate = True\n",
        "                            token._.frames.append(frame_instance.name)\n",
        "\n",
        "        return doc\n",
        "\n",
        "    def _identify_predicates(self, doc: Doc) -> List[Span]:\n",
        "        \"\"\"\n",
        "        Identify potential frame-evoking elements (predicates) in the document.\n",
        "\n",
        "        Returns verbs, relevant nouns, and adjectives that might evoke frames.\n",
        "        \"\"\"\n",
        "        predicates = []\n",
        "\n",
        "        # Verbs are primary frame evokers\n",
        "        for token in doc:\n",
        "            if token.pos_ == \"VERB\" and not token.is_stop:\n",
        "                # Check for phrasal verbs and multi-word expressions\n",
        "                span = self._expand_predicate_span(token)\n",
        "                predicates.append(span)\n",
        "\n",
        "        # Some nouns and adjectives also evoke frames\n",
        "        for chunk in doc.noun_chunks:\n",
        "            head = chunk.root\n",
        "            # Check if head noun is in our lexical unit cache\n",
        "            if (head.lemma_.lower(), 'n') in self.lexical_unit_cache:\n",
        "                predicates.append(doc[head.i:head.i+1])\n",
        "\n",
        "        # Adjectives in predicative position\n",
        "        for token in doc:\n",
        "            if token.pos_ == \"ADJ\" and token.dep_ in [\"acomp\", \"xcomp\"]:\n",
        "                if (token.lemma_.lower(), 'a') in self.lexical_unit_cache:\n",
        "                    predicates.append(doc[token.i:token.i+1])\n",
        "\n",
        "        return predicates\n",
        "\n",
        "    def _expand_predicate_span(self, verb: Token) -> Span:\n",
        "        \"\"\"\n",
        "        Expand verb to include particles and auxiliaries for phrasal verbs.\n",
        "        E.g., \"pick up\", \"look forward to\"\n",
        "        \"\"\"\n",
        "        doc = verb.doc\n",
        "        start = verb.i\n",
        "        end = verb.i + 1\n",
        "\n",
        "        # Include particles\n",
        "        for child in verb.children:\n",
        "            if child.dep_ == \"prt\" and child.i > verb.i:\n",
        "                end = max(end, child.i + 1)\n",
        "\n",
        "        # Include auxiliary verbs\n",
        "        for child in verb.children:\n",
        "            if child.dep_ == \"aux\" and child.i < verb.i:\n",
        "                start = min(start, child.i)\n",
        "\n",
        "        return doc[start:end]\n",
        "\n",
        "    def _get_frames_for_predicate(self, pred_span: Span) -> List[str]:\n",
        "        \"\"\"Get possible frames for a predicate using lexical units and WordNet.\"\"\"\n",
        "        frames = set()\n",
        "\n",
        "        # Primary lookup via lexical units\n",
        "        head = pred_span.root\n",
        "        pos_map = {'VERB': 'v', 'NOUN': 'n', 'ADJ': 'a', 'ADV': 'adv'}\n",
        "        if head.pos_ in pos_map:\n",
        "            fn_pos = pos_map[head.pos_]\n",
        "            key = (head.lemma_.lower(), fn_pos)\n",
        "            if key in self.lexical_unit_cache:\n",
        "                frames.update(self.lexical_unit_cache[key])\n",
        "\n",
        "        # WordNet expansion if enabled\n",
        "        if self.use_wordnet_expansion and head.pos_ == \"VERB\":\n",
        "            expanded_frames = self._get_frames_via_wordnet(head.lemma_, wn.VERB)\n",
        "            frames.update(expanded_frames)\n",
        "\n",
        "        return list(frames)\n",
        "\n",
        "    def _get_frames_via_wordnet(self, lemma: str, pos) -> Set[str]:\n",
        "        \"\"\"Expand frame search using WordNet synonyms\"\"\"\n",
        "        frames = set()\n",
        "        for synset in wn.synsets(lemma, pos=pos)[:3]:  # Limit to top 3 senses\n",
        "            for syn_lemma in synset.lemma_names():\n",
        "                key = (syn_lemma.lower(), 'v')\n",
        "                if key in self.lexical_unit_cache:\n",
        "                    frames.update(self.lexical_unit_cache[key])\n",
        "        return frames\n",
        "\n",
        "    def _select_best_frame(self, pred_span: Span,\n",
        "                          frame_names: List[str],\n",
        "                          doc: Doc) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Select the best frame for a predicate based on context.\n",
        "        Uses simple heuristics - could be enhanced with ML model.\n",
        "        \"\"\"\n",
        "        if len(frame_names) == 1:\n",
        "            return frame_names[0]\n",
        "\n",
        "        scores = {}\n",
        "        for frame_name in frame_names:\n",
        "            frame = self.frame_cache[frame_name]\n",
        "            score = 0.0\n",
        "\n",
        "            # Score based on lexical unit match\n",
        "            pred_text = pred_span.text.lower()\n",
        "            for lu_name in frame.lexUnit:\n",
        "                if pred_text in lu_name.lower():\n",
        "                    score += 2.0\n",
        "\n",
        "            # Score based on frame element compatibility with syntax\n",
        "            score += self._score_syntactic_compatibility(pred_span, frame, doc)\n",
        "\n",
        "            scores[frame_name] = score\n",
        "\n",
        "        # Return highest scoring frame if above threshold\n",
        "        best_frame = max(scores, key=scores.get)\n",
        "        if scores[best_frame] >= self.min_confidence:\n",
        "            return best_frame\n",
        "        return None\n",
        "\n",
        "    def _score_syntactic_compatibility(self, pred_span: Span,\n",
        "                                      frame, doc: Doc) -> float:\n",
        "        \"\"\"Score how well frame elements align with syntactic structure\"\"\"\n",
        "        score = 0.0\n",
        "        pred_head = pred_span.root\n",
        "\n",
        "        # Check for core frame elements in syntactic dependents\n",
        "        core_fes = [fe for fe, data in frame.FE.items()\n",
        "                   if data.coreType == \"Core\"]\n",
        "\n",
        "        # Subject (nsubj) often maps to Agent-like FEs\n",
        "        for child in pred_head.children:\n",
        "            if child.dep_ == \"nsubj\":\n",
        "                if any(fe in [\"Agent\", \"Theme\", \"Experiencer\"]\n",
        "                      for fe in core_fes):\n",
        "                    score += 1.0\n",
        "            elif child.dep_ in [\"dobj\", \"obj\"]:\n",
        "                if any(fe in [\"Theme\", \"Patient\", \"Goal\"]\n",
        "                      for fe in core_fes):\n",
        "                    score += 1.0\n",
        "\n",
        "        return score\n",
        "\n",
        "    def _extract_frame_elements(self, doc: Doc, pred_span: Span,\n",
        "                               frame_name: str) -> Optional[FrameInstance]:\n",
        "        \"\"\"\n",
        "        Extract and align frame elements with syntactic spans.\n",
        "        \"\"\"\n",
        "        frame = self.frame_cache[frame_name]\n",
        "        frame_elements = []\n",
        "        pred_head = pred_span.root\n",
        "\n",
        "        # Map syntactic dependents to frame elements\n",
        "        for child in pred_head.children:\n",
        "            fe = self._map_dependent_to_fe(child, frame, pred_head)\n",
        "            if fe:\n",
        "                # Get the span for this frame element\n",
        "                span = self._get_fe_span(child, doc)\n",
        "                if span:\n",
        "                    element = FrameElement(\n",
        "                        name=fe,\n",
        "                        span=span,\n",
        "                        frame_name=frame_name,\n",
        "                        confidence=self._calculate_confidence(child, fe, frame),\n",
        "                        fe_type=self.fe_coreness_cache.get((frame_name, fe), \"Non-Core\"),\n",
        "                        definition=frame.FE[fe].definition if fe in frame.FE else \"\"\n",
        "                    )\n",
        "                    frame_elements.append(element)\n",
        "                    # Set span extension\n",
        "                    span._.frame_element = fe\n",
        "                    span._.frame = frame_name\n",
        "\n",
        "        # Also check for frame elements in prepositional phrases\n",
        "        for pp in self._get_prepositional_phrases(pred_head):\n",
        "            fe = self._map_pp_to_fe(pp, frame, pred_head)\n",
        "            if fe:\n",
        "                span = doc[pp.left_edge.i:pp.right_edge.i+1]\n",
        "                element = FrameElement(\n",
        "                    name=fe,\n",
        "                    span=span,\n",
        "                    frame_name=frame_name,\n",
        "                    confidence=self._calculate_confidence(pp, fe, frame),\n",
        "                    fe_type=self.fe_coreness_cache.get((frame_name, fe), \"Non-Core\"),\n",
        "                    definition=frame.FE[fe].definition if fe in frame.FE else \"\"\n",
        "                )\n",
        "                frame_elements.append(element)\n",
        "                span._.frame_element = fe\n",
        "                span._.frame = frame_name\n",
        "\n",
        "        if frame_elements or True:  # Create instance even without elements\n",
        "            return FrameInstance(\n",
        "                name=frame_name,\n",
        "                target=pred_span,\n",
        "                elements=frame_elements,\n",
        "                confidence=self._calculate_frame_confidence(frame_elements),\n",
        "                definition=frame.definition,\n",
        "                lexical_unit=f\"{pred_span.text}.{pred_head.pos_[0].lower()}\"\n",
        "            )\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _map_dependent_to_fe(self, dep: Token, frame,\n",
        "                            pred: Token) -> Optional[str]:\n",
        "        \"\"\"Map syntactic dependent to frame element\"\"\"\n",
        "        dep_role = dep.dep_\n",
        "\n",
        "        # Mapping rules based on dependency and frame\n",
        "        mappings = {\n",
        "            \"nsubj\": [\"Agent\", \"Experiencer\", \"Theme\", \"Cognizer\", \"Speaker\"],\n",
        "            \"nsubjpass\": [\"Patient\", \"Theme\", \"Undergoer\"],\n",
        "            \"dobj\": [\"Theme\", \"Patient\", \"Goal\", \"Stimulus\", \"Content\"],\n",
        "            \"obj\": [\"Theme\", \"Patient\", \"Goal\", \"Stimulus\", \"Content\"],\n",
        "            \"iobj\": [\"Recipient\", \"Beneficiary\", \"Goal\"],\n",
        "            \"xcomp\": [\"Event\", \"State\", \"Content\"],\n",
        "            \"ccomp\": [\"Message\", \"Content\", \"Topic\"],\n",
        "            \"advcl\": [\"Time\", \"Purpose\", \"Manner\", \"Condition\"],\n",
        "        }\n",
        "\n",
        "        if dep_role in mappings:\n",
        "            # Find first matching FE that exists in this frame\n",
        "            for fe_candidate in mappings[dep_role]:\n",
        "                if fe_candidate in frame.FE:\n",
        "                    return fe_candidate\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _map_pp_to_fe(self, pp_head: Token, frame,\n",
        "                     pred: Token) -> Optional[str]:\n",
        "        \"\"\"Map prepositional phrase to frame element\"\"\"\n",
        "        prep = pp_head.text.lower()\n",
        "\n",
        "        # Preposition-based mappings\n",
        "        prep_mappings = {\n",
        "            \"in\": [\"Place\", \"Location\", \"Time\", \"Manner\"],\n",
        "            \"on\": [\"Place\", \"Topic\", \"Time\"],\n",
        "            \"at\": [\"Place\", \"Location\", \"Time\"],\n",
        "            \"to\": [\"Goal\", \"Recipient\", \"Destination\"],\n",
        "            \"from\": [\"Source\", \"Origin\"],\n",
        "            \"with\": [\"Instrument\", \"Manner\", \"Comitative\"],\n",
        "            \"by\": [\"Agent\", \"Means\", \"Time\"],\n",
        "            \"for\": [\"Purpose\", \"Beneficiary\", \"Duration\"],\n",
        "            \"about\": [\"Topic\", \"Content\"],\n",
        "            \"through\": [\"Path\", \"Means\"],\n",
        "            \"during\": [\"Time\", \"Duration\"],\n",
        "        }\n",
        "\n",
        "        if prep in prep_mappings:\n",
        "            for fe_candidate in prep_mappings[prep]:\n",
        "                if fe_candidate in frame.FE:\n",
        "                    return fe_candidate\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _get_fe_span(self, token: Token, doc: Doc) -> Optional[Span]:\n",
        "        \"\"\"Get the span for a frame element starting from a token\"\"\"\n",
        "        # For noun phrases, get the full chunk\n",
        "        for chunk in doc.noun_chunks:\n",
        "            if token in chunk:\n",
        "                return chunk\n",
        "\n",
        "        # For clauses, get the full subtree\n",
        "        if token.dep_ in [\"xcomp\", \"ccomp\", \"advcl\"]:\n",
        "            left = token.left_edge.i\n",
        "            right = token.right_edge.i + 1\n",
        "            return doc[left:right]\n",
        "\n",
        "        # Default to token and its subtree\n",
        "        left = token.left_edge.i\n",
        "        right = token.right_edge.i + 1\n",
        "        return doc[left:right]\n",
        "\n",
        "    def _get_prepositional_phrases(self, verb: Token) -> List[Token]:\n",
        "        \"\"\"Get prepositional phrases attached to verb\"\"\"\n",
        "        pps = []\n",
        "        for child in verb.children:\n",
        "            if child.dep_ == \"prep\":\n",
        "                pps.append(child)\n",
        "        return pps\n",
        "\n",
        "    def _calculate_confidence(self, token: Token, fe: str,\n",
        "                             frame) -> float:\n",
        "        \"\"\"Calculate confidence score for frame element assignment\"\"\"\n",
        "        confidence = 0.5  # Base confidence\n",
        "\n",
        "        # Boost for core frame elements\n",
        "        if (frame.name, fe) in self.fe_coreness_cache:\n",
        "            if self.fe_coreness_cache[(frame.name, fe)] == \"Core\":\n",
        "                confidence += 0.2\n",
        "\n",
        "        # Boost for clear syntactic mapping\n",
        "        if token.dep_ in [\"nsubj\", \"dobj\", \"obj\"]:\n",
        "            confidence += 0.15\n",
        "\n",
        "        # Penalty for distant tokens\n",
        "        if hasattr(token, 'head'):\n",
        "            distance = abs(token.i - token.head.i)\n",
        "            confidence -= min(0.1, distance * 0.02)\n",
        "\n",
        "        return min(1.0, max(0.0, confidence))\n",
        "\n",
        "    def _calculate_frame_confidence(self, elements: List[FrameElement]) -> float:\n",
        "        \"\"\"Calculate overall confidence for frame instance\"\"\"\n",
        "        if not elements:\n",
        "            return 0.3  # Low confidence if no elements found\n",
        "\n",
        "        # Average element confidences with boost for core elements\n",
        "        total_conf = 0.0\n",
        "        total_weight = 0.0\n",
        "\n",
        "        for elem in elements:\n",
        "            weight = 2.0 if elem.fe_type == \"Core\" else 1.0\n",
        "            total_conf += elem.confidence * weight\n",
        "            total_weight += weight\n",
        "\n",
        "        return total_conf / total_weight if total_weight > 0 else 0.0\n",
        "\n",
        "    def get_frame_summary(self, doc: Doc) -> Dict:\n",
        "        \"\"\"\n",
        "        Get a summary of all frames and elements in the document.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with frame statistics and details\n",
        "        \"\"\"\n",
        "        summary = {\n",
        "            \"text\": doc.text,\n",
        "            \"frames\": [],\n",
        "            \"statistics\": {\n",
        "                \"total_frames\": len(doc._.frames),\n",
        "                \"total_elements\": sum(len(f.elements) for f in doc._.frames),\n",
        "                \"predicates\": []\n",
        "            }\n",
        "        }\n",
        "\n",
        "        for frame_inst in doc._.frames:\n",
        "            frame_data = {\n",
        "                \"frame\": frame_inst.name,\n",
        "                \"predicate\": frame_inst.target.text,\n",
        "                \"confidence\": frame_inst.confidence,\n",
        "                \"elements\": []\n",
        "            }\n",
        "\n",
        "            for element in frame_inst.elements:\n",
        "                frame_data[\"elements\"].append({\n",
        "                    \"role\": element.name,\n",
        "                    \"text\": element.span.text,\n",
        "                    \"type\": element.fe_type,\n",
        "                    \"confidence\": element.confidence\n",
        "                })\n",
        "\n",
        "            summary[\"frames\"].append(frame_data)\n",
        "            summary[\"statistics\"][\"predicates\"].append(frame_inst.target.text)\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def visualize_frames(self, doc: Doc) -> str:\n",
        "        \"\"\"\n",
        "        Create a text-based visualization of frames and elements.\n",
        "\n",
        "        Returns:\n",
        "            Formatted string representation\n",
        "        \"\"\"\n",
        "        lines = []\n",
        "        lines.append(\"=\" * 80)\n",
        "        lines.append(f\"Text: {doc.text}\")\n",
        "        lines.append(\"=\" * 80)\n",
        "\n",
        "        for frame_inst in doc._.frames:\n",
        "            lines.append(f\"\\nFrame: {frame_inst.name}\")\n",
        "            lines.append(f\"  Predicate: '{frame_inst.target.text}' \"\n",
        "                        f\"[{frame_inst.target.start_char}:{frame_inst.target.end_char}]\")\n",
        "            lines.append(f\"  Confidence: {frame_inst.confidence:.2f}\")\n",
        "            lines.append(f\"  Definition: {frame_inst.definition[:100]}...\")\n",
        "\n",
        "            if frame_inst.elements:\n",
        "                lines.append(\"  Frame Elements:\")\n",
        "                for elem in sorted(frame_inst.elements,\n",
        "                                 key=lambda x: x.span.start_char):\n",
        "                    marker = \"**\" if elem.fe_type == \"Core\" else \"  \"\n",
        "                    lines.append(f\"    {marker}{elem.name}: '{elem.span.text}' \"\n",
        "                               f\"(conf: {elem.confidence:.2f})\")\n",
        "            else:\n",
        "                lines.append(\"  No frame elements identified\")\n",
        "\n",
        "        if not doc._.frames:\n",
        "            lines.append(\"\\nNo frames identified in the text.\")\n",
        "\n",
        "        lines.append(\"=\" * 80)\n",
        "        return \"\\n\".join(lines)"
      ],
      "metadata": {
        "id": "3fz6T2vIKUDB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests"
      ],
      "metadata": {
        "id": "VuOkAxTBN2rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1: Use as standalone class\n",
        "print(\"METHOD 1: Standalone Usage\")\n",
        "print(\"-\" * 40)\n",
        "srl = FrameNetSpaCySRL(use_wordnet_expansion=True)\n",
        "\n",
        "# Test sentences\n",
        "test_sentences = [\n",
        "    \"John gave Mary a book in the library.\",\n",
        "    \"The chef cooked dinner for the guests with great skill.\",\n",
        "    \"She quickly ran to the store to buy milk.\",\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    # Process the sentence\n",
        "    doc = srl.process_text(sentence)\n",
        "\n",
        "    # Print visualization\n",
        "    print(srl.visualize_frames(doc))\n",
        "    print()\n",
        "\n",
        "# Method 2: Use as SpaCy pipeline component\n",
        "print(\"\\nMETHOD 2: SpaCy Pipeline Component\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Load SpaCy and add the component\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"framenet_srl\", config={\"min_confidence\": 0.5})\n",
        "\n",
        "# Process text through the pipeline\n",
        "doc = nlp(\"The company announced its new product at the conference.\")\n",
        "\n",
        "# Access frame information\n",
        "for frame in doc._.frames:\n",
        "    print(f\"\\nFrame: {frame.name}\")\n",
        "    print(f\"Predicate: '{frame.target.text}'\")\n",
        "    print(f\"Confidence: {frame.confidence:.2f}\")\n",
        "    for element in frame.elements:\n",
        "        print(f\"  {element.name}: '{element.span.text}' ({element.fe_type})\")\n",
        "\n",
        "# Method 3: Quick one-liner for testing\n",
        "print(\"\\nMETHOD 3: Quick Testing\")\n",
        "print(\"-\" * 40)\n",
        "quick_srl = FrameNetSpaCySRL()\n",
        "doc = quick_srl.process_text(\"The student solved the problem by thinking creatively.\")\n",
        "summary = quick_srl.get_frame_summary(doc)\n",
        "print(f\"Found {summary['statistics']['total_frames']} frames \"\n",
        "      f\"with {summary['statistics']['total_elements']} elements\")\n",
        "for frame_data in summary['frames']:\n",
        "    print(f\"  {frame_data['frame']}: {frame_data['predicate']}\")\n",
        "    for elem in frame_data['elements']:\n",
        "        print(f\"    - {elem['role']}: {elem['text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4sbpVR3jCSt",
        "outputId": "58a1e9fa-2077-4986-8789-31e11a42cb08"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METHOD 1: Standalone Usage\n",
            "----------------------------------------\n",
            "================================================================================\n",
            "Text: John gave Mary a book in the library.\n",
            "================================================================================\n",
            "\n",
            "Frame: Giving\n",
            "  Predicate: 'gave' [5:9]\n",
            "  Confidence: 0.81\n",
            "  Definition: A Donor transfers a Theme from a Donor to a Recipient.  This frame includes only actions that are in...\n",
            "  Frame Elements:\n",
            "    **Theme: 'John' (conf: 0.83)\n",
            "    **Theme: 'a book' (conf: 0.79)\n",
            "\n",
            "Frame: Text\n",
            "  Predicate: 'book' [17:21]\n",
            "  Confidence: 0.48\n",
            "  Definition: A Text is an entity that contains linguistic, symbolic information on a Topic, created by an Author ...\n",
            "  Frame Elements:\n",
            "      Place: 'in the library' (conf: 0.48)\n",
            "\n",
            "Frame: Buildings\n",
            "  Predicate: 'library' [29:36]\n",
            "  Confidence: 0.30\n",
            "  Definition: This frame contains words which name permanent fixed structures forming an enclosure and providing p...\n",
            "  No frame elements identified\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Text: The chef cooked dinner for the guests with great skill.\n",
            "================================================================================\n",
            "\n",
            "Frame: Arriving\n",
            "  Predicate: 'cooked' [9:15]\n",
            "  Confidence: 0.70\n",
            "  Definition: An object Theme moves in the direction of a Goal. The Goal may be expressed or it may be understood ...\n",
            "  Frame Elements:\n",
            "    **Theme: 'The chef' (conf: 0.83)\n",
            "    **Theme: 'dinner' (conf: 0.83)\n",
            "      Purpose: 'for the guests' (conf: 0.46)\n",
            "      Manner: 'with great skill' (conf: 0.40)\n",
            "\n",
            "Frame: People_by_vocation\n",
            "  Predicate: 'chef' [4:8]\n",
            "  Confidence: 0.30\n",
            "  Definition: This frame contains  words for Individuals as viewed in terms of their vocation.   The Person is con...\n",
            "  No frame elements identified\n",
            "\n",
            "Frame: Social_event\n",
            "  Predicate: 'dinner' [16:22]\n",
            "  Confidence: 0.30\n",
            "  Definition: A Social_event occurs at which Attendees are present to conduct a social function or joint activity....\n",
            "  No frame elements identified\n",
            "\n",
            "Frame: Guest_and_host\n",
            "  Predicate: 'guests' [31:37]\n",
            "  Confidence: 0.30\n",
            "  Definition: Two individuals, the Guest and the Host are in a temporary social relationship within which the Host...\n",
            "  No frame elements identified\n",
            "\n",
            "Frame: Expertise\n",
            "  Predicate: 'skill' [49:54]\n",
            "  Confidence: 0.30\n",
            "  Definition: This frame concerns people's knowledge or skill in certain domains.  It does not concern acquaintanc...\n",
            "  No frame elements identified\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Text: She quickly ran to the store to buy milk.\n",
            "================================================================================\n",
            "\n",
            "Frame: Departing\n",
            "  Predicate: 'ran' [12:15]\n",
            "  Confidence: 0.62\n",
            "  Definition: An object (the Theme) moves away from a Source. The Source may be expressed or it may be understood ...\n",
            "  Frame Elements:\n",
            "    **Theme: 'She' (conf: 0.81)\n",
            "      Goal: 'to the store' (conf: 0.48)\n",
            "      Time: 'to buy milk' (conf: 0.40)\n",
            "\n",
            "Frame: Commerce_buy\n",
            "  Predicate: 'to buy' [29:35]\n",
            "  Confidence: 0.30\n",
            "  Definition: These are words describing a basic commercial transaction involving a Buyer and a Seller exchanging ...\n",
            "  No frame elements identified\n",
            "\n",
            "Frame: Businesses\n",
            "  Predicate: 'store' [23:28]\n",
            "  Confidence: 0.30\n",
            "  Definition: A Proprietor owns or runs a Business which provides a Product (which may be goods or services).  'Th...\n",
            "  No frame elements identified\n",
            "\n",
            "Frame: Food\n",
            "  Predicate: 'milk' [36:40]\n",
            "  Confidence: 0.30\n",
            "  Definition: This frame contains words referring to items of food.  'Specialties include deep fried shredded beef...\n",
            "  No frame elements identified\n",
            "================================================================================\n",
            "\n",
            "\n",
            "METHOD 2: SpaCy Pipeline Component\n",
            "----------------------------------------\n",
            "\n",
            "Frame: Businesses\n",
            "Predicate: 'company'\n",
            "Confidence: 0.30\n",
            "\n",
            "Frame: Manufacturing\n",
            "Predicate: 'product'\n",
            "Confidence: 0.30\n",
            "\n",
            "Frame: Discussion\n",
            "Predicate: 'conference'\n",
            "Confidence: 0.30\n",
            "\n",
            "METHOD 3: Quick Testing\n",
            "----------------------------------------\n",
            "Found 3 frames with 3 elements\n",
            "  Work: solved\n",
            "    - Agent: The student\n",
            "    - Goal: the problem\n",
            "    - Agent: by thinking creatively\n",
            "  Education_teaching: student\n",
            "  Predicament: problem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jugDQxN2bDVW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}